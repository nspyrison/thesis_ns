<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Interrogating the linear variable importance of local explanations of non-linear models with animated linear projections | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models</title>
  <meta name="description" content="Chapter 4 Interrogating the linear variable importance of local explanations of non-linear models with animated linear projections | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models" />
  <meta name="generator" content="bookdown 0.24.4 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Interrogating the linear variable importance of local explanations of non-linear models with animated linear projections | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Interrogating the linear variable importance of local explanations of non-linear models with animated linear projections | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models" />
  
  
  

<meta name="author" content="Nicholas S Spyrison" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="3-ch-efficacy_radial_tour.html"/>
<link rel="next" href="5-ch-conclusion.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="template/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./"> </a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#research-questions"><i class="fa fa-check"></i><b>1.1</b> Research questions</a></li>
<li class="chapter" data-level="1.2" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#methodology"><i class="fa fa-check"></i><b>1.2</b> Methodology</a></li>
<li class="chapter" data-level="1.3" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#contributions"><i class="fa fa-check"></i><b>1.3</b> Contributions</a></li>
<li class="chapter" data-level="1.4" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#ch-background"><i class="fa fa-check"></i><b>1.4</b> Background</a></li>
<li class="chapter" data-level="1.5" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#thesis-structure"><i class="fa fa-check"></i><b>1.5</b> Thesis structure</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html"><i class="fa fa-check"></i><b>2</b> spinifex: an R package for creating user-controlled animated linear projections</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#sec:algorithm"><i class="fa fa-check"></i><b>2.2</b> Algorithm</a></li>
<li class="chapter" data-level="2.3" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#sec:pkgstructure"><i class="fa fa-check"></i><b>2.3</b> Package structure</a></li>
<li class="chapter" data-level="2.4" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#sec:usecases"><i class="fa fa-check"></i><b>2.4</b> Use cases</a></li>
<li class="chapter" data-level="2.5" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#sec:discussion"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
<li class="chapter" data-level="2.6" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#acknowledgments-1"><i class="fa fa-check"></i><b>2.6</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html"><i class="fa fa-check"></i><b>3</b> The benefit of user-controlled radial tour for understanding variable contributions to structure in linear projections</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html#sec:background"><i class="fa fa-check"></i><b>3.2</b> Background</a></li>
<li class="chapter" data-level="3.3" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html#sec:userstudy"><i class="fa fa-check"></i><b>3.3</b> User study</a></li>
<li class="chapter" data-level="3.4" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html#sec:results"><i class="fa fa-check"></i><b>3.4</b> Results</a></li>
<li class="chapter" data-level="3.5" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html#response-time-regression"><i class="fa fa-check"></i><b>3.5</b> Response time regression</a></li>
<li class="chapter" data-level="3.6" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html#sec:conclusion"><i class="fa fa-check"></i><b>3.6</b> Conclusion</a></li>
<li class="chapter" data-level="3.7" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html#sec:spinifex"><i class="fa fa-check"></i><b>3.7</b> Accompanying tool: radial tour application</a></li>
<li class="chapter" data-level="3.8" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html#sec:acknowledgments"><i class="fa fa-check"></i><b>3.8</b> Acknowledgments</a></li>
<li class="chapter" data-level="3.9" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html#appendix"><i class="fa fa-check"></i><b>3.9</b> Appendix</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html"><i class="fa fa-check"></i><b>4</b> Interrogating the linear variable importance of local explanations of non-linear models with animated linear projections</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#sec:intro"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#sec:explanations"><i class="fa fa-check"></i><b>4.2</b> Local explanation statistics</a></li>
<li class="chapter" data-level="4.3" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#tours-and-the-radial-tour"><i class="fa fa-check"></i><b>4.3</b> Tours and the radial tour</a></li>
<li class="chapter" data-level="4.4" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#sec:applicationdesign"><i class="fa fa-check"></i><b>4.4</b> Application design</a></li>
<li class="chapter" data-level="4.5" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#sec:casestudies"><i class="fa fa-check"></i><b>4.5</b> Case studies</a></li>
<li class="chapter" data-level="4.6" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#sec:cheemdiscussion"><i class="fa fa-check"></i><b>4.6</b> Discussion</a></li>
<li class="chapter" data-level="4.7" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#acknowledgments-2"><i class="fa fa-check"></i><b>4.7</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-ch-conclusion.html"><a href="5-ch-conclusion.html"><i class="fa fa-check"></i><b>5</b> Conclusion</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5-ch-conclusion.html"><a href="5-ch-conclusion.html#software-development"><i class="fa fa-check"></i><b>5.1</b> Software development</a></li>
<li class="chapter" data-level="5.2" data-path="5-ch-conclusion.html"><a href="5-ch-conclusion.html#further-extensions"><i class="fa fa-check"></i><b>5.2</b> Further extensions</a></li>
<li class="chapter" data-level="5.3" data-path="5-ch-conclusion.html"><a href="5-ch-conclusion.html#other-contributions"><i class="fa fa-check"></i><b>5.3</b> Other contributions</a></li>
</ul></li>
<li class="divider"></li>
<li><strong><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></strong></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-cheem" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Interrogating the linear variable importance of local explanations of non-linear models with animated linear projections</h1>
<!-- WARNING, XXX:TO ADRESS -->
<p><em>THE CHAPTER IS A RELATIVELY RECENT SNAPSHOT OF THE ORIGINAL REPOSITORY</em></p>
<!-- Segue -->
<p>Now we can be confident that the radial tour leads to better analysis of variable-level attribution to features identified in a projection. We want to increase the interpretability of complex models. Specifically, I suggest a using the radial tour to explore variable sensitivity to the structure identified in linear local explanations of non-linear models.</p>
<!-- Abstract -->
<p>Artificial Intelligence (AI) has seen a revitalization in recent years from the use of increasingly hard-to-interpret black-box models. In such models, increased predictive power comes at the cost of opaque factor analysis, which has led to the field of eXplainable AI (XAI). XAI attempts to shed light on these models, one such approach is the use of local explanations. A local explanation of a model gives a point-estimate of linear variable importance in the vicinity of one observation. We extract explanations for each observation, and approximate data and this attribution space side-by-side with linked brushing. After identifying an observation of interest its local explanation is used as a 1D projection basis. We then manipulate the magnitude of the variable contributions with a technique called the tour. This tour animates many projections over small changes in the projection basis. Doing so allows a user to visually explore the data space through the lens of this local explanation and interrogate its variable importance. The implementation of our framework is available as an <strong>R</strong> package <strong>cheem</strong> available on CRAN.</p>
<div id="sec:intro" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Introduction</h2>
<!-- Higher-level topics -->
<!-- History of regression and classification -->
<p>Mathematically rigorous approaches to predictive modeling are attributed to the least-squares method, over two centuries ago by Legendre and Gauss in 1805 and 1809, respectively. In 1886 Francis Galton coined the term <em>regression</em> to refer to continuous, quantitative predictions. While <em>classification</em> refers to discrete predictions as introduced by Fisher in 1936.</p>
<!-- Introduce explanatory vs predictive modeling -->
<p>Breiman and Shmueli <span class="citation">(<a href="5-ch-conclusion.html#ref-breiman_statistical_2001" role="doc-biblioref">Breiman 2001</a>; <a href="5-ch-conclusion.html#ref-shmueli_explain_2010" role="doc-biblioref">Shmueli 2010</a>)</span> introduce the idea of distinguishing modeling based on its purpose; <em>explanatory</em> modeling is done for some inferential purpose such as hypothesis testing, while <em>predictive</em> modeling predicts new, out-of-sample, observations. This distinction draws attention to the divide between interpretable models and black-box models. In explanatory modeling, interpretability is vital for drawing inferential conclusions. While predictive modeling may opt for potentially more accurate black-box models. The intended use has important implications for model selection and development.</p>
<!-- XAI & interpretability crisis -->
<p>Black-box models are becoming increasingly common, but not without their share of controversy and issues <span class="citation">(<a href="5-ch-conclusion.html#ref-oneil_weapons_2016" role="doc-biblioref">O’neil 2016</a>; <a href="5-ch-conclusion.html#ref-kodiyan_overview_2019" role="doc-biblioref">Kodiyan 2019</a>)</span>. Black-box models have been known to reflect common biases, including sex <span class="citation">(<a href="5-ch-conclusion.html#ref-dastin_amazon_2018" role="doc-biblioref">Dastin 2018</a>; <a href="5-ch-conclusion.html#ref-duffy_apple_2019" role="doc-biblioref">Duffy 2019</a>)</span>, race <span class="citation">(<a href="5-ch-conclusion.html#ref-larson_how_2016" role="doc-biblioref">Larson et al. 2016</a>)</span>, and age <span class="citation">(<a href="5-ch-conclusion.html#ref-diaz_addressing_2018" role="doc-biblioref">Díaz et al. 2018</a>)</span>. Such issues occur when biases existent in the training data, the model picks up on this influence on the response variable, which is then built into the model. Another issue is data drift when new data is outside the support of latent or exogenous explanatory variables. Data drift can lead to worse predictions <span class="citation">(<a href="5-ch-conclusion.html#ref-lazer_parable_2014" role="doc-biblioref">Lazer et al. 2014</a>; <a href="5-ch-conclusion.html#ref-salzberg_why_2014" role="doc-biblioref">Salzberg 2014</a>)</span>. Such cases highlight the need to make models fair, accountable, ethical, and transparent, which has led to the movement of XAI <span class="citation">(<a href="5-ch-conclusion.html#ref-adadi_peeking_2018" role="doc-biblioref">Adadi and Berrada 2018</a>; <a href="5-ch-conclusion.html#ref-arrieta_explainable_2020" role="doc-biblioref">Arrieta et al. 2020</a>)</span>.</p>
<!-- Local explanations -->
<p>One branch of XAI is local explanations, which take a variable attribution approach to bring transparency to a model. Local explanations attempt to approximate linear variable importance at the location of one observation. There are many such local explanations.</p>
<!-- SHAP -->
<p>To illustrate our work, we apply the model-agnostic explanation SHAP <span class="citation">(<a href="5-ch-conclusion.html#ref-strumbelj_efficient_2010" role="doc-biblioref">Strumbelj and Kononenko 2010</a>; <a href="5-ch-conclusion.html#ref-strumbelj_explaining_2014" role="doc-biblioref">Štrumbelj and Kononenko 2014</a>)</span>. The exact details of SHAP are tangent to the ideas of this work, but suffice it to say that SHAP approximates variable importance by taking the median importance over permutations of the explanatory variables. To be exact, we apply a variant that enjoys a lower computational complexity, known as tree SHAP <span class="citation">(<a href="5-ch-conclusion.html#ref-lundberg_consistent_2018" role="doc-biblioref">S. M. Lundberg, Erion, and Lee 2018</a>)</span>.
<!-- 2nd half is more application details; move lower? --></p>
<!-- ## Data visualization tours -->
<p>In multivariate data visualization, a <em>tour</em> <span class="citation">(<a href="5-ch-conclusion.html#ref-asimov_grand_1985" role="doc-biblioref">Asimov 1985</a>; <a href="5-ch-conclusion.html#ref-buja_grand_1986" role="doc-biblioref">Buja and Asimov 1986</a>; <a href="5-ch-conclusion.html#ref-lee_state_2021" role="doc-biblioref">S. Lee et al. 2021</a>)</span> is a sequence of linear projections of data onto a lower-dimensional space, typically 1-3D. Tours are viewed as an animation over small changes to the projection basis. Structure in a projection can then be explored visually to see which variables contribute to the formation of that structure. The intuition is similar to watching the shadow of a hidden 3D object change as the object is rotated; watching the structural shape of the shadow change gleans insight into the shape and features of the object.</p>
<!-- manual tours -->
<p>There are various types of tours, which are distinguished by the generation of projection bases. In a <em>manual</em> tour <span class="citation">(<a href="5-ch-conclusion.html#ref-cook_manual_1997" role="doc-biblioref">Cook and Buja 1997</a>; <a href="5-ch-conclusion.html#ref-spyrison_spinifex_2020" role="doc-biblioref">Spyrison and Cook 2020</a>)</span>, the path is defined by changing the contribution of a selected variable. <!--tours and models --> Applying tours to models has been done in a couple of contexts. Specifically for exploring various statistical model fits and classification boundaries <span class="citation">(<a href="5-ch-conclusion.html#ref-wickham_visualizing_2015" role="doc-biblioref">Hadley Wickham, Cook, and Hofmann 2015</a>)</span>, and using tree- and forest-based approaches as a projection pursuit index to generate a tour basis path <span class="citation">(<a href="5-ch-conclusion.html#ref-lee_pptree_2013" role="doc-biblioref">Y. D. Lee et al. 2013</a>; <a href="5-ch-conclusion.html#ref-da_silva_projection_2021" role="doc-biblioref">Silva, Cook, and Lee 2021</a>)</span>.</p>
<!-- Purposed approach -->
<p>The proposed approach is to use the radial, manual tour to interrogate a local explanation. After identifying an observation of interest, its explanation can be evaluated by testing the support of the structure identified by the explanation as the contributions of the variables are varied with the radial tour. We provide a free and open-source R package <strong>cheem</strong> with an interactive application to facilitate analysis. We give case studies of toy and modern datasets for both classification and regression tasks.</p>
<!-- What-if & counterfactual analysis -->
<p>The change in the projection basis might feel similar to counterfactual, what-if analysis, such as <em>ceteris paribus</em> <span class="citation">(<a href="5-ch-conclusion.html#ref-biecek_ceterisparibus_2020" role="doc-biblioref">Biecek 2020</a>)</span>. Latin for “other things held constant” or “all else unchanged” is a counterfactual analysis showing how an observation’s prediction would change from a change in one explanatory variable given that other variables are held constant. It ignores correlations of the variables and imagines a case that was not observed. In contrast, our approach is a geometric explanation of the factual, observed case by that varies contributions of the basis, essentially the orientation of the data object. Another difference is that the basis must maintain orthonormality. That is to say, when the contribution of one variable decreases, the contributions of others necessarily increase such that there is a complete component in that direction.</p>
<!-- Paper structure -->
<p>The remainder of this paper is organized as follows. The following section, <a href="4-ch-cheem.html#sec:explanations">Local explanation statistics</a>, covers the background of the local explanation, SHAP, and the traditional visuals produced from it. <a href="#sec:tour">Tours and the radial tour</a> digs deeper into these animations of continuous linear projections. The section <a href="4-ch-cheem.html#sec:applicationdesign">Application Design</a> discusses the layout of the application, how it facilitates analysis, preprocessing, and package infrastructure. The section <a href="4-ch-cheem.html#sec:casestudies">Case Studies</a> illustrates several applications of this method. We conclude with a <a href="4-ch-cheem.html#sec:cheemdiscussion">Discussion</a> of the insights we draw from classification and regression tasks.</p>
</div>
<div id="sec:explanations" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Local explanation statistics</h2>
<!-- reminder of local explanation -->
<p>Consider a highly non-linear model. At face value, it is hard to say which variable(s) are sensitive to the crossing of a classification boundary or identifies which variables caused an observation to have a relatively extreme residual. Local explanations shed light on these cases by approximating linear variable importance in the vicinity of one observation.</p>
<!-- Taxonomy of local explanations -->
<p>Figure 6 of <span class="citation"><a href="5-ch-conclusion.html#ref-arrieta_explainable_2020" role="doc-biblioref">Arrieta et al.</a> (<a href="5-ch-conclusion.html#ref-arrieta_explainable_2020" role="doc-biblioref">2020</a>)</span> gives comprehensive summarization of the taxonomy and literature of explanation techniques. This includes a large number of model-specific explanations such as deepLIFT, <span class="citation">(<a href="5-ch-conclusion.html#ref-shrikumar_not_2016" role="doc-biblioref">Shrikumar et al. 2016</a>; <a href="5-ch-conclusion.html#ref-shrikumar_learning_2017" role="doc-biblioref">Shrikumar, Greenside, and Kundaje 2017</a>)</span> a popular recursive method for estimating importance in neural networks. There are a fewer number of model-agnostic explanations, of which LIME, <span class="citation">(<a href="5-ch-conclusion.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>)</span> SHAP, <span class="citation">(<a href="5-ch-conclusion.html#ref-lundberg_unified_2017" role="doc-biblioref">S. Lundberg and Lee 2017</a>)</span> and their variants are popular.</p>
<!-- Uses of local explanations -->
<p>These instance-level explanations are used in a variety of ways depending on the data. In images, saliency maps overlay or offset a heatmap indicating which pixels were important <span class="citation">(<a href="5-ch-conclusion.html#ref-simonyan_deep_2014" role="doc-biblioref">Simonyan, Vedaldi, and Zisserman 2014</a>)</span>. For instance, snow may be highlighted when distinguishing if a picture contains a wolf or husky. In text analysis, word-level contextual sentiment analysis can be used to highlight the sentiment and magnitude of influential words <span class="citation">(<a href="5-ch-conclusion.html#ref-vanni_textual_2018" role="doc-biblioref">Vanni et al. 2018</a>)</span>. In the case of numeric regression, they are used to explain variable additive contributions from model intercept to the observation’s prediction <span class="citation">(<a href="5-ch-conclusion.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>)</span>.</p>
<div id="shap-and-tree-shap" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> SHAP and tree SHAP</h3>
<!-- SHAP and history -->
<p>SHaply Additive exPlanations (SHAP) approximates the variable importance in the vicinity of one observation by taking the median importance of a subset of permutations in the explanatory variables. This idea stems from the field of game theory, where <span class="citation"><a href="5-ch-conclusion.html#ref-shapley_value_1953" role="doc-biblioref">Shapley</a> (<a href="5-ch-conclusion.html#ref-shapley_value_1953" role="doc-biblioref">1953</a>)</span> devised a method to evaluate an individual’s contribution to cooperative games by permuting the players that contribute to the score.</p>
<!-- fifa example -->
<p>To illustrate SHAP and its original use, explaining the difference between the intercept and an observation’s prediction, we use soccer data from FIFA 2020 season <span class="citation">(<a href="5-ch-conclusion.html#ref-leone_fifa_2020" role="doc-biblioref">Leone 2020</a>)</span>. We have 5000 observations of nine skill measures (after aggregating highly correlated variables). A random forest model is fit to regress the log wages, in 2020 Euros, from the skill measures. We then extract the SHAP values of a star offensive player (L. Messi) and defensive player (V. van Dijk). We expect to see a difference in the attribution of the variable importance across the two positions of the players.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch5fig1"></span>
<img src="figures/ch5_fig1_shap_distr_bd.png" alt="Illustration of the distribution of SHAP attributions and a break-down plot. From FIFA 2020 data, a random forest model regresses wages from nine skill attributes for a star offensive and defensive player. The players have very different wages, but a) shows the distributions of the attributions permuting over 25 permutations in the explanatory variables. The medians of these distributions are the final SHAP values. The variable importance differs across the exogenous information of player position. These explanations make sense; the variable importances seem realistic given the positions of players. b) Break-down plots of the observations using their explanations to additively cover the difference between the model intercept and the observation predictions." width="100%"  />
<p class="caption">
Figure 4.1: Illustration of the distribution of SHAP attributions and a break-down plot. From FIFA 2020 data, a random forest model regresses wages from nine skill attributes for a star offensive and defensive player. The players have very different wages, but a) shows the distributions of the attributions permuting over 25 permutations in the explanatory variables. The medians of these distributions are the final SHAP values. The variable importance differs across the exogenous information of player position. These explanations make sense; the variable importances seem realistic given the positions of players. b) Break-down plots of the observations using their explanations to additively cover the difference between the model intercept and the observation predictions.
</p>
</div>
<!-- Illustration -->
<p>Figure <a href="4-ch-cheem.html#fig:ch5fig1">4.1</a> shows the SHAP values of these players. Panel a) shows these players receive a sizable difference in wages. Panel b) shows the underlying distribution of the SHAP attributions while permuting the explanatory variables, with the medians being the SHAP values. In the light of the player position, the difference in the variable importance makes sense; offensive and movement are more important for the offensive player, while defensive and power skills are more informative to the model for explaining the prediction of the defensive player. We would likewise expect the profile of variable importance to be unique for star players of other positions, such as goalkeepers or middle fielders. Panel c) shows a simplified break-down plot <span class="citation">(<a href="5-ch-conclusion.html#ref-gosiewska_ibreakdown_2019" role="doc-biblioref">Gosiewska and Biecek 2019</a>)</span>, where a local explanation is used to additively explain the difference from the intercept to the observations prediction. Such additive approaches will show an asymmetry in the variable ordering, so we opt to fix the order to panel b), namely, by decreasing the sum of the SHAP values.</p>
<!-- Segue -->
<p>In summary, this highlights how local explanations bring interpretability to a model, at least in the vicinity of their observations. In this instance, we showed how two players with different positions receive different profiles of variable importance to explain the prediction of their wages. In the following section, we will be using normalized explanations as the starting projection basis to interrogate the explanation further.</p>
</div>
</div>
<div id="tours-and-the-radial-tour" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Tours and the radial tour</h2>
<!-- tours intro -->
<p>A data visualization <em>tour</em> animates many linear projections over small changes in the basis. One of the key features of the tour is the object permanence of the data points; one can track the relative changes of observations as the basis moves, as opposed to discretely jumping to an orthogonal view with no intermediate information. There are various types of tours, which are distinguished by the selection of their basis paths <span class="citation">(<a href="5-ch-conclusion.html#ref-lee_state_2021" role="doc-biblioref">S. Lee et al. 2021</a>; <a href="5-ch-conclusion.html#ref-cook_grand_2008" role="doc-biblioref">Cook et al. 2008</a>)</span>.</p>
<div id="manual-tours-and-the-radial-subset" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Manual tours and the radial subset</h3>
<!-- Manual tour -->
<p>The manual tour <span class="citation">(<a href="5-ch-conclusion.html#ref-cook_manual_1997" role="doc-biblioref">Cook and Buja 1997</a>)</span> defines its basis path by manipulating the basis contribution of a selected variable. A manipulation dimension is appended onto the projection plane, with a full contribution given to the selected variable. The bases are then selected based on rotating this newly created manipulation space. A crucial feature of the manual tour is that it allows users to control the variable contributions of the basis. This means that such manipulations can be selected and queued in advance or selected on the spot for human-in-the-loop analysis <span class="citation">(<a href="5-ch-conclusion.html#ref-karwowski_international_2006" role="doc-biblioref">Karwowski 2006</a>)</span>. However, this navigation is relatively time-consuming due to the huge volume of <span class="math inline">\(p\)</span>-space. It is advisable to use this method to explore the sensitivity of the variable contribution to a previously identified feature of interest. In this case, the projection of the normalized explanations is the feature of interest.</p>
<!-- radial tour -->
<p>More generally, the manual tour can change the contribution of the variable in any of the display dimensions. We will with a specific more directed interaction, namely, a <em>radial</em> tour. In a radial tour, the selected variable is allowed to change its magnitude of contribution but not its angle; it must move along the direction of its original contribution radius.</p>
</div>
</div>
<div id="sec:applicationdesign" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Application design</h2>
<p>Below we illustrate the two primary displays of the application: the global view and the tour view. Then we cover what we take away from the classification and regression tasks. Lastly, we discuss the preprocessing that before display.</p>
<div id="global-view" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Global view</h3>
<!-- Purpose -->
<p>The global view provides an essential context of all observations and facilitates the exploration of the separability of the data- and attribution-spaces. While ultimately, its purpose is to facilitate the selection of a primary point and comparison point.</p>
<!-- Approximations of the spaces, PC1:2 -->
<p>An approximation of these spaces is given as the first two principal components of their respective spaces. This is shown side-by-side next to model information, the prediction, and the observed response variable. The orientation and magnitude of the variables are inscribed on a unit circle. While a single 2D projection will rarely encompass all of the structure of higher-dimensional spaces, it is a reasonable summarization, given the real task at hand, the selection of observations to explore further.</p>
<!-- Predicted by observed -->
<p>It is insightful to explore these two approximations against a visual of the model; prediction by observation is also displayed. Linked brushing and preserved aesthetic features such as circling misclassified observations help link information from the different spaces together.</p>
</div>
<div id="radial-cheem-tour" class="section level3" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Radial cheem tour</h3>
<!-- a function of the obs selected above -->
<p>The global view facilitated the selection of a primary and optional comparison observation. The variable-level attribution of primary observation is normalized and used as the initial 1D basis in a radial tour. This is an approximation of the contributions of the linear variables that best explain the difference between the model intercept and an observations prediction, not the local shape of the model surface.</p>
<!-- Tour start frame -->
<p>The initial frame is the normalized SHAP values of the primary observation. The current projection basis is depicted as the width of a bar, the variable’s contribution to the horizontal axis. The normalized values of all observations are shown as vertical parallel coordinate plots.</p>
<!-- tour animation -->
<p>The radial tour creates a basis path by varying the contribution of a selected variable, fully into and out of a projection frame. Doing so tests an individual variable’s sensitivity to the structure identified by the local explanation. The default variable selected has the largest discrepancy between the primary and comparison observations attribution. In the following sections, we elaborate on the takeaways we draw from applying this approach in classification and regression tasks, respectively.</p>
<!-- Segue to classification -->
<p>Now that we have introduced the global view and corresponding cheem radial tour, let’s discuss the differences between the classification and regression cases.</p>
</div>
<div id="classification-task" class="section level3" number="4.4.3">
<h3><span class="header-section-number">4.4.3</span> Classification task</h3>
<p>What information do we glean from using this method on a classification task? Typically we select a misclassified observation compared to a correctly classified point that is nearby in data space. We start by seeing the data projected through the linear attribution, the combination that best justifies that prediction. By default, the manual tour varies the contribution of the variable with the largest difference between the primary and comparison observation. That is, we can test the sensitivity of each variable to structure identified by the local explanation; we are exploring the support of the explanation, evaluating the support or robustness of the prediction.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch5fig2"></span>
<img src="figures/ch5_fig2_app_classification.PNG" alt="Display illustrating the classification case. Both views are colored on predicted class and while red circles identify misclassified observations. The radial tour is a 1D projection starting at the normalized tree SHAP values of the primary point. The first frame is the linear-variable importances that best describe the difference from model intercept to this observation's prediction. We probe the support of the variable contributions by selecting a variable to vary the contribution." width="100%"  />
<p class="caption">
Figure 4.2: Display illustrating the classification case. Both views are colored on predicted class and while red circles identify misclassified observations. The radial tour is a 1D projection starting at the normalized tree SHAP values of the primary point. The first frame is the linear-variable importances that best describe the difference from model intercept to this observation’s prediction. We probe the support of the variable contributions by selecting a variable to vary the contribution.
</p>
</div>
</div>
<div id="regression-task" class="section level3" number="4.4.4">
<h3><span class="header-section-number">4.4.4</span> Regression task</h3>
<p>The regression case, we opt to color the global view on a statistic, the observation’s residual, the log Mahalanobis distance in data space (a measure of outlyingness), and the correlation of the attribution projection with the observed response. In the radial tour, the horizontal positions are the same, the attribution projection, then varying as the radial tour changes basis. While the vertical position is fixed to the observed response variable and residuals for the middle and right panels, respectively. This changes the display from 1D density to a scatterplot. The basis is still one component, the horizontal position, independent of the vertical position.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch5fig3"></span>
<img src="figures/ch5_fig3_app_regression.PNG" alt="Display of the regression case. The global view can be colored on the correlation of the attribution projection and observed response. In the tour, the horizontal values are the same as the classification case. The vertical position is fixed to the observed response and residuals in the middle and right facets, respectively." width="100%"  />
<p class="caption">
Figure 4.3: Display of the regression case. The global view can be colored on the correlation of the attribution projection and observed response. In the tour, the horizontal values are the same as the classification case. The vertical position is fixed to the observed response and residuals in the middle and right facets, respectively.
</p>
</div>
</div>
<div id="interactive-features" class="section level3" number="4.4.5">
<h3><span class="header-section-number">4.4.5</span> Interactive features</h3>
<!-- reactive vs exploration interactions -->
<p>The application has several reactive selections that affect the data, coloring, and inputs for the tour to improve flexibility by extending the use cases. It also has interactive features in the global view that aid the analysis.</p>
<!-- Interaction except -->
<p>A tooltip displays while the cursor hovers over a point displays the observation number/name and classification information if appropriate. Linked brushing allows for the selection of points (with left click and drag) where those points will be highlighted in both plots. The information corresponding to the selected points is populated on a dynamic table. These interactions aid exploration of the spaces and ultimately the selection of the primary and comparison observations.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch5fig4"></span>
<img src="figures/ch5_fig4_app_interactions.PNG" alt="Illustration of data explorations interactions in the global view. This view has linked brushing of the points where observations selected in one facet are highlighted in the other facets and populate an interactive tabular display below. Tooltips display when hovering over a point" width="100%"  />
<p class="caption">
Figure 4.4: Illustration of data explorations interactions in the global view. This view has linked brushing of the points where observations selected in one facet are highlighted in the other facets and populate an interactive tabular display below. Tooltips display when hovering over a point
</p>
</div>
</div>
<div id="preprocessing" class="section level3" number="4.4.6">
<h3><span class="header-section-number">4.4.6</span> Preprocessing</h3>
<p>The benefit of having dynamic interaction with data is predicated on a reasonably short render time. It is important to preprocess expensive operations so the application resources can be used efficiently. The work remaining at runtime is solely responding to inputs and rendering of visuals and tables. Below we discuss the steps and details of the reprocessing.</p>
(ref:citeRf) <span class="citation"><a href="5-ch-conclusion.html#ref-liaw_classification_2002" role="doc-biblioref">Liaw and Wiener</a> (<a href="5-ch-conclusion.html#ref-liaw_classification_2002" role="doc-biblioref">2002</a>)</span>
(ref:citeTs) <span class="citation"><a href="5-ch-conclusion.html#ref-kominsarczyk_treeshap_2021" role="doc-biblioref">Kominsarczyk et al.</a> (<a href="5-ch-conclusion.html#ref-kominsarczyk_treeshap_2021" role="doc-biblioref">2021</a>)</span>
<!-- note on time of execution -->
<p>The time to preprocess the data will vary significantly with the model and local explanation. For reference, the FIFA data, 5000 observations of nine explanatory variables, took 2.9 seconds to fit a random forest model of modest hyperparameters. Extracting the tree SHAP values of each observation took 254 seconds combined. PCA and statistics of the variables and attributions took 0.6 seconds. These runtimes were from a non-parallelized R session on a modern laptop, but suffice it to say that the bulk of the time will be spent on the local attribution. Increased model complexity or data dimensionality will quickly become an obstacle. This makes tree SHAP, with its reduced computational complexity, a good candidate to start with. Alternatively, the package <strong>fastshap</strong> <span class="citation">(<a href="5-ch-conclusion.html#ref-greenwell_fastshap_2020" role="doc-biblioref">Greenwell 2020</a>)</span> claims extremely low runtimes, which are attributed to fewer calls to the prediction function, partial implementation in C++, and efficient use of logical subsetting.</p>
</div>
<div id="sec:infrastructure" class="section level3" number="4.4.7">
<h3><span class="header-section-number">4.4.7</span> Package infrastructure</h3>
<p>The above-described method and application are implemented as an open-source <strong>R</strong> package, <strong>cheem</strong> available on <a href="https://CRAN.R-project.org/package=cheem">CRAN</a>. Preprocessing was facilitated with models created via <strong>randomForest</strong> [liaw_classification_2002], and explanations calculated with <strong>treeshap</strong> <span class="citation">(<a href="5-ch-conclusion.html#ref-kominsarczyk_treeshap_2021" role="doc-biblioref">Kominsarczyk et al. 2021</a>)</span>. The application was made with <strong>shiny</strong> <span class="citation">(<a href="5-ch-conclusion.html#ref-chang_shiny_2021" role="doc-biblioref">Chang et al. 2021</a>)</span>. The tour visual is an extension of <strong>spinifex</strong> <span class="citation">(<a href="5-ch-conclusion.html#ref-spyrison_spinifex_2020" role="doc-biblioref">Spyrison and Cook 2020</a>)</span>. Both views are created first with first with <strong>ggplot2</strong> <span class="citation">(<a href="5-ch-conclusion.html#ref-wickham_ggplot2_2016" role="doc-biblioref">Hadley Wickham 2016</a>)</span> and then rendered as interactive HTML widgets with <strong>plotly</strong> <span class="citation">(<a href="5-ch-conclusion.html#ref-sievert_interactive_2020" role="doc-biblioref">Sievert 2020</a>)</span>. <strong>DALEX</strong> <span class="citation">(<a href="5-ch-conclusion.html#ref-biecek_dalex_2018" role="doc-biblioref">Biecek 2018</a>)</span> and the free ebook, <em>Explanatory Model Analysis</em> <span class="citation">(<a href="5-ch-conclusion.html#ref-biecek_explanatory_2021" role="doc-biblioref">Biecek and Burzykowski 2021</a>)</span> was a huge boon to understanding local explanations and how to apply them.</p>
</div>
<div id="installation-and-getting-started" class="section level3" number="4.4.8">
<h3><span class="header-section-number">4.4.8</span> Installation and getting started</h3>
<p>The following <strong>R</strong> code will help getting up and running:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="4-ch-cheem.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Download the package</span></span>
<span id="cb4-2"><a href="4-ch-cheem.html#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;cheem&quot;</span>, <span class="at">dependencies =</span> <span class="cn">TRUE</span>)</span>
<span id="cb4-3"><a href="4-ch-cheem.html#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Restart the R session so the IDE has the correct directory structure</span></span>
<span id="cb4-4"><a href="4-ch-cheem.html#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="fu">restartSession</span>()</span>
<span id="cb4-5"><a href="4-ch-cheem.html#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Load cheem into session</span></span>
<span id="cb4-6"><a href="4-ch-cheem.html#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;cheem&quot;</span>)</span>
<span id="cb4-7"><a href="4-ch-cheem.html#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Try the app</span></span>
<span id="cb4-8"><a href="4-ch-cheem.html#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="fu">run_app</span>()</span>
<span id="cb4-9"><a href="4-ch-cheem.html#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="4-ch-cheem.html#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Processing your data</span></span>
<span id="cb4-11"><a href="4-ch-cheem.html#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Install treeshap from github, to use as a local explainer</span></span>
<span id="cb4-12"><a href="4-ch-cheem.html#cb4-12" aria-hidden="true" tabindex="-1"></a>remotes<span class="sc">::</span><span class="fu">install_github</span>(<span class="st">&#39;ModelOriented/treeshap&#39;</span>) <span class="do">## Local </span></span>
<span id="cb4-13"><a href="4-ch-cheem.html#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Follow the examples in cheem_ls()</span></span>
<span id="cb4-14"><a href="4-ch-cheem.html#cb4-14" aria-hidden="true" tabindex="-1"></a>?cheem_ls</span></code></pre></div>
</div>
</div>
<div id="sec:casestudies" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Case studies</h2>
<p>To illustrate the use of this analysis, we apply it to modern datasets, two classification examples and then two of regression.</p>
<div id="penguin-species-classification" class="section level3" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> 1) Penguin, species classification</h3>
<p>Palmer penguins data <span class="citation">(<a href="5-ch-conclusion.html#ref-gorman_ecological_2014" role="doc-biblioref">Gorman, Williams, and Fraser 2014</a>; <a href="5-ch-conclusion.html#ref-horst_palmerpenguins_2020" role="doc-biblioref">Horst, Hill, and Gorman 2020</a>)</span> consist of 330 observations across four physical measurements of three species of penguins foraging near Palmer Station, Antarctica. A random forest model was fit, classifying the species of the penguin given the physical measurements.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:casepenguins"></span>
<img src="figures/ch5_fig5_case1_penguins.png" alt="Species classification of Palmer penguin data." width="100%"  />
<p class="caption">
Figure 4.5: Species classification of Palmer penguin data.
</p>
</div>
<p>In figure <a href="4-ch-cheem.html#fig:casepenguins">4.5</a>, a misclassified point is contrasted with a correctly classified point of its observed class nearby in data-space. The attribution space from the tree SHAP local explanations is a more separable space, where the comparison is squarely in the middle of the orange distribution. The primary observation is in-between the predicted and observed clusters, a sign of uncertainty in the prediction. The tour varies the contribution of bill length (b_l) as this variable differs most from the contribution of the comparison observation. Downplaying the contribution of bill length is crucial to the linear explanation of this observation being misclassified.</p>
</div>
<div id="chocolates-milkdark-chocolate-classification" class="section level3" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> 2) Chocolates, milk/dark chocolate classification</h3>
<p>The chocolates dataset consists of 88 observations of 10 nutritional measurements from their labels. Each of which was labeled as being either milk or dark chocolates. With this data, we can see if a manufacturer gives an accurate portrayal of the chocolate. We are curious to see if there are chocolates that nutritionally look like milk chocolates that are labeled as dark chocolates, which may hold a higher market value. We should note that not all chocolates consist wholly of chocolate. The addition of other ingredients will decrease the predictive power of the model nutritional explanatory variable. A random forest model is fit classifying the type of chocolate. We selected a chocolate labeled dark, through predicted to be milk chocolate with a comparison with chocolate labeled 85% cocoa.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:casechocolates"></span>
<img src="figures/ch5_fig6_case2_chocolates.png" alt="Chocolates data type classification (milk or dark)." width="100%"  />
<p class="caption">
Figure 4.6: Chocolates data type classification (milk or dark).
</p>
</div>
<p>In figure <a href="4-ch-cheem.html#fig:casechocolates">4.6</a>, we similarly see that attribution space is more separable relative to data-space. Interestingly there is not the class imbalance that we suspected; there are only six chocolates labeled as dark and predicted as milk, while eight of the inverse case. Calories from fat is the variable with the largest difference in treeshap attribution between these points.</p>
</div>
<div id="fifa-wage-regression" class="section level3" number="4.5.3">
<h3><span class="header-section-number">4.5.3</span> 3) FIFA, wage regression</h3>
<p>The 2020 season FIFA data <span class="citation">(<a href="5-ch-conclusion.html#ref-leone_fifa_2020" role="doc-biblioref">Leone 2020</a>; <a href="5-ch-conclusion.html#ref-biecek_dalex_2018" role="doc-biblioref">Biecek 2018</a>)</span> contains many skill measurements of soccer/football players and wage information. After aggregation of the skill measurements, we regress the log wages [2020 euros] given just the skill aggregates. The model was fit from 5000 observations of the nine skill aggregates before being thinned to 500 players to mitigate occlusion and render time. We compare a leading offensive fielder (L. Messi) with that of a top defensive fielder (V. van Dijk), the same observations were used in figure <a href="4-ch-cheem.html#fig:ch5fig1">4.1</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:casefifa"></span>
<img src="figures/ch5_fig7_case3_fifa.png" alt="FIFA 2020, regressing log wages [2020 Euros] from aggregations of skill measurements. The primary observation is a star offensive player (L. Messi) compared with a top defensive player (V. van Dijk)." width="100%"  />
<p class="caption">
Figure 4.7: FIFA 2020, regressing log wages [2020 Euros] from aggregations of skill measurements. The primary observation is a star offensive player (L. Messi) compared with a top defensive player (V. van Dijk).
</p>
</div>
<p>With figure <a href="4-ch-cheem.html#fig:casefifa">4.7</a>, we will test the premise of the local explanation. If we remove reaction and movement skills from the basis, then offense skills have almost singular importance for the explanation of the offensive player. We vary the contribution of offensive skills. In the tour (3rd frame of b), offensive skills moved, and Messi is no longer separated from the group. We also notice that accuracy has rotated into the frame, maintaining some separability.</p>
</div>
<div id="ames-housing-2018-sales-price-regression" class="section level3" number="4.5.4">
<h3><span class="header-section-number">4.5.4</span> 4) Ames housing 2018, sales price regression</h3>
<p>Ames 2018, housing data was subset to North Ames (the neighborhood with the most house sales). The remaining are 338 house sales across nine variables. Using interaction from the global view, we select a house with an extreme negative residual and an accurate observation close to it in the data.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:caseames"></span>
<img src="figures/ch5_fig8_case4_ames2018.png" alt="Ames housing 2018 regressing log sales price [2018 USD]." width="100%"  />
<p class="caption">
Figure 4.8: Ames housing 2018 regressing log sales price [2018 USD].
</p>
</div>
<p>Figure <a href="4-ch-cheem.html#fig:caseames">4.8</a> shows the global view and extrema of the tour. The horizontal distance in the tour didn’t show a significant disparity between our selected points. This is not particularly surprising as most variables have a sizable contribution. Rotating any one variable out of the frame will rotate other vital variables into the frame, preserving most of the distance from intercept to prediction. However, the tour has revealed an interesting feature worth discussing. Notice that the observations pivot about the origin, the basis roughly halfway between bases in frames 1 and 2 of panel b) the data is near a singular profile. This means that there is a basis orthogonal to this point that describes sizable variation. Knowing these singular bases can point toward others that have meaningful variation in the data.</p>
</div>
</div>
<div id="sec:cheemdiscussion" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Discussion</h2>
<p>The need to maintain the interpretability of black-box models is evident. One aspect uses local explanations of the model in the vicinity of an observation. Local explanations approximate the linear variable importance to the model. Our contribution is to assess the explanations by examining the support of explanation, varying the contribution with a radial tour. First, a global view visualizes approximations of the data and explanation spaces side-by-side, using dynamic interaction to compare and contrast, and ultimately, identify primary and comparison observations of interest. Then normalized the linear importance from the explanation of the primary observation to use as the initial basis of a manual tour. The variable sensitivity to the structure identified in the explanation is explored with the tours varying basis.</p>
<p>We have illustrated this method on random forest models using the tree SHAP local explanation, while it could be generally used with any compatible model-explanation pairing. We apply it to the classification and regression tasks. We have created an open-source <strong>R</strong> package <strong>cheem</strong>, available on <a href="https://CRAN.R-project.org/package=cheem">CRAN</a>, to facilitate preprocessing and exploration with the described interactive application. Toy and real data are provided, or upload your data after preprocessing.</p>
</div>
<div id="acknowledgments-2" class="section level2" number="4.7">
<h2><span class="header-section-number">4.7</span> Acknowledgments</h2>
<p>We would like to thank Professor Przemyslaw Biecek for his input early in the project and to the broader <span class="math inline">\(\text{MI}^\text2\)</span> lab group for the <strong>DALEX</strong> ecosystem of <strong>R</strong> and <strong>Python</strong> packages. This research was supported by Australian Government Research Training Program (RTP) scholarships.</p>
<p>The namesake, Cheem, refers to a fictional race of humanoid trees from Doctor Who lore. <strong>DALEX</strong> pulls on from that universe, and we initially apply tree SHAP explanations that are specific to tree-based models.</p>
<!-- Adds a bib section at the end of every chapter -->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3-ch-efficacy_radial_tour.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="5-ch-conclusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
