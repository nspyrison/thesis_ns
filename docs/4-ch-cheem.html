<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Application of the radial tour to model-agnostic local explanations | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models</title>
  <meta name="description" content="Chapter 4 Application of the radial tour to model-agnostic local explanations | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models" />
  <meta name="generator" content="bookdown 0.24.4 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Application of the radial tour to model-agnostic local explanations | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Application of the radial tour to model-agnostic local explanations | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models" />
  
  
  

<meta name="author" content="Nicholas S Spyrison" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="3-ch-efficacy_radial_tour.html"/>
<link rel="next" href="5-ch-conclusion.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="template/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./"> </a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#research-questions"><i class="fa fa-check"></i><b>1.1</b> Research questions</a></li>
<li class="chapter" data-level="1.2" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#methodology"><i class="fa fa-check"></i><b>1.2</b> Methodology</a></li>
<li class="chapter" data-level="1.3" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#contributions"><i class="fa fa-check"></i><b>1.3</b> Contributions</a></li>
<li class="chapter" data-level="1.4" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#ch-background"><i class="fa fa-check"></i><b>1.4</b> Background</a></li>
<li class="chapter" data-level="1.5" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#thesis-structure"><i class="fa fa-check"></i><b>1.5</b> Thesis structure</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html"><i class="fa fa-check"></i><b>2</b> spinifex: an R package for creating user-controlled animated linear projections</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#sec:algorithm"><i class="fa fa-check"></i><b>2.2</b> Algorithm</a></li>
<li class="chapter" data-level="2.3" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#package-structure-and-functionality"><i class="fa fa-check"></i><b>2.3</b> Package structure and functionality</a></li>
<li class="chapter" data-level="2.4" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#sec:usecases"><i class="fa fa-check"></i><b>2.4</b> Use cases</a></li>
<li class="chapter" data-level="2.5" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#sec:discussion"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
<li class="chapter" data-level="2.6" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#acknowledgments-1"><i class="fa fa-check"></i><b>2.6</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html"><i class="fa fa-check"></i><b>3</b> The benefit of user-controlled radial tour for understanding variable contributions to structure in linear projections</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html#sec:background"><i class="fa fa-check"></i><b>3.2</b> Background</a></li>
<li class="chapter" data-level="3.3" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html#sec:userstudy"><i class="fa fa-check"></i><b>3.3</b> User study</a></li>
<li class="chapter" data-level="3.4" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html#sec:results"><i class="fa fa-check"></i><b>3.4</b> Results</a></li>
<li class="chapter" data-level="3.5" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html#sec:conclusion"><i class="fa fa-check"></i><b>3.5</b> Conclusion</a></li>
<li class="chapter" data-level="3.6" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html#sec:spinifex"><i class="fa fa-check"></i><b>3.6</b> Accompanying tool: radial tour application</a></li>
<li class="chapter" data-level="3.7" data-path="3-ch-efficacy_radial_tour.html"><a href="3-ch-efficacy_radial_tour.html#acknowledgments-2"><i class="fa fa-check"></i><b>3.7</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html"><i class="fa fa-check"></i><b>4</b> Application of the radial tour to model-agnostic local explanations</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#sec:intro"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#motivation"><i class="fa fa-check"></i><b>4.2</b> Motivation</a></li>
<li class="chapter" data-level="4.3" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#extending-the-interpretation-of-black-box-models-with-the-use-of-interactive-continuous-linear-projections"><i class="fa fa-check"></i><b>4.3</b> Extending the interpretation of black-box models with the use of interactive continuous linear projections</a></li>
<li class="chapter" data-level="4.4" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#acknowledgments-3"><i class="fa fa-check"></i><b>4.4</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-ch-conclusion.html"><a href="5-ch-conclusion.html"><i class="fa fa-check"></i><b>5</b> Conclusion</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5-ch-conclusion.html"><a href="5-ch-conclusion.html#further-extensions"><i class="fa fa-check"></i><b>5.1</b> Further extensions</a></li>
<li class="chapter" data-level="5.2" data-path="5-ch-conclusion.html"><a href="5-ch-conclusion.html#other-contributions"><i class="fa fa-check"></i><b>5.2</b> Other contributions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-ch-appendix.html"><a href="6-ch-appendix.html"><i class="fa fa-check"></i><b>6</b> Appendix</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><strong><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></strong></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-cheem" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Application of the radial tour to model-agnostic local explanations</h1>
<p>THIS CHAPTER IS CURRENTLY BEING EDITED IN ITs OWN REPOSITORY, THIS VERSION IS OUT OF DATE.</p>
<p>TODO:XXX Segue.</p>
<div id="sec:intro" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Introduction</h2>
<!-- WHAT TOPICS -->
<!-- multivariate vis, tours -->
<p>This work is concerned with linear projections of multivariate data. More specifically, we focus on the class of visualizations known as <em>tours</em> <span class="citation">(<a href="bibliography.html#ref-cook_grand_2008" role="doc-biblioref">Cook et al. 2008</a>; <a href="bibliography.html#ref-lee_state_2021" role="doc-biblioref">Lee et al. 2021</a>)</span>. Tours are viewed near-continuously through small changes to the projection basis. There are many variants of tours. We focus on one of the variants, <em>radial tours</em> <span class="citation">(<a href="bibliography.html#ref-cook_manual_1997" role="doc-biblioref">Cook and Buja 1997</a>; <a href="bibliography.html#ref-spyrison_spinifex_2020" role="doc-biblioref">Spyrison and Cook 2020</a>)</span>. Radial tours can be used to control or steer the projection basis, while other tour variants select bases randomly or optimize an objective function. Because of this unique attribute, <em>user interaction</em> is another key aspect of interest in this work.</p>
<!-- radial tour, RQ#1 -->
<p>Radial tours change the basis by selecting one variable and specifying how to change its contribution to the current projection. By controlling the contribution of a single variable, a user can explore its sensitivity to the structure of the projection and identify which variables are ultimately most important to the structure in question. In the work addressing RQ#1, we improve upon the geodesic interpolator for use in the radial tour, and apply it in an open-source <code>R</code> package, <code>spinifex</code>, this package facilitates making radial tours and extends the graphics packages interoperability for itself and the tours made from the existing package <code>tourr</code>.</p>
<!--user study, radial tour, RQ#2 -->
<p>Next, we substantiated the efficacy of radial tours as compared with user-selected, discrete combinations of principal components <span class="citation">(<a href="bibliography.html#ref-pearson_liii._1901" role="doc-biblioref">Pearson 1901</a>)</span> and continuous projections without interaction with the <em>grand tour</em> <span class="citation">(<a href="bibliography.html#ref-asimov_grand_1985" role="doc-biblioref">Asimov 1985</a>)</span>. We conducted an <span class="math inline">\(N=108\)</span> within-participant user study, where all participants use each of these visual factors. This is performed over balanced trials across the other experimental factors: location, shape, and dimension of the data. This addresses the second research question.</p>
<!-- XAI, SHAP values, and RQ#3 -->
<p>In our latest work, we want to see if we can apply the radial tour to aid the interpretability of black-box models. Such models have a non-linear space making them hard to interpret or understand. One recent branch in explainable artificial intelligence (XAI, <span class="citation"><a href="bibliography.html#ref-adadi_peeking_2018" role="doc-biblioref">Adadi and Berrada</a> (<a href="bibliography.html#ref-adadi_peeking_2018" role="doc-biblioref">2018</a>)</span>; <span class="citation"><a href="bibliography.html#ref-arrieta_explainable_2020" role="doc-biblioref">Arrieta et al.</a> (<a href="bibliography.html#ref-arrieta_explainable_2020" role="doc-biblioref">2020</a>)</span>) is the use of local explanations or the attribution of the variables for one observation of a black-box model. We use such an explanation to form a 1D basis and perform radial tours to explore how the SHAP values behave differently for misclassified observations against neighboring correctly classified observations.</p>
</div>
<div id="motivation" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Motivation</h2>
<!-- WHY/IMPORTANCE OF THESE TOPICS -->
<!-- EDA, visuals are important, numeric summarization insufficient -->
<p>The term exploratory data analysis (EDA) was coined by <span class="citation"><a href="bibliography.html#ref-tukey_exploratory_1977" role="doc-biblioref">Tukey</a> (<a href="bibliography.html#ref-tukey_exploratory_1977" role="doc-biblioref">1977</a>)</span>, who leaves it as an intentionally broad term that encompasses the initial summarization and visualization of a data set before a testing hypothesis has been formulated. This is a critical first step for understanding and becoming familiar with data and validating model assumptions. It may be tempting to review a series of summary statistics to check model assumptions. However, there are known datasets where the same summary statistics miss glaringly obvious visual patterns <span class="citation">(<a href="bibliography.html#ref-anscombe_graphs_1973" role="doc-biblioref">Anscombe 1973</a>; <a href="bibliography.html#ref-matejka_same_2017" role="doc-biblioref">Matejka and Fitzmaurice 2017</a>)</span>. It is easy to look at the wrong, or incomplete set of statistics needed to validate assumptions. Data visualization is crucial in EDA, it <em>forces</em> you to see details and peculiarities of the data which are opaque to numeric summarization, or more nefariously, obscure their true values. Data visualization does and must remain a primary component of data analysis and model validation.</p>
<!--interaction, wants citations -->
<p>studies/support –&gt;
While static documents are the norm, there are sizable benefits of user interaction. Interactive data visualization shift the locus of control back to the user, inviting them to explore and interact with the data, and offers a compact way to explore a wider range of dimensions, questions as they arise, which helped to keep the curiosity and the interest of the user.</p>
<!-- Black box, interprebility vs accuracy -->
<p>With the emerging field of XAI, the constant tension between the interpretability of a model and its predictive power is receiving more attention. Linear models are the champions of interpretability with modest accuracy while increasing complex models improve accuracy but they can scarcely be interpreted even by experienced practitioners. One way to gain insight into a model is to focus on the local vicinity of one observation, and explain the variable weighting around that location, in an agnostic non-linear model. We call this observation level variable weights a <em>local explanation</em>. There are various such local explanations, many are tied to specific classes of models, while others are model-agnostic.</p>
<!-- summary -->
<p>We know that data visualization is important in EDA and assumption validation. User interaction allows us to explore widely and quickly while allowing us to explore ideas as they arise. These 2 elements were especially important to consider from the work addressing RQ#1 as it forms a foundation to build on in further work. The efficacy of radial tours was supported by a user study in response to the second objective. In the latest work, we apply tours in tandem with local explanations to extend the interpretability of black-box models, to address RQ#3.</p>
</div>
<div id="extending-the-interpretation-of-black-box-models-with-the-use-of-interactive-continuous-linear-projections" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Extending the interpretation of black-box models with the use of interactive continuous linear projections</h2>
<p>Local explanations describe the linear variable weights in the vicinity of an observation in <span class="math inline">\(p-dim\)</span> space. Local explanations are point-measurements of the weights for each variable describing the importance to the prediction at that point. In a highly non-linear space, the importance of the variables may change rapidly with even a small change in the explanatory variables. There are several <em>model-agnostic</em> local explanations such as LIME <span class="citation">(<a href="bibliography.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>)</span>, and SHAP <span class="citation">(<a href="bibliography.html#ref-lundberg_unified_2017" role="doc-biblioref">Lundberg and Lee 2017</a>)</span>. In practice, this application can be used with any model and compatible local explanation. However, below we apply and discuss SHAP values extracted from a random forest model.</p>
<div id="how-a-shap-value-is-calculated" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> How a SHAP value is calculated</h3>
<p>We use FIFA soccer data <span class="citation">(<a href="bibliography.html#ref-leone_fifa_2020" role="doc-biblioref">Leone 2020</a>)</span> to explain SHAP values. We use 5000 player-observations of 9 aggregate skill measures to predict that player’s wages. We use SHAP to observe how the skill attribution changes for different players, which should reflect different fielding positions. Fielding position is not explicitly used in the model, but we would expect that SHAP finds a way to change the variable weights across differing skill distributions to improve the accuracy of its wage predictions.</p>
<p>We have trained a random forest model and wish to further explore the weightings of this non-linear model. Following the work in <span class="citation">(<a href="bibliography.html#ref-biecek_dalex_2018" role="doc-biblioref">Biecek 2018</a>; <a href="bibliography.html#ref-biecek_explanatory_2021" role="doc-biblioref">Biecek and Burzykowski 2021</a>)</span> we can similarly extract SHAP values, highlighting that different skills are valued differently across player positions within the model. We also show “break down” profiles, that is additive prediction explanations, how much of each player’s predicted wages is added by each of the skill evaluations. Figure <a href="4-ch-cheem.html#fig:ch5fig1">4.1</a> takes a look at the SHAP and break down profiles of a star offensive and defensive player.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch5fig1"></span>
<img src="figures/ch5_fig1_shap_distr_bd.png" alt="SHAP values and prediction explanations of an offensive player (Messi, top) and a defensive player (van Dijk). SHAP values show a change in weights at the location of each player. Break down profiles show one order-sensitive explanation for the prediction of that observation." width="100%"  />
<p class="caption">
Figure 4.1: SHAP values and prediction explanations of an offensive player (Messi, top) and a defensive player (van Dijk). SHAP values show a change in weights at the location of each player. Break down profiles show one order-sensitive explanation for the prediction of that observation.
</p>
</div>
<!-- Explanation of the wage, SHAP, and breakdown profiles. -->
<p>At the top of figure <a href="4-ch-cheem.html#fig:ch5fig1">4.1</a> we see the relative wages of the 2 different players. SHAP uses permutations of the X variables to approximate the variable weights. In the middle, we see the distributions of the variable weightings for each player across 25 such permutations. We use the median of those distributions (large dot) as the SHAP value. We see a difference in the weights across the 2 players that makes sense considering their positions. Reaction skills are important for both players, while offensive and movement are weighted higher of the offensive player. Conversely, defensive, power, and accuracy skills are weighted higher for the defensive player. The bottom used the SHAP values to explain their relative wage predictions. We see a similar trend of the SHAP values additively explaining the difference of the global intercept to their predictions. We would expect that other offensive players have explanations closer to Messi and defensive players to lie closer to van Dijk’s. The local explanations of middle fielders and goalkeepers would be expected to be different from both of these.</p>
</div>
<div id="our-application-trees-of-cheem" class="section level3" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Our application; Trees of Cheem</h3>
<!-- transistion to simulated data -->
<p>Above, the FIFA data was used to regress continuous wages. In introducing this application, <code>Trees of Cheem</code>, we will be discussing the classification of simulated data. We simulation of 3 spherical clusters on the vertices of a triangle. The difference between the clusters is contained in the first 2 dimensions with no mean separation in another 2 noise dimensions. After extracting all observations’ SHAP values, forming a SHAP <em>matrix</em>, of the original dimensionality, <span class="math inline">\((n~\times~p)\)</span>.</p>
<!-- global view; pc1:2 approximation -->
<p>We want to show a global view of the SHAP matrix and show how its sensitivity differs from that of the original data space. In figure <a href="4-ch-cheem.html#fig:ch5fig2">4.2</a> we create an approximate of the data- and SHAP-spaces with their first 2 principal components.</p>
<p>This gives an orientation showing where selected observation lies. We facilitate exploration and observation identification by adding a hovering tooltip displaying row number and class (actual &amp; predicted) with linked brushing highlighting selected points and displaying their data tabularly below the plot.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch5fig2"></span>
<img src="figures/ch5_fig2_global_space.png" alt="The first two principal conompents of the data- and SHAP-spaces (left and right respectively) of simulated data. The points are colored and shaped according to their predicted class, misclassified points are identified with a red circle. A target observation '*' is shown in contrast to a comparison point 'x', containing a nearby value in data space, but quite different SHAP space. These same 2 points are tracked in the proceeding tour." width="100%"  />
<p class="caption">
Figure 4.2: The first two principal conompents of the data- and SHAP-spaces (left and right respectively) of simulated data. The points are colored and shaped according to their predicted class, misclassified points are identified with a red circle. A target observation ’*’ is shown in contrast to a comparison point ‘x,’ containing a nearby value in data space, but quite different SHAP space. These same 2 points are tracked in the proceeding tour.
</p>
</div>
<p>Looking at the SHAP space (right) the bulk of the correctly classified points are clustered in relatively small areas. This means that the distribution of their SHAP values is quite similar; the model is selecting a very tight variable contribution to explain the predicted class of each observation. Conversely, misclassified points tend to lie in between 2 clusters of correctly classified points. Using the interactive brushing and hover tooltips we confirm that these points lie between the actual and predicted classes.</p>
<p>Given the global view shown in figure <a href="4-ch-cheem.html#fig:ch5fig2">4.2</a>, we want to look at the local explanation of primary and comparison points (highlighted as ’*/x’). In this case, the primary observation is misclassified while the comparison point is a correctly classified nearby point. These 2 points that are classified differently, but otherwise have very similar values in the explanatory variables, lie quite far apart in SHAP space.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch5fig3"></span>
<img src="figures/ch5_fig3_cheem_initial_bas.png" alt="Top: 1D projection basis, the normalized SHAP values of the select observation are shown as grey/black bars. The distribution of the other observations is shown with parallel coordinate lines. The primary and comparison points ('*/x' previous) are shown as bolder dashed lines and fainter dotted lines respectively. Bottom: the 1D density and rug marks of the current projection basis." width="100%"  />
<p class="caption">
Figure 4.3: Top: 1D projection basis, the normalized SHAP values of the select observation are shown as grey/black bars. The distribution of the other observations is shown with parallel coordinate lines. The primary and comparison points (’*/x’ previous) are shown as bolder dashed lines and fainter dotted lines respectively. Bottom: the 1D density and rug marks of the current projection basis.
</p>
</div>
<p>Now we have the global context and a comparison of the sensitivity of the SHAP spaces let’s turn more toward the target observations. Figure <a href="4-ch-cheem.html#fig:ch5fig3">4.3</a> highlights the same selected points, bolder dashed and fainter dotted lines (previously ’*/x’ points respectively). The top of the figure shows the selected observation’s SHAP values as grey/black bars. The other values of the SHAP matrix are displayed as short vertical bars and joining parallel coordinate lines and are similarly colored with their predicted class. The majority of the easy-to-classify points form a relatively tight spread in the first 2 variables (signal) and more chaotic behavior in the two noise variables as we would expect. Our dotted comparison lies with the bulk of the purple class. The dashed primary point is actually a green point misclassified as a purple, its weight for V1 looks like a green point, but its V2 weight deviates and is much closer to that of a purple. V2 is the variable that is crucial to the explanation of the model for why this particular point is misclassified. In the absence of V2, our target observation is weighted much closer to the that of its true class. This is the variable we want to explore the structure of in the radial tour as indicated with the black contribution bar.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch5fig4"></span>
<img src="figures/ch5_fig4_cheem_endpts.png" alt="The first frame of the radial tour. The SHAP values of the selected observation set the initial basis, shown as the grey and black bars on top. Within class distributions of the SHAP values are shown as parallel coordinate plots next to the variable contributions. The class densities and observation positions of the 1D projection are shown on the bottom. The tour animates over small changes in the basis (top bars) as the variable with the largest contribution (weight) is rotated to have a full contribution, zero contribution, and then back to the initial contribution. Variable 2 is critical to this local explanation; In the initial contribution, both selected observations are in the middle of the purple. With a full contribution of V2 typical of most purple, our primary observation is toward the tail. When there is no contribution of variable 2, the selected observation is in the middle of the green distribution, its true class." width="100%"  />
<p class="caption">
Figure 4.4: The first frame of the radial tour. The SHAP values of the selected observation set the initial basis, shown as the grey and black bars on top. Within class distributions of the SHAP values are shown as parallel coordinate plots next to the variable contributions. The class densities and observation positions of the 1D projection are shown on the bottom. The tour animates over small changes in the basis (top bars) as the variable with the largest contribution (weight) is rotated to have a full contribution, zero contribution, and then back to the initial contribution. Variable 2 is critical to this local explanation; In the initial contribution, both selected observations are in the middle of the purple. With a full contribution of V2 typical of most purple, our primary observation is toward the tail. When there is no contribution of variable 2, the selected observation is in the middle of the green distribution, its true class.
</p>
</div>
<p>Now that we have identified that V2 is the variable that deviated most from its true peer observation, we select it to rotate with the radial tour. In figure <a href="#fig:cheemTour"><strong>??</strong></a> illustrates the difference in the extremes of the contribution. The tour starts at the original contributions as described by the SHAP values (left frame). As the tour gives V2 a full contribution (middle frame) the comparison lies in the middle of the purples while the target observation is in the tails of both the purple and green (true class). In the right-most frame, the contribution from variable 2 is zero and the target variable lies in the middle of its true class. That is to say, without the contribution of variable 2 the local explanation does not seem reasonable; the selected observation looks much more like that of its true class.</p>
</div>
<div id="discussion" class="section level3" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Discussion</h3>
<p>We have used radial tours to improve the interpretability of black-box models. We display 2D approximations of the data to keep the global context in mind and facilitate the selection of points to focus on. After a target point have been identified, its SHAP values are used as the initial projection basis. Where the within-class distributions of the value values are displayed. The default variable selected contains the largest difference from the median of SHAP values within that point’s actual classification. That is that variable that deviates the most from what would be expected from a typical correctly classified observation.</p>
<p>Taking a step back, it is important to remember that this visualization and analysis is sensitive to the model and local explanation; it describes them as they are, independent of their validity and quality. The insight gleaned by this is predicated on meaningful selection of model and explanation.</p>
<p>Turning for a minute to real-world application, developing methods to better interpret black-box models is an important and impactful challenge. Private corporations and nation-states increasingly using complex models to classify and predict their customers and citizens from loans and insurance claims to employment and credit scores, the real-world impact of highly non-linear is here to stay. Being able to see the specifics of one observation may seem small in the context of a model, but it is crucial to that instance; if misclassified, that observation may receive an outcome a world apart from its actual peers.</p>
</div>
</div>
<div id="acknowledgments-3" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Acknowledgments</h2>
<p>I would like to thank Professor Przemyslaw Biecek (Warsaw University of Technology) for his time and input in suggesting to look at SHAP local explanations and try applying to the FIFA dataset.</p>
<p>This research was supported by an Australian government Research Training Program (RTP) scholarship. This article was created in <code>R</code> <span class="citation">(<a href="bibliography.html#ref-r_core_team_r:_2020" role="doc-biblioref">R Core Team 2020</a>)</span> and <code>rmarkdown</code> <span class="citation">(<a href="bibliography.html#ref-xie_r_2018" role="doc-biblioref">Xie, Allaire, and Grolemund 2018</a>)</span>.</p>
<p>For transparency and reproducibility, the source files are made available at <a href="https://github.com/nspyrison/phd_milestones">github.com/nspyrison/phd_milestones</a>.</p>
<!-- Adds a bib section at the end of every chapter -->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3-ch-efficacy_radial_tour.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="5-ch-conclusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
