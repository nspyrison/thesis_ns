<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Exploring Local Explanations of Non-linear Models Using Animated Linear Projections | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models</title>
  <meta name="description" content="Chapter 4 Exploring Local Explanations of Non-linear Models Using Animated Linear Projections | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Exploring Local Explanations of Non-linear Models Using Animated Linear Projections | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Exploring Local Explanations of Non-linear Models Using Animated Linear Projections | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models" />
  
  
  

<meta name="author" content="Nicholas S Spyrison" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="3-ch-userstudy.html"/>
<link rel="next" href="5-ch-conclusion.html"/>
<script src="libs/header-attrs-2.11.4/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="template/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./"> </a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#research-questions"><i class="fa fa-check"></i><b>1.1</b> Research questions</a></li>
<li class="chapter" data-level="1.2" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#methodology"><i class="fa fa-check"></i><b>1.2</b> Methodology</a></li>
<li class="chapter" data-level="1.3" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#contributions"><i class="fa fa-check"></i><b>1.3</b> Contributions</a></li>
<li class="chapter" data-level="1.4" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#ch-background"><i class="fa fa-check"></i><b>1.4</b> Background</a></li>
<li class="chapter" data-level="1.5" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#thesis-structure"><i class="fa fa-check"></i><b>1.5</b> Thesis structure</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html"><i class="fa fa-check"></i><b>2</b> spinifex: an R package for creating user-controlled animated linear projections</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#sec:algorithm"><i class="fa fa-check"></i><b>2.2</b> Algorithm</a></li>
<li class="chapter" data-level="2.3" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#sec:pkgstructure"><i class="fa fa-check"></i><b>2.3</b> Package structure</a></li>
<li class="chapter" data-level="2.4" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#sec:usecases"><i class="fa fa-check"></i><b>2.4</b> Use cases</a></li>
<li class="chapter" data-level="2.5" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#sec:discussion"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
<li class="chapter" data-level="2.6" data-path="2-ch-spinifex.html"><a href="2-ch-spinifex.html#acknowledgments-1"><i class="fa fa-check"></i><b>2.6</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ch-userstudy.html"><a href="3-ch-userstudy.html"><i class="fa fa-check"></i><b>3</b> A Study on the Benefit of a User-Controlled Radial Tour for Variable Importance for Structure in High-Dimensional Data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3-ch-userstudy.html"><a href="3-ch-userstudy.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="3-ch-userstudy.html"><a href="3-ch-userstudy.html#sec:background"><i class="fa fa-check"></i><b>3.2</b> Background</a></li>
<li class="chapter" data-level="3.3" data-path="3-ch-userstudy.html"><a href="3-ch-userstudy.html#sec:userstudy"><i class="fa fa-check"></i><b>3.3</b> User study</a></li>
<li class="chapter" data-level="3.4" data-path="3-ch-userstudy.html"><a href="3-ch-userstudy.html#sec:results"><i class="fa fa-check"></i><b>3.4</b> Results</a></li>
<li class="chapter" data-level="3.5" data-path="3-ch-userstudy.html"><a href="3-ch-userstudy.html#sec:conclusion"><i class="fa fa-check"></i><b>3.5</b> Conclusion</a></li>
<li class="chapter" data-level="3.6" data-path="3-ch-userstudy.html"><a href="3-ch-userstudy.html#sec:spinifex"><i class="fa fa-check"></i><b>3.6</b> Accompanying tool: radial tour application</a></li>
<li class="chapter" data-level="3.7" data-path="3-ch-userstudy.html"><a href="3-ch-userstudy.html#sec:acknowledgments"><i class="fa fa-check"></i><b>3.7</b> Acknowledgments</a></li>
<li class="chapter" data-level="3.8" data-path="3-ch-userstudy.html"><a href="3-ch-userstudy.html#references"><i class="fa fa-check"></i><b>3.8</b> References</a></li>
<li class="chapter" data-level="3.9" data-path="3-ch-userstudy.html"><a href="3-ch-userstudy.html#sec:appendix"><i class="fa fa-check"></i><b>3.9</b> Appendix</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html"><i class="fa fa-check"></i><b>4</b> Exploring Local Explanations of Non-linear Models Using Animated Linear Projections</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#sec:intro"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#sec:explanations"><i class="fa fa-check"></i><b>4.2</b> Local explanation statistics</a></li>
<li class="chapter" data-level="4.3" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#sec:tour"><i class="fa fa-check"></i><b>4.3</b> Tours and the radial tour</a></li>
<li class="chapter" data-level="4.4" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#sec:cheemviwer"><i class="fa fa-check"></i><b>4.4</b> The cheem viewer</a></li>
<li class="chapter" data-level="4.5" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#sec:casestudies"><i class="fa fa-check"></i><b>4.5</b> Case studies</a></li>
<li class="chapter" data-level="4.6" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#sec:cheemdiscussion"><i class="fa fa-check"></i><b>4.6</b> Discussion</a></li>
<li class="chapter" data-level="4.7" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#acknowledgments-2"><i class="fa fa-check"></i><b>4.7</b> Acknowledgments</a></li>
<li class="chapter" data-level="4.8" data-path="4-ch-cheem.html"><a href="4-ch-cheem.html#references-1"><i class="fa fa-check"></i><b>4.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-ch-conclusion.html"><a href="5-ch-conclusion.html"><i class="fa fa-check"></i><b>5</b> Conclusion</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5-ch-conclusion.html"><a href="5-ch-conclusion.html#contributions-1"><i class="fa fa-check"></i><b>5.1</b> Contributions</a></li>
<li class="chapter" data-level="5.2" data-path="5-ch-conclusion.html"><a href="5-ch-conclusion.html#summary-statement"><i class="fa fa-check"></i><b>5.2</b> Summary statement</a></li>
<li class="chapter" data-level="5.3" data-path="5-ch-conclusion.html"><a href="5-ch-conclusion.html#future-work"><i class="fa fa-check"></i><b>5.3</b> Future work</a></li>
</ul></li>
<li class="divider"></li>
<li><strong><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></strong></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-cheem" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Exploring Local Explanations of Non-linear Models Using Animated Linear Projections</h1>
<!-- Segue -->
<p>In the previous chapter I perform a within-participants user study. Now we can be more confident that the radial tour leads to better analysis of variable-level attribution to features identified in a projection. We want to increase the interpretability of complex models. Specifically, I suggest a using the radial tour to explore variable sensitivity to the structure identified in linear local explanations of non-linear models.</p>
<!-- Abstract -->
<p>The increased predictive power comes at the cost of interpretability, which has led to the emergence of eXplainable AI (XAI). XAI attempts to shed light on how models use predictors to arrive at a prediction with a point estimate of the linear feature importance in the vicinity of each instance. These can be considered linear projections and can be further explored interactively to understand better the interaction between features used to make predictions across the predictive model surface. Here we describe interactive linear interpolation used for exploration at any instance and illustrate with examples with categorical (penguin species, chocolate types) and quantitative (football salaries, house prices) response features. The methods are implemented in the <strong>R</strong> package <strong>cheem</strong>, available on CRAN.</p>
<div id="sec:intro" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Introduction</h2>
<!-- Introduce explanatory vs predictive modeling -->
<p>There are different reasons and emphases to fit a model. <span class="citation"><a href="3-ch-userstudy.html#ref-breiman_statistical_2001" role="doc-biblioref">Breiman</a> (<a href="3-ch-userstudy.html#ref-breiman_statistical_2001" role="doc-biblioref">2001</a>)</span>, reiterated by <span class="citation"><a href="3-ch-userstudy.html#ref-shmueli_explain_2010" role="doc-biblioref">Shmueli</a> (<a href="3-ch-userstudy.html#ref-shmueli_explain_2010" role="doc-biblioref">2010</a>)</span>, taxonomize modeling based on its purpose; <em>explanatory</em> modeling is done for some inferential purpose, while <em>predictive</em> modeling focuses more on the predictions of out-of-sample instances. The intended use has important implications for model selection and development. In explanatory modeling, interpretability is vital for drawing inferential conclusions. While predictive modeling may opt for more accurate non-linear models. The use of black-box models is becoming increasingly common, but not without their share of controversy <span class="citation">(<a href="3-ch-userstudy.html#ref-oneil_weapons_2016" role="doc-biblioref">O’Neil 2016</a>; <a href="3-ch-userstudy.html#ref-kodiyan_overview_2019" role="doc-biblioref">Kodiyan 2019</a>)</span>. However, the loss of interpretation presents a challenge.</p>
<!-- Interpretability & baises -->
<p>Interpretability is vital for exploring and protecting against potential biases (e.g., sex <span class="citation">(<a href="3-ch-userstudy.html#ref-dastin_amazon_2018" role="doc-biblioref">Dastin 2018</a>; <a href="3-ch-userstudy.html#ref-duffy_apple_2019" role="doc-biblioref">Duffy 2019</a>)</span>, race <span class="citation">(<a href="3-ch-userstudy.html#ref-larson_how_2016" role="doc-biblioref">Larson et al. 2016</a>)</span>, and age <span class="citation">(<a href="3-ch-userstudy.html#ref-diaz_addressing_2018" role="doc-biblioref">Díaz et al. 2018</a>)</span>) in any model. For instance, models regularly pick up on biases in the training data that have observed influence on the response (output) feature, which is then built into the model. Feature-level (variable-level) interpretability of models is essential in evaluating such biases. It is also generally important for many problems, where it is not enough to predict accurately. Still, one must be able to explain which predictors are most responsible for generating a response value.</p>
<!-- Interpretability & data drift -->
<p>Another concern is data drift, a shift in support or domain of the explanatory features (variable or predictors). Non-linear models are typically more sensitive and do not extrapolate well outside the training data domain. Better interpretability of the model means more transparency where models’ predictions may be plausible or completely unreliable.</p>
<!-- Local explanations -->
<p>Explainable Artificial Intelligence (XAI) is an emerging field of research that tries to increase the interpretability of black-box models. A common approach is to use <em>local explanations</em>, which attempt to approximate linear feature importance at the location of each instance (observation), or the predictions at a specific point in the data domain. Because these are point-specific, it is challenging to visualize them to comprehensively understand a model.</p>
<!-- Data visualization tours -->
<p>In multivariate data visualization, a <em>tour</em> <span class="citation">(<a href="3-ch-userstudy.html#ref-asimov_grand_1985" role="doc-biblioref">Asimov 1985</a>; <a href="3-ch-userstudy.html#ref-buja_grand_1986" role="doc-biblioref">Buja and Asimov 1986</a>; <a href="3-ch-userstudy.html#ref-lee_state_2021" role="doc-biblioref">S. Lee et al. 2021</a>)</span> is a sequence of linear projections of data onto a lower-dimensional space. Tours are viewed as an animation over minor changes to the projection basis. Structure in a projection can then be explored visually to see which features contribute to the formation of that structure. The intuition is similar to watching the shadow of a hidden 3D object change as the object is rotated; watching the shape of the shadow change conveys information about the structure and features of the object.</p>
<!-- Manual tours -->
<p>There are various types of tours distinguished by the generation of projection bases. In a <em>manual</em> tour <span class="citation">(<a href="3-ch-userstudy.html#ref-cook_manual_1997" role="doc-biblioref">Cook and Buja 1997</a>; <a href="3-ch-userstudy.html#ref-spyrison_spinifex_2020" role="doc-biblioref">Spyrison and Cook 2020</a>)</span>, the path is defined by changing the contribution of a selected feature. <!--tours and models --> Applying tours to models has been done in a couple of contexts. Specifically for exploring various statistical model fits and classification boundaries <span class="citation">(<a href="3-ch-userstudy.html#ref-wickham_visualizing_2015" role="doc-biblioref">Hadley Wickham, Cook, and Hofmann 2015</a>)</span>, and using tree- and forest-based approaches as a projection pursuit index to generate a tour basis paths <span class="citation">(<a href="3-ch-userstudy.html#ref-lee_pptree_2013" role="doc-biblioref">Y. D. Lee et al. 2013</a>; <a href="3-ch-userstudy.html#ref-da_silva_projection_2021" role="doc-biblioref">da Silva, Cook, and Lee 2021</a>)</span>.</p>
<!-- Purposed approach -->
<p>We use the radial manual tour to scrutinize a local explanation in our proposed approach. Additional interactivity allows the user to identify an instance of interest, then explore its local explanation by changing feature contribution with the radial tour. The methods are implemented in R package <strong>cheem</strong>. Example datasets are provided to illustrate usage for classification and regression tasks.</p>
<!-- What-if & counterfactual analysis -->
<p>Using a radial tour can be considered similar to counterfactual, what-if analysis, such as <em>ceteris paribus</em> <span class="citation">(<a href="3-ch-userstudy.html#ref-biecek_ceterisparibus_2020" role="doc-biblioref">Biecek 2020</a>)</span>. This phrase, Latin for “other things held constant” or “all else unchanged,” shows how an instance’s prediction would change from a marginal change in one explanatory feature given that other features are held constant. It ignores correlations of the features and imagines a case that was not observed. In contrast, our approach is a geometric explanation of the factual; it varies contributions of the features by rotating the basis, a reorientation of the data object. A constraint in our approach is that the basis must remain orthonormal. When the contribution of one feature decreases, the contributions of others necessarily increase such that there is a complete component in that direction. This also ensures that what is seen is strictly a low-dimensional projection from high-dimensions and is thus an interpretable visualization.</p>
<!-- Paper structure -->
<p>The remainder of this paper is organized as follows. The following Section, <a href="4-ch-cheem.html#sec:explanations">4.2</a>, covers the background of the local explanation and the traditional visuals produced. Section <a href="4-ch-cheem.html#sec:tour">4.3</a> explains the animations of continuous linear projections. Section <a href="4-ch-cheem.html#sec:cheemviwer">4.4</a> discusses the visual layout in the interactive interface, how they facilitate analysis, data preprocessing, and package infrastructure. Then Section <a href="4-ch-cheem.html#sec:casestudies">4.5</a> illustrates the application to supervised learning with categorical and quantitative response features. We conclude with Section <a href="4-ch-cheem.html#sec:cheemdiscussion">4.6</a> of the insights gained and directions that might be explored in the future.</p>
</div>
<div id="sec:explanations" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Local explanation statistics</h2>
<!-- Reminder of local explanation -->
<p>Consider a highly non-linear model. It can be hard to determine whether small changes in a feature’s value will make a class prediction change group or identify which features contribute to an extreme residual. Local explanations shed light on these situations by approximating linear feature importance in the vicinity of a single instance.</p>
<!-- Taxonomy of local explanations -->
<p>A comprehensive summary of the taxonomy and literature of explanation techniques is provided in Figure 6 of <span class="citation"><a href="3-ch-userstudy.html#ref-arrieta_explainable_2020" role="doc-biblioref">Arrieta et al.</a> (<a href="3-ch-userstudy.html#ref-arrieta_explainable_2020" role="doc-biblioref">2020</a>)</span>. It includes a large number of model-specific explanations such as deepLIFT <span class="citation">(<a href="3-ch-userstudy.html#ref-shrikumar_not_2016" role="doc-biblioref">Shrikumar et al. 2016</a>; <a href="3-ch-userstudy.html#ref-shrikumar_learning_2017" role="doc-biblioref">Shrikumar, Greenside, and Kundaje 2017</a>)</span>, a popular recursive method for estimating importance in neural networks. There are fewer model-agnostic explanations, of which LIME, <span class="citation">(<a href="3-ch-userstudy.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>)</span> SHAP, <span class="citation">(<a href="3-ch-userstudy.html#ref-lundberg_unified_2017" role="doc-biblioref">Lundberg and Lee 2017</a>)</span>, and their variants are popular.</p>
<!-- Uses of local explanations -->
<p>These instance-level explanations are used in various ways depending on the data. In image classification, where pixels correspond to predictors, saliency maps overlay or offset a heatmap indicating important pixels <span class="citation">(<a href="3-ch-userstudy.html#ref-simonyan_deep_2014" role="doc-biblioref">Simonyan, Vedaldi, and Zisserman 2014</a>)</span>. For instance, pixels corresponding to snow may be highlighted when distinguishing if a picture contains a wolf or husky. In text analysis, word-level contextual sentiment analysis can be used to highlight the sentiment and magnitude of influential words <span class="citation">(<a href="3-ch-userstudy.html#ref-vanni_textual_2018" role="doc-biblioref">Vanni et al. 2018</a>)</span>. In the case of numeric regression, they are used to explain feature additive contributions from the model intercept to the instance’s prediction <span class="citation">(<a href="3-ch-userstudy.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>)</span>.</p>
<!-- SHAP -->
<p>SHaply Additive exPlanations (SHAP) quantifies the feature contributions of one instance by examining the effect of other features on the predictions. The explanations of SHAP almost all refer to <span class="citation"><a href="3-ch-userstudy.html#ref-shapley_value_1953" role="doc-biblioref">Shapley</a> (<a href="3-ch-userstudy.html#ref-shapley_value_1953" role="doc-biblioref">1953</a>)</span>’s method to evaluate an individual’s contribution to cooperative games by assessing the performance of this player in the presence or absence of other players. <span class="citation"><a href="3-ch-userstudy.html#ref-strumbelj_efficient_2010" role="doc-biblioref">Strumbelj and Kononenko</a> (<a href="3-ch-userstudy.html#ref-strumbelj_efficient_2010" role="doc-biblioref">2010</a>)</span> introduced the use of SHAP for local explanations in ML models. The attribution of feature importance depends on the sequence of the features already included. The SHAP values are the mean contributions over different feature sequences. The approach is related to partial dependence plots <span class="citation">(<a href="3-ch-userstudy.html#ref-molnar_interpretable_2020" role="doc-biblioref">Molnar 2020</a>)</span>, used to explain the effect of a feature by predicting the response for a range of values on this feature after fixing the value of all other features to their mean. Partial dependence plots are a global approximation of the feature importance, while SHAP is specific to one instance. It could also be considered similar to examining the coefficients from all subsets regression, as described in <span class="citation"><a href="3-ch-userstudy.html#ref-wickham_visualizing_2015" role="doc-biblioref">Hadley Wickham, Cook, and Hofmann</a> (<a href="3-ch-userstudy.html#ref-wickham_visualizing_2015" role="doc-biblioref">2015</a>)</span>, which helps to understand the relative importance of each feature in the context of all other candidate features.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch5fig1"></span>
<img src="figures/ch5_fig1_shap_distr_bd.png" alt="Illustration SHAP values for a random forest model for salaries of FIFA 2020 players based on nine predictors corresponding to different skills. A star offensive and defensive player are compared, L. Messi and V. van Dijk, respectively. Panel (a) shows breakdown plots of three sequences of the features, order and magnitude change. Panel (b) shows the distribution of attribution for each feature across 25 sequences of predictors, with the mean displayed as a dot, for each players. Offense and movement are important for Messi but not van Dijk, and conversely, defense and power are important for van Dijk but not Messi." width="100%"  />
<p class="caption">
Figure 4.1: Illustration SHAP values for a random forest model for salaries of FIFA 2020 players based on nine predictors corresponding to different skills. A star offensive and defensive player are compared, L. Messi and V. van Dijk, respectively. Panel (a) shows breakdown plots of three sequences of the features, order and magnitude change. Panel (b) shows the distribution of attribution for each feature across 25 sequences of predictors, with the mean displayed as a dot, for each players. Offense and movement are important for Messi but not van Dijk, and conversely, defense and power are important for van Dijk but not Messi.
</p>
</div>
<!-- tree SHAP -->
<p>For our application, we use <em>tree SHAP</em>, a variant of SHAP that enjoys a lower computational complexity <span class="citation">(<a href="3-ch-userstudy.html#ref-lundberg_consistent_2018" role="doc-biblioref">Lundberg, Erion, and Lee 2018</a>)</span>. Instead of aggregating over sequences of the features, tree SHAP calculates instance-level feature importance by exploring the structure of the decision trees. Tree SHAP is only compatible with tree-based models; we illustrate random forests. The following section will use normalized SHAP values as a projection basis (call this the <em>attribution projection</em>) will have coefficients varied to further scrutinize the feature contributions.</p>
<!-- Fifa example -->
<p>Following the use case <em>Explanatory Model Analysis</em> <span class="citation">(<a href="3-ch-userstudy.html#ref-biecek_explanatory_2021" role="doc-biblioref">Biecek and Burzykowski 2021</a>)</span>, we use FIFA data to illustrate the use of SHAP. Consider soccer data from the FIFA 2020 season <span class="citation">(<a href="3-ch-userstudy.html#ref-leone_fifa_2020" role="doc-biblioref">Leone 2020</a>)</span>. There are 5000 instances of 9 skill measures (after aggregating highly correlated features). A random forest model is fit regressing wages [2020 Euros], from the skill measures. We then extract the SHAP values of a star offensive player (L. Messi) and defensive player (V. van Dijk). The results are displayed in Figure <a href="4-ch-cheem.html#fig:ch5fig1">4.1</a>. We expect to see a difference in the attribution of the feature importance across the two positions of the players, which would be interpreted as how the player’s salary depends on this combination of skill sets. Plot (b) is a modified breakdown plot <span class="citation">(<a href="3-ch-userstudy.html#ref-gosiewska_ibreakdown_2019" role="doc-biblioref">Gosiewska and Biecek 2019</a>)</span> where the order of features is fixed, so the two instances can be more easily compared.</p>
<!-- Segue -->
<p>In summary, these plots highlight how local explanations bring interpretability to a model, at least in the vicinity of their instances. In this instance, two players with different positions receive different profiles of feature importance to explain the prediction of their wages.</p>
</div>
<div id="sec:tour" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Tours and the radial tour</h2>
<!-- Tours intro -->
<p>A <em>tour</em> enables viewing of high-dimensional data by animating many linear projections with small incremental changes. It is achieved by following a path of linear projections (bases) of high-dimensional space. One of the features of the tour is the object permanence of the data points; one can track the relative change of instances in time and gain information about the relationships between points across multiple features. There are various types of tours that are distinguished by how the paths are generated <span class="citation">(<a href="3-ch-userstudy.html#ref-lee_state_2021" role="doc-biblioref">S. Lee et al. 2021</a>; <a href="3-ch-userstudy.html#ref-cook_grand_2008" role="doc-biblioref">Cook et al. 2008</a>)</span>.</p>
<!-- Manual tour -->
<p>The manual tour <span class="citation">(<a href="3-ch-userstudy.html#ref-cook_manual_1997" role="doc-biblioref">Cook and Buja 1997</a>)</span> defines its path by changing a selected feature’s contribution to a basis to allow the feature to contribute more or less to the projection. The requirement constrains the contribution of all other features that a basis needs to be orthonormal (column correspond to vectors, with unit length, and orthogonal to each other). The manual tour is primarily used to assess the importance of a feature to structure visible in a projection. It also lends itself to pre-computation queued in advance or computed on-the-fly for human-in-the-loop analysis <span class="citation">(<a href="3-ch-userstudy.html#ref-karwowski_international_2006" role="doc-biblioref">Karwowski 2006</a>)</span>. <!-- However, this navigation is relatively time-consuming due to the vast volume of the display space. It is advisable to use this method to explore the sensitivity of the feature contribution to a previously identified feature of interest. In this case, the projection of the normalized explanations is the feature of interest.--></p>
<!-- Radial tour -->
<p>A version of the manual tour called a <em>radial tour</em> is implemented in <span class="citation"><a href="3-ch-userstudy.html#ref-spyrison_spinifex_2020" role="doc-biblioref">Spyrison and Cook</a> (<a href="3-ch-userstudy.html#ref-spyrison_spinifex_2020" role="doc-biblioref">2020</a>)</span> and forms the basis of the new work. In a radial tour, the selected feature can change its magnitude of contribution but not its angle; it must move along the direction of its original contribution. The implementation allows for pre-computation and interactive re-calculation to focus on a different feature.</p>
</div>
<div id="sec:cheemviwer" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> The cheem viewer</h2>
<p>To explore the local explanations, an ensemble of plots <span class="citation">(<a href="3-ch-userstudy.html#ref-unwin_ensemble_2018" role="doc-biblioref">Unwin and Valero-Mora 2018</a>)</span> is provided, called the <em>cheem viewer</em>. There are two primary plots: the global view to give the context of all of the SHAP values, and the radial tour view to explore the local explanations with user-controlled rotation. In addition, there are numerous user inputs, including feature selection for the radial tour and instance selection for making comparisons. Figures <a href="4-ch-cheem.html#fig:ch5fig2">4.2</a> and <a href="4-ch-cheem.html#fig:ch5fig3">4.3</a> contain screenshots showing the cheem viewer for the two primary tasks: classification (categorical response) and regression (quantitative response).</p>
<div id="global-view" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Global view</h3>
<!-- Purpose -->
<p>The global view provides the context of all instances and facilitates the exploration of the separability of the data- and attribution-spaces. Both of these spaces are of dimension <span class="math inline">\(n\times p\)</span>, where <span class="math inline">\(n\)</span> is the number of instances and <span class="math inline">\(p\)</span> is the number of predictors. The attribution space corresponds to the local explanations for each instance, which will have <span class="math inline">\(p\)</span> values for each instance.</p>
<!-- Approximations of the spaces, PC1:2 -->
<p>A visualization of these spaces is provided by the first two principal components of their respective spaces. In addition, a plot observed by predicted response is also provided (Fig. <a href="4-ch-cheem.html#fig:ch5fig2">4.2</a>, b) A single 2D projection will not encompass all of the structure of higher-dimensional space, but it is generally a useful visual summary. <!-- Interactions --> For classification tasks, misclassified instances are circled in red if applicable. Linked brushing between the plots is provided, and a tabular display of selected points helps to facilitate exploration of the spaces and the model (shown in Fig. <a href="4-ch-cheem.html#fig:ch5fig2">4.2</a> a and c).</p>
<p>While the comparison of these spaces is interesting, the main purpose of the global view is to enable the selection of instances to explore the local explanations. The projection attribution of the Primary Instance (PI) is examined and typically viewed with an optional Comparison Instance (CI). These instances are highlighted as asterisk and <span class="math inline">\(\times\)</span> in 2D spaces and long dashed and short dotted lines on 1D spaces.</p>
</div>
<div id="radial-cheem-tour" class="section level3" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Radial cheem tour</h3>
<!-- Segue of PI to attribution projection -->
<p>The global view facilitated the selection of instances. The attribution projection of the PI is the initial 1D basis in a radial tour. This approximation of the feature importance for the prediction of the instance best explains the difference between the data mean and an instance’s prediction, not the local shape of the model surface.</p>
<!-- Tour start frame -->
<p>Normalized attributions of the PI and CI are shown as dashed and dotted lines against the attribution distribution from all instances. These are depicted as a vertical parallel coordinate plot, where each line connects one instance’s feature attribution (Fig. <a href="4-ch-cheem.html#fig:ch5fig2">4.2</a>, d). This is an important global context, the summarization of the attribution of all instances.</p>
<!-- Tour animation -->
<p>The current projection basis is depicted as the width of a bar, the feature contributions to the horizontal positions. The radial tour will vary one selected feature to a full contribution, zero contribution, and back to the attribution projection. Doing so tests a feature’s sensitivity to the structure identified by the local explanation. The default feature selected has the largest discrepancy between the attribution of the PI and CI. The data (scaled by standard deviation away from the mean) is projected through the current basis — the horizontal position of Fig. <a href="4-ch-cheem.html#fig:ch5fig2">4.2</a> e and Fig. <a href="4-ch-cheem.html#fig:ch5fig3">4.3</a> d.</p>
<p>The following sections elaborate on the difference in displays from applying this approach in classification and regression tasks. Now that we have introduced the global view and corresponding radial tour, let us discuss the differences between the classification and regression cases.</p>
</div>
<div id="classification-task" class="section level3" number="4.4.3">
<h3><span class="header-section-number">4.4.3</span> Classification task</h3>
<p>Typically we select a misclassified instance compared to a correctly classified point nearby in data space. The model information in the global view depicts the model confusion matrix. The radial tour is 1D, with a density display, while the goal of the tour; exploring the sensitivity of each feature to structure identified by the local explanation, evaluating the support or robustness of the prediction.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch5fig2"></span>
<img src="figures/ch5_fig2_app_classification.PNG" alt="Overview of the cheem viewer for classification tasks. Global view inputs, (a), set the PI, CI, and color statistic. Global view, (b) PC1 by PC2 for data space, attribution space, and prediction by observed y (visual of the confusion matrix for classification). Points are colored by predicted class, and red circles indicate misclassified instances. Radial tour inputs (c) select features to include and which feature is changed in the tour. The visual depicting 1D basis (d) shows the distribution of the feature attribution, and bars show the current basis. The black bar is being varied. Panel (e) is the resulting projection of the data indicated as density in the classification case." width="100%"  />
<p class="caption">
Figure 4.2: Overview of the cheem viewer for classification tasks. Global view inputs, (a), set the PI, CI, and color statistic. Global view, (b) PC1 by PC2 for data space, attribution space, and prediction by observed y (visual of the confusion matrix for classification). Points are colored by predicted class, and red circles indicate misclassified instances. Radial tour inputs (c) select features to include and which feature is changed in the tour. The visual depicting 1D basis (d) shows the distribution of the feature attribution, and bars show the current basis. The black bar is being varied. Panel (e) is the resulting projection of the data indicated as density in the classification case.
</p>
</div>
</div>
<div id="regression-task" class="section level3" number="4.4.4">
<h3><span class="header-section-number">4.4.4</span> Regression task</h3>
<p>Structure resolved in the attribution space can be highlighted by coloring points on a statistic. For this purpose, we include residuals, log Mahalanobis distance of data space (a measure of outlyingness), and the correlation of the attribution projection with the observed response. In the radial tour, the horizontal positions are the same, the basis projection of the radial tour. The vertical position is fixed to the observed response feature and residuals in the middle and right panels. Correspondingly, the display changes from univariate density to 2D scatterplot. The basis is still one component (horizontal) independent of the vertical position.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch5fig3"></span>
<img src="figures/ch5_fig3_app_regression_interactions.PNG" alt="Overview of the cheem viewer for regression task highlighting the differences from the classification task and interactive features. Panel (a) shows linked brushing across the global view and the tooltip display when the cursor hovers over an instance. Coloring on a statistic (b) highlights structure organized in the attribution space. Interactive tabular display (d) populates when instances are selected. Regression projection (e) uses the same horizontal projection and fixes the vertical positions to the observed y and residuals, (left and right)." width="100%"  />
<p class="caption">
Figure 4.3: Overview of the cheem viewer for regression task highlighting the differences from the classification task and interactive features. Panel (a) shows linked brushing across the global view and the tooltip display when the cursor hovers over an instance. Coloring on a statistic (b) highlights structure organized in the attribution space. Interactive tabular display (d) populates when instances are selected. Regression projection (e) uses the same horizontal projection and fixes the vertical positions to the observed y and residuals, (left and right).
</p>
</div>
</div>
<div id="interactive-features" class="section level3" number="4.4.5">
<h3><span class="header-section-number">4.4.5</span> Interactive features</h3>
<!-- Reactive vs exploration interactions -->
<p>The application has several reactive inputs that affect the data used, aesthetic display, and tour manipulation. These reactive inputs make the software flexible and extensible. The application also has more exploratory interactions to help link points across displays and reveal structure found in different spaces.</p>
<!-- Exploratory interactions -->
<p>A tooltip displays instance number/name and classification information while the cursor hovers over a point. Linked brushing allows the selection of points (left click and drag) where those points will be highlighted across plots. The information corresponding to the selected points is populated on a dynamic table. These interactions aid exploration of the spaces and, finally, identification of a primary and comparison instance.</p>
</div>
<div id="preprocessing" class="section level3" number="4.4.6">
<h3><span class="header-section-number">4.4.6</span> Preprocessing</h3>
<p>It is vital to mitigate the render time of visuals, especially when users may want to iterate many times. All computational operations should be prepared before runtime. The work remaining when an application is run solely reacts to inputs and rendering of visuals and tables. Below we discuss the steps and details of the reprocessing.</p>
(ref:citeRf) <span class="citation"><a href="3-ch-userstudy.html#ref-liaw_classification_2002" role="doc-biblioref">Liaw and Wiener</a> (<a href="3-ch-userstudy.html#ref-liaw_classification_2002" role="doc-biblioref">2002</a>)</span>
(ref:citeTs) <span class="citation"><a href="3-ch-userstudy.html#ref-kominsarczyk_treeshap_2021" role="doc-biblioref">Kominsarczyk et al.</a> (<a href="3-ch-userstudy.html#ref-kominsarczyk_treeshap_2021" role="doc-biblioref">2021</a>)</span>
<!-- Note on time of execution -->
<p>The time to preprocess the data will vary significantly with the model and local explanation. For reference, the FIFA data, 5000 instances of nine explanatory features, took 2.5 seconds to fit a random forest model of modest hyperparameters. Extracting the tree SHAP values of each instance took 270 seconds combined. PCA and statistics of the features and attributions took 2.8 seconds. These runtimes were from a non-parallelized R session on a modern laptop, but suffice to say that most of the time will be spent on the local attribution. An increase in model complexity or data dimensionality will quickly become an obstacle. Its reduced computational complexity makes tree SHAP a good candidate to start with. Alternatively, the package <strong>fastshap</strong> <span class="citation">(<a href="3-ch-userstudy.html#ref-greenwell_fastshap_2020" role="doc-biblioref">Greenwell 2020</a>)</span> claims extremely low runtimes, attributed to fewer calls to the prediction function, partial implementation in C++, and efficient use of logical subsetting.</p>
</div>
<div id="sec:infrastructure" class="section level3" number="4.4.7">
<h3><span class="header-section-number">4.4.7</span> Package infrastructure</h3>
<p>The above-described method and application are implemented as an open-source <strong>R</strong> package, <strong>cheem</strong> available on <a href="https://CRAN.R-project.org/package=cheem">CRAN</a>. Preprocessing was facilitated with models created via <strong>randomForest</strong> <span class="citation">(<a href="3-ch-userstudy.html#ref-liaw_classification_2002" role="doc-biblioref">Liaw and Wiener 2002</a>)</span> and explanations calculated with <strong>treeshap</strong> <span class="citation">(<a href="3-ch-userstudy.html#ref-kominsarczyk_treeshap_2021" role="doc-biblioref">Kominsarczyk et al. 2021</a>)</span>. The application was made with <strong>shiny</strong> <span class="citation">(<a href="3-ch-userstudy.html#ref-chang_shiny_2021" role="doc-biblioref">Chang et al. 2021</a>)</span>. The tour visual is built with <strong>spinifex</strong> <span class="citation">(<a href="3-ch-userstudy.html#ref-spyrison_spinifex_2020" role="doc-biblioref">Spyrison and Cook 2020</a>)</span>. Both views are created first with first with <strong>ggplot2</strong> <span class="citation">(<a href="3-ch-userstudy.html#ref-wickham_ggplot2_2016" role="doc-biblioref">Hadley Wickham 2016</a>)</span> and then rendered as interactive HTML widgets with <strong>plotly</strong> <span class="citation">(<a href="3-ch-userstudy.html#ref-sievert_interactive_2020" role="doc-biblioref">Sievert 2020</a>)</span>. <strong>DALEX</strong> <span class="citation">(<a href="3-ch-userstudy.html#ref-biecek_dalex_2018" role="doc-biblioref">Biecek 2018</a>)</span> and the free ebook, <em>Explanatory Model Analysis</em> <span class="citation">(<a href="3-ch-userstudy.html#ref-biecek_explanatory_2021" role="doc-biblioref">Biecek and Burzykowski 2021</a>)</span> were a huge boon to understanding local explanations and how to apply them.</p>
</div>
<div id="installation-and-getting-started" class="section level3" number="4.4.8">
<h3><span class="header-section-number">4.4.8</span> Installation and getting started</h3>
<p>The following <strong>R</strong> code will help with getting up and running:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="4-ch-cheem.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Download the package</span></span>
<span id="cb4-2"><a href="4-ch-cheem.html#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;cheem&quot;</span>, <span class="at">dependencies =</span> <span class="cn">TRUE</span>)</span>
<span id="cb4-3"><a href="4-ch-cheem.html#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Restart the R session, so the IDE has the correct directory structure</span></span>
<span id="cb4-4"><a href="4-ch-cheem.html#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="fu">restartSession</span>()</span>
<span id="cb4-5"><a href="4-ch-cheem.html#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Load cheem into session</span></span>
<span id="cb4-6"><a href="4-ch-cheem.html#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;cheem&quot;</span>)</span>
<span id="cb4-7"><a href="4-ch-cheem.html#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Try the app</span></span>
<span id="cb4-8"><a href="4-ch-cheem.html#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="fu">run_app</span>()</span>
<span id="cb4-9"><a href="4-ch-cheem.html#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="4-ch-cheem.html#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Processing your data</span></span>
<span id="cb4-11"><a href="4-ch-cheem.html#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Install treeshap from github to use as a local explainer</span></span>
<span id="cb4-12"><a href="4-ch-cheem.html#cb4-12" aria-hidden="true" tabindex="-1"></a>remotes<span class="sc">::</span><span class="fu">install_github</span>(<span class="st">&#39;ModelOriented/treeshap&#39;</span>)</span>
<span id="cb4-13"><a href="4-ch-cheem.html#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Follow the examples in cheem_ls()</span></span>
<span id="cb4-14"><a href="4-ch-cheem.html#cb4-14" aria-hidden="true" tabindex="-1"></a>?cheem_ls</span></code></pre></div>
</div>
</div>
<div id="sec:casestudies" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Case studies</h2>
<p>To illustrate the use of the cheem method, we apply it to modern datasets, two classification examples and then two of regression.</p>
<div id="palmer-penguin-species-classification" class="section level3" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Palmer penguin, species classification</h3>
<p>The Palmer penguins data <span class="citation">(<a href="3-ch-userstudy.html#ref-gorman_ecological_2014" role="doc-biblioref">Gorman, Williams, and Fraser 2014</a>; <a href="3-ch-userstudy.html#ref-horst_palmerpenguins_2020" role="doc-biblioref">Horst, Hill, and Gorman 2020</a>)</span> was collected on three species of penguins foraging near Palmer Station, Antarctica. The data was publicly available to substitute for the overly-used iris data and is quite similar in form. After removing incomplete instances, there are 333 instances and we will use the four physical measurements, <code>bill_length_mm</code> (<code>b_l</code>), <code>bill_depth_mm</code> (<code>b_d</code>), <code>flipper_length_mm</code> (<code>f_l</code>), <code>body_mass_g</code> (<code>wgt</code>), for this illustration. A random forest model was fit with species as the response feature.</p>
(ref:casepenguins) Examining the SHAP values for a random forest model classifying Palmer penguin species. The PI is an Chinstrap (orange) penguin that is misclassified as a Gentoo (purple), marked as an asterisk in (a), and the dashed vertical line in (b). The radial view shows varying the contribution of <code>f_l</code> from the initial attribution projection (b, left), which produces a linear combination where the PI is more probably a Chinstrap than a Gentoo (b, right). (The animation of the radial tour is at <a href="https://vimeo.com/666431172">vimeo.com/666431172</a>.)
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:casepenguins"></span>
<img src="figures/ch5_fig4_case_penguins.png" alt="(ref:casepenguins)" width="100%"  />
<p class="caption">
Figure 4.4: (ref:casepenguins)
</p>
</div>
<p>Figure <a href="4-ch-cheem.html#fig:casepenguins">4.4</a> shows plots from the cheem viewer for exploring the random forest model on the penguins data. Panel (a) shows the global view, and panel (b) shows several 1D projections generated with the radial tour. Penguin 243, a Gentoo (purple), is the PI because it has been misclassified as a Chinstrap (orange).</p>
(ref:casepenguinsblfl) Checking what is learned from the cheem viewer. This is a plot of flipper length (<code>f_l</code>) and bill length (<code>b_l</code>), where an asterisk highlights the PI. A Gentoo (purple) misclassified as a Chinstrap (orange). The PI has an unusually small <code>f_l</code> length which is why it is confused with a Chinstrap.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:casepenguinsblfl"></span>
<img src="figures/ch5_fig5_case_penguins_BlFl.png" alt="(ref:casepenguinsblfl)" width="60%"  />
<p class="caption">
Figure 4.5: (ref:casepenguinsblfl)
</p>
</div>
<p>There is more separation visible in the attribution space than the data space, as would be expected. The predicted vs observed plot reveals a handful of misclassified instances. We will explore why a Gentoo has been wrongly labeled as a Chinstrap for this illustration. The PI is a misclassified point (represented by the asterisk in the global view and as a dashed vertical line in the tour view). The CI is a correctly classified point (represented by an <span class="math inline">\(\times\)</span> and a vertical dotted line).</p>
<p>The radial tour starts from the attribution projection of the misclassified instance (b, left). The important features identified by SHAP in the (wrong) prediction for this instance are mostly <code>b_l</code> and <code>b_d</code> with small contributions of <code>f_l</code> and <code>wgt</code>. This projection is a view where the Gentoo (purple) looks much more likely for this instance than Chinstrap. That is, this combination of features is not particularly useful because the PI looks very much like other Gentoo penguins. To explore this, we use the radial tour to vary the contribution of flipper length (<code>f_l</code>). (In our exploration, this was the third feature explored. It is typically useful to explore the features with larger contributions, here <code>b_l</code> and <code>b_d</code>, but when doing this, nothing was revealed about how the PI differed from other Gentoos). On varying <code>f_l</code> as it contributes more to the projection (b, right), we see that more, and more, this penguin looks like a Chinstrap. This suggests that <code>f_l</code> should be considered an important feature for explaining the (wrong) prediction.</p>
<p>Figure <a href="4-ch-cheem.html#fig:casepenguinsblfl">4.5</a> confirms that flipper length (<code>f_l</code>) is important for the confusion of the PI as a Chinstrap. Here, flipper length and body length are plotted, and we can see that the PI is closer to the Chinstrap group in these two features, mostly because it has an unusually low value of flipper length relative to other Gentoos. From this view it makes sense that its a hard instance to account for as decision trees can only partition only vertical and horizontal lines.</p>
</div>
<div id="chocolates-milkdark-chocolate-classification" class="section level3" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> Chocolates, milk/dark chocolate classification</h3>
<p>The chocolates dataset consists of 88 instances of ten nutritional measurements determined from their labels and labeled as either milk or dark. Dark chocolate is considered healthier than milk. The data was collected by students during the Iowa State University class STAT503 from nutritional information from the manufacturer’s website and normalized to 100g equivalents. The data is available in the <strong>cheem</strong> package. A random forest model is used for the classification of chocolate type.</p>
<p>It could be interesting to examine the nutritional properties of any dark chocolates that have been misclassified as milk. A reason to do this is that a dark chocolate that is nutritionally more like milk should not be considered a healthy alternative. It is interesting to explore which of the nutritional features contribute most to misclassification.</p>
(ref:casechocolates) Chocolates data type classification (milk or dark). We select a chocolate labeled as dark though a random forest model predicts it to be milk chocolate from the values on the nutritional label. The attribution projection already looks more like a dark chocolate than milk. We remove four features with the lowest contribution for the selected instance and vary the contribution of Fiber. The misclassification seems improbable when sugar is near the max contribution. Animated tour can be found at <a href="https://vimeo.com/666431143">vimeo.com/666431143</a>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:casechocolates"></span>
<img src="figures/ch5_fig6_case_chocolates.png" alt="(ref:casechocolates)" width="100%"  />
<p class="caption">
Figure 4.6: (ref:casechocolates)
</p>
</div>
<p>This type of exploration is shown in Figure <a href="4-ch-cheem.html#fig:casechocolates">4.6</a>, where a chocolate labeled dark but predicted to be milk is chosen as the PI (instance 22). It is compared with a CI that is a correctly classified dark chocolate (instance 7). The PCA plot, and the SHAP PCA plots (a) show a big difference between the two chocolate types but with confusion for a handful of instances. The misclassifications are clearer in the observed vs predicted plot, and can be seen to be mistaken in both ways: milk to dark and dark to milk.</p>
<p>The attribution projection for chocolate 22 suggests that Fiber, Sugars and Calories are most responsible for its incorrect prediction. The way to read this plot is to see that Fiber has a large negative value, while Sugars and Calories have reasonably large positive values. In the density plot, instances on the very left of the display would have high values of Fiber (matching the negative projection coefficient) and low values of Sugars and Calories. The opposite would be the interpretation of a point with high values in this plot. The dark chocolates (orange) are mostly on the left, and this is a reason why they are considered to be healthier: high fiber and low sugar. The density for milk chocolates is further to the right, indicating that they generally have low fiber and high sugar.</p>
<p>The instance of interest (dashed line) can be viewed against the comparison instance (dotted line). Now one needs to pay different attention to the parallel plot of the SHAP values, which are local to a particular instance, and the density plot, which is the same projection of all instances as specified by the SHAP values of the instance of interest.</p>
<p>We can quickly compare the feature contributions to the two different predictions from the parallel coordinate plot. The instance of interest differs with the comparison primarily on the Fiber feature, which suggests that this is the reason for the incorrect prediction.</p>
<p>From the density plot, which is the attribution projection corresponding to the instance of interest, both instances are more like dark chocolates. If we vary the contribution of Sugars, and completely remove Sugars from the projection, this is where the difference becomes apparent. When primarily Fiber is examined, instance 22 looks more like a milk chocolate.</p>
(ref:casechocolatesinverse) Chocolates data type classification (milk or dark). Looking at the inverse misclassification, we select a milk chocolate while the model predicts it to be dark chocolate. Sodium and Fiber (<code>Na</code> and <code>Fbr</code>) have the largest differences in attributed feature importance. We remove features with the lowest contributions and vary the contribution of Sodium. This misclassification is not supported when sodium has contribution close to the attribution aligned with the other milk chocolates. Animated tour can be found at <a href="https://vimeo.com/666431143">vimeo.com/666431143</a>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:casechocolatesinverse"></span>
<img src="figures/ch5_fig7_case_chocolates_inverse.png" alt="(ref:casechocolatesinverse)" width="100%"  />
<p class="caption">
Figure 4.7: (ref:casechocolatesinverse)
</p>
</div>
<p>It would also be interesting to explore the inverse case; which features lead to a mild chocolate being misclassified as dark. Chocolate 84 is just this case, and we compare it with a correctly predicted milk chocolate (instance 71). This exploration is shown in Figure <a href="4-ch-cheem.html#fig:casechocolatesinverse">4.7</a>. From the parallel coordinate lines, we identify discrepancies in Sodium and Fiber. We opt to vary Sodium, and find the attribution of the other milk chocolates not to support the prediction of a dark chocolate.</p>
</div>
<div id="fifa-wage-regression" class="section level3" number="4.5.3">
<h3><span class="header-section-number">4.5.3</span> FIFA, wage regression</h3>
<p>The 2020 season FIFA data <span class="citation">(<a href="3-ch-userstudy.html#ref-leone_fifa_2020" role="doc-biblioref">Leone 2020</a>; <a href="3-ch-userstudy.html#ref-biecek_dalex_2018" role="doc-biblioref">Biecek 2018</a>)</span> contains many skill measurements of soccer/football players and wage information. After aggregation of the skill measurements, we regress player wages [2020 euros] given just the skill aggregates. The model was fit from 5000 instances of the nine skill aggregates before being thinned to 500 players to mitigate occlusion and render time. We compare a leading offensive fielder (L. Messi) with that of a top defensive fielder (V. van Dijk). The same instances were used in figure <a href="4-ch-cheem.html#fig:ch5fig1">4.1</a>.</p>
(ref:casefifa) FIFA 2020 data, a random forest model, regresses wages [2020 Euros] from nine aggregated skill measurements. The PI is a star offensive player (L. Messi) compared with a top defensive player (V. van Dijk). We remove three features with low attribution from both players. The attribution projection starts with the selected instance on the right. We vary the contribution from defense (<code>def</code>), the star offensive player is not distinguished in the horizontal direction. At this point, defensive players have been rotated to the highest horizontal value. The animate radial tour can be found at <a href="https://vimeo.com/666431163">vimeo.com/666431163</a>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:casefifa"></span>
<img src="figures/ch5_fig8_case_fifa.png" alt="(ref:casefifa)" width="100%"  />
<p class="caption">
Figure 4.8: (ref:casefifa)
</p>
</div>
<p>With figure <a href="4-ch-cheem.html#fig:casefifa">4.8</a>, we will test the premise of the local explanation. While offensive and reaction skills (<code>off</code> and <code>rct</code>) are both crucial to explaining a star offensive player. As the contribution of defensive skills increases, Messi’s is no longer separated from the group, and other defensive players are better predicted in this attribution case. In terms of what-if analysis, his predicted wages would be halved if Messi’s tree SHAP attributions at these levels.</p>
</div>
<div id="ames-housing-2018-sales-price-regression" class="section level3" number="4.5.4">
<h3><span class="header-section-number">4.5.4</span> Ames housing 2018, sales price regression</h3>
<p>Ames 2018, housing data was subset to North Ames (the neighborhood with the most house sales). The remaining are 338 house sales. A random forest model has regressed this price with the features Lot Area (<code>LtA</code>), Overall Quality (<code>Qlt</code>), Year the house was Build (<code>YrB</code>), Living Area (<code>LvA</code>), number of Bathrooms (<code>Bth</code>), number of Bedrooms (<code>Bdr</code>), total number of Rooms (<code>Rms</code>), Year the Garage was Build (<code>GYB</code>), and Garage Area (<code>GrA</code>). Using interaction from the global view, we select a house with an extreme negative residual and an accurate instance close to it in the data.</p>
(ref:caseames) Ames housing 2018 regressing sales price [USD]. The PI sale price was under predicted and had sizable attribution to lot area (<code>LtA</code>). The CI was predicted sales price was similar and much more accurate with its observed sales price while it has very little attribution to lot area. Varying the contribution lot area the separation between these house sales crosses when there is a low contribution of <code>LtA</code>, which is important to explaining the PI and near invariant to the sales price of the CI. The corresponding animation is at <a href="https://vimeo.com/666431134">vimeo.com/666431134</a>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:caseames"></span>
<img src="figures/ch5_fig9_case_ames2018.png" alt="(ref:caseames)" width="100%"  />
<p class="caption">
Figure 4.9: (ref:caseames)
</p>
</div>
<p>Figure <a href="4-ch-cheem.html#fig:caseames">4.9</a> selects the house sale 74, a sizable under prediction that has a large contribution from lot area. The CI has a similar predicted price though the prediction was accurate and gives almost no attribution to lot size. The attribution projection is places instances with high living areas to the right. We control the contribution of this feature. As the contribution of lot area decreases, the predictive power decreases for the PI, while the CI remains stationary. This large of an importance is living area is relatively uncommon. Boosting tree models may be more resilient to such an under prediction as up-weighting this residual would force its inclusion in the final model.</p>
</div>
</div>
<div id="sec:cheemdiscussion" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Discussion</h2>
<p>The need to maintain the interpretability of black-box models is evident. One aspect uses local explanations of the model in the vicinity of an instance. Local explanations approximate the linear feature importance to the model. Our contribution is to assess explanations by examining the support by varying the contributions with a radial tour. First, a global view visualizes approximations of the data space, explanation space, model predictions side-by-side, using dynamic interaction to compare and contrast and identify instances of interest. The normalized linear importance from the explanation of the PI becomes the feature of interest to further explore with the radial tour. The tours explore the feature sensitivity to the structure identified in the explanation.</p>
<p>We have illustrated this method on random forest models using the tree SHAP local explanation, while it could be generally used with any compatible model-explanation pairing. We apply it to the classification and regression tasks. We have created an open-source <strong>R</strong> package <strong>cheem</strong>, available on <a href="https://CRAN.R-project.org/package=cheem">CRAN</a>, to facilitate preprocessing and exploration with the described interactive application. Toy and real data are provided, or upload your data after preprocessing.</p>
</div>
<div id="acknowledgments-2" class="section level2" number="4.7">
<h2><span class="header-section-number">4.7</span> Acknowledgments</h2>
<p>We would like to thank Professor Przemyslaw Biecek for his input early in the project and to the broader <span class="math inline">\(\text{MI}^\text2\)</span> lab group for the <strong>DALEX</strong> ecosystem of <strong>R</strong> and <strong>Python</strong> packages. This research was supported by Australian Government Research Training Program (RTP) scholarships. Thanks to Jieyang Chong for helping proofread this article.</p>
<p>The namesake, Cheem, refers to a fictional race of humanoid trees from Doctor Who lore. <strong>DALEX</strong> pulls on from that universe, and we initially apply tree SHAP explanations specific to tree-based models.</p>
</div>
<div id="references-1" class="section level2" number="4.8">
<h2><span class="header-section-number">4.8</span> References</h2>
<!-- Adds a bib section at the end of every chapter -->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3-ch-userstudy.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="5-ch-conclusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
