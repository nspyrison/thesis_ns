<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Exploring Local Explanations of Nonlinear Models Using the Radial tour | Interactive and dynamic visualization of high-dimensional data via animated linear projections</title>
  <meta name="description" content="Chapter 5 Exploring Local Explanations of Nonlinear Models Using the Radial tour | Interactive and dynamic visualization of high-dimensional data via animated linear projections" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Exploring Local Explanations of Nonlinear Models Using the Radial tour | Interactive and dynamic visualization of high-dimensional data via animated linear projections" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Exploring Local Explanations of Nonlinear Models Using the Radial tour | Interactive and dynamic visualization of high-dimensional data via animated linear projections" />
  
  
  

<meta name="author" content="Nicholas S Spyrison" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="4-ch-userstudy.html"/>
<link rel="next" href="6-ch-conclusion.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="template/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./"> </a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#research-questions"><i class="fa fa-check"></i><b>1.1</b> Research questions</a></li>
<li class="chapter" data-level="1.2" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#methodology"><i class="fa fa-check"></i><b>1.2</b> Methodology</a></li>
<li class="chapter" data-level="1.3" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#contributions"><i class="fa fa-check"></i><b>1.3</b> Contributions</a></li>
<li class="chapter" data-level="1.4" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#thesis-structure"><i class="fa fa-check"></i><b>1.4</b> Thesis structure</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-background.html"><a href="2-ch-background.html"><i class="fa fa-check"></i><b>2</b> Background</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2-ch-background.html"><a href="2-ch-background.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="2-ch-background.html"><a href="2-ch-background.html#multivariate-visualization"><i class="fa fa-check"></i><b>2.2</b> Multivariate visualization</a></li>
<li class="chapter" data-level="2.3" data-path="2-ch-background.html"><a href="2-ch-background.html#linear-projections"><i class="fa fa-check"></i><b>2.3</b> Linear projections</a></li>
<li class="chapter" data-level="2.4" data-path="2-ch-background.html"><a href="2-ch-background.html#evaluating-multivariate-data-visualization"><i class="fa fa-check"></i><b>2.4</b> Evaluating multivariate data visualization</a></li>
<li class="chapter" data-level="2.5" data-path="2-ch-background.html"><a href="2-ch-background.html#nonlinear-models"><i class="fa fa-check"></i><b>2.5</b> Nonlinear models</a></li>
<li class="chapter" data-level="2.6" data-path="2-ch-background.html"><a href="2-ch-background.html#sec:explanations"><i class="fa fa-check"></i><b>2.6</b> Local explanations</a></li>
<li class="chapter" data-level="2.7" data-path="2-ch-background.html"><a href="2-ch-background.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html"><i class="fa fa-check"></i><b>3</b> A User-Controlled Manual Tour for Animated Linear Projections</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:algorithm"><i class="fa fa-check"></i><b>3.1</b> Algorithm</a></li>
<li class="chapter" data-level="3.2" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#oblique-cursor-movement"><i class="fa fa-check"></i><b>3.2</b> Oblique cursor movement</a></li>
<li class="chapter" data-level="3.3" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:pkgstructure"><i class="fa fa-check"></i><b>3.3</b> Package structure</a></li>
<li class="chapter" data-level="3.4" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:usecases"><i class="fa fa-check"></i><b>3.4</b> Use cases</a></li>
<li class="chapter" data-level="3.5" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:discussion"><i class="fa fa-check"></i><b>3.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html"><i class="fa fa-check"></i><b>4</b> A Study on the Benefit of a User-Controlled Radial Tour for Variable Importance for Structure in High-Dimensional Data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:userstudy"><i class="fa fa-check"></i><b>4.1</b> User study</a></li>
<li class="chapter" data-level="4.2" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:results"><i class="fa fa-check"></i><b>4.2</b> Results</a></li>
<li class="chapter" data-level="4.3" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:conclusion"><i class="fa fa-check"></i><b>4.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html"><i class="fa fa-check"></i><b>5</b> Exploring Local Explanations of Nonlinear Models Using the Radial tour</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:shap"><i class="fa fa-check"></i><b>5.1</b> SHAP and tree SHAP local explanations</a></li>
<li class="chapter" data-level="5.2" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:cheemviwer"><i class="fa fa-check"></i><b>5.2</b> The cheem viewer</a></li>
<li class="chapter" data-level="5.3" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:casestudies"><i class="fa fa-check"></i><b>5.3</b> Case studies</a></li>
<li class="chapter" data-level="5.4" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:cheemdiscussion"><i class="fa fa-check"></i><b>5.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html"><i class="fa fa-check"></i><b>6</b> Conclusion</a>
<ul>
<li class="chapter" data-level="6.1" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html#contributions-1"><i class="fa fa-check"></i><b>6.1</b> Contributions</a></li>
<li class="chapter" data-level="6.2" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html#limitations"><i class="fa fa-check"></i><b>6.2</b> Limitations</a></li>
<li class="chapter" data-level="6.3" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html#future-work"><i class="fa fa-check"></i><b>6.3</b> Future work</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-supplementary-material-for-radial-tour-user-study.html"><a href="A-supplementary-material-for-radial-tour-user-study.html"><i class="fa fa-check"></i><b>A</b> Supplementary material for radial tour user study</a>
<ul>
<li class="chapter" data-level="A.1" data-path="A-supplementary-material-for-radial-tour-user-study.html"><a href="A-supplementary-material-for-radial-tour-user-study.html#sec:spinifex"><i class="fa fa-check"></i><b>A.1</b> Accompanying radial tour application</a></li>
<li class="chapter" data-level="A.2" data-path="A-supplementary-material-for-radial-tour-user-study.html"><a href="A-supplementary-material-for-radial-tour-user-study.html#sec:extanalysis"><i class="fa fa-check"></i><b>A.2</b> Extended analysis</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="B-overall-appendix.html"><a href="B-overall-appendix.html"><i class="fa fa-check"></i><b>B</b> Overall appendix</a>
<ul>
<li class="chapter" data-level="B.1" data-path="B-overall-appendix.html"><a href="B-overall-appendix.html#sec-glossary"><i class="fa fa-check"></i><b>B.1</b> Glossary</a></li>
<li class="chapter" data-level="B.2" data-path="B-overall-appendix.html"><a href="B-overall-appendix.html#animated-tour-links"><i class="fa fa-check"></i><b>B.2</b> Animated tour links</a></li>
<li class="chapter" data-level="B.3" data-path="B-overall-appendix.html"><a href="B-overall-appendix.html#supplementary-material"><i class="fa fa-check"></i><b>B.3</b> Supplementary material</a></li>
</ul></li>
<li class="divider"></li>
<li><strong><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></strong></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interactive and dynamic visualization of high-dimensional data via animated linear projections</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-cheem" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Exploring Local Explanations of Nonlinear Models Using the Radial tour</h1>
<!-- Segue -->
<p>The previous chapter discussed the within-participants user study comparing PCA, the grand tour, and the radial tour in a supervised variable attribution task. There was strong evidence that the radial tour led to a large increase in accuracy. The analyst can be more confident that the radial tour leads to better analysis of variable-level attribution to features identified in a projection.</p>
<p>Given the interpretability crisis of nonlinear models, it would be interesting to see if the radial tour can help. Specifically, we will investigate using the radial tour to explore variable sensitivity to the structure identified in linear local explanations of nonlinear models. That is, under what range of variable importance does an explanation make sense, and what contributions fail to be supported the prediction. This can be used to examine which variables lead to the misclassification of observation or an otherwise extreme residual. The radial tour can also test how susceptible a variable’s contribution is to discerning the predictions of two observations.</p>
<!-- Abstract -->
<p>The increased predictive power comes at the cost of interpretability, which has led to the emergence of eXplainable AI (XAI). XAI attempts to shed light on how models use predictors to arrive at a prediction with a point estimate of the linear feature importance in the vicinity of each instance. These can be considered linear projections and can be further explored interactively to understand better the interaction between features used to make predictions across the predictive model surface. Here we describe interactive linear interpolation used for exploration at any instance and illustrate with examples with categorical (penguin species, chocolate types) and quantitative (football salaries, house prices) output. The methods are implemented in the <strong>R</strong> package <strong>cheem</strong>, available on CRAN.</p>
<!-- ## Introduction {#sec:intro} -->
<!-- Segue -->
<p>Chapter <a href="2-ch-background.html#ch-background">2</a> introduced predictive modeling, the interpretability crisis of nonlinear models, and local explanations — approximations of linear variable importance in the vicinity of one observation.<!-- chapter structure --> The remainder of this chapter is organized as follows. Section <a href="5-ch-cheem.html#sec:shap">5.1</a> induces the SHAP and tree SHAP local explanation. Section <a href="2-ch-background.html#sec:tour">2.3.1</a> explains the animations of continuous linear projections. Section <a href="5-ch-cheem.html#sec:cheemviwer">5.2</a> discusses the visual layout in the interactive interface, how it facilitate analysis, data preprocessing, and package infrastructure. Then Section <a href="5-ch-cheem.html#sec:casestudies">5.3</a> illustrates the application to supervised learning with categorical and quantitative output. Section <a href="5-ch-cheem.html#sec:cheemdiscussion">5.4</a> concludes with the insights gained and possible directions to explored in the future.</p>
<div id="sec:shap" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> SHAP and tree SHAP local explanations</h2>
<!-- SHAP -->
<p>SHaply Additive exPlanations (SHAP) quantifies the feature contributions of one instance by examining the effect of other features on the predictions. The explanations of SHAP almost all refer to <span class="citation"><a href="bibliography.html#ref-shapley_value_1953" role="doc-biblioref">Shapley</a> (<a href="bibliography.html#ref-shapley_value_1953" role="doc-biblioref">1953</a>)</span>’s method to evaluate an individual’s contribution to cooperative games by assessing this player’s performance in the presence or absence of other players. <span class="citation"><a href="bibliography.html#ref-strumbelj_efficient_2010" role="doc-biblioref">Strumbelj and Kononenko</a> (<a href="bibliography.html#ref-strumbelj_efficient_2010" role="doc-biblioref">2010</a>)</span> introduced SHAP for local explanations in ML models. The attribution of feature importance depends on the sequence of the included features. The SHAP values are the mean contributions over different feature sequences. The approach is related to partial dependence plots <span class="citation">(<a href="bibliography.html#ref-molnar_interpretable_2020" role="doc-biblioref">Molnar 2020</a>)</span>, used to explain the effect of a feature by predicting the response for a range of values on this feature after fixing the value of all other features to their mean. Partial dependence plots are a global approximation of the feature importance, while SHAP is specific to one instance. It could also be considered similar to examining the coefficients from all subsets regression, as described in <span class="citation"><a href="bibliography.html#ref-wickham_visualizing_2015" role="doc-biblioref">Hadley Wickham, Cook, and Hofmann</a> (<a href="bibliography.html#ref-wickham_visualizing_2015" role="doc-biblioref">2015</a>)</span>, which helps to understand the relative importance of each feature in the context of all other candidate features.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:shapdistrbd"></span>
<img src="figures/ch5_fig1_shap_distr_bd.png" alt="Illustration SHAP values for a random forest model FIFA 2020 player wages from nine skill predictors. A star offensive and defensive player are compared, L. Messi and V. van Dijk, respectively. Panel (a) shows breakdown plots of three sequences of the features. The sequence of the variables impacts the magnitude of their attribution. Panel (b) shows the distribution of attribution for each feature across 25 sequences of predictors, with the mean displayed as a dot for each player. Offense and movement are important for Messi but not van Dijk, and conversely, defense and power are important for van Dijk but not Messi." width="100%"  />
<p class="caption">
Figure 5.1: Illustration SHAP values for a random forest model FIFA 2020 player wages from nine skill predictors. A star offensive and defensive player are compared, L. Messi and V. van Dijk, respectively. Panel (a) shows breakdown plots of three sequences of the features. The sequence of the variables impacts the magnitude of their attribution. Panel (b) shows the distribution of attribution for each feature across 25 sequences of predictors, with the mean displayed as a dot for each player. Offense and movement are important for Messi but not van Dijk, and conversely, defense and power are important for van Dijk but not Messi.
</p>
</div>
<!-- tree SHAP -->
<p>For the application, we use <em>tree SHAP</em>, a variant of SHAP that enjoys a lower computational complexity <span class="citation">(<a href="bibliography.html#ref-lundberg_consistent_2018" role="doc-biblioref">Lundberg, Erion, and Lee 2018</a>)</span>. Instead of aggregating over sequences of the features, tree SHAP calculates instance-level feature importance by exploring the structure of the decision trees. Tree SHAP is only compatible with tree-based models; random forests are used for illustration. The following section will use normalized SHAP values as a projection basis (call this the <em>attribution projection</em>) will have coefficients to scrutinize the feature contributions further.</p>
<!-- Fifa example -->
<p>Following the use case <em>Explanatory Model Analysis</em> <span class="citation">(<a href="bibliography.html#ref-biecek_explanatory_2021" role="doc-biblioref">Biecek and Burzykowski 2021</a>)</span>, FIFA data illustrates SHAP use. Consider soccer data from the FIFA 2020 season <span class="citation">(<a href="bibliography.html#ref-leone_fifa_2020" role="doc-biblioref">Leone 2020</a>)</span>. There are 5000 instances of 9 skill measures (after aggregating highly correlated features). A random forest model is fit, regressing player’s wages [2020 Euros] from their skill measurements. The SHAP values are compared for a star offensive player (L. Messi) and defensive player (V. van Dijk). The results are displayed in Figure <a href="5-ch-cheem.html#fig:shapdistrbd">5.1</a>. A difference in the attribution of the feature importance across the two positions of the players can be expected. This would be interpreted as how a player’s salary depends on this combination of skill sets. Plot (b) is a modified breakdown plot <span class="citation">(<a href="bibliography.html#ref-gosiewska_ibreakdown_2019" role="doc-biblioref">Gosiewska and Biecek 2019</a>)</span> where the order of features is fixed, so the two instances can be more easily compared.</p>
<!-- Segue -->
<p>In summary, these plots highlight how local explanations bring interpretability to a model, at least in the vicinity of their instances. In this instance, two players with different positions receive different profiles of feature importance to explain the prediction of their wages.</p>
<!-- ## Tours and the radial tour {#sec:tour} -->
<!-- Moved to ch2-background -->
</div>
<div id="sec:cheemviwer" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> The cheem viewer</h2>
<p>To explore the local explanations, an ensemble of plots <span class="citation">(<a href="bibliography.html#ref-unwin_ensemble_2018" role="doc-biblioref">Unwin and Valero-Mora 2018</a>)</span> is provided, called the <em>cheem viewer</em>. There are two primary plots: the <strong>global view</strong> to give the context of all of the SHAP values and the <strong>radial tour view</strong> to explore the local explanations with user-controlled rotation. In addition, there are numerous user inputs, including feature selection for the radial tour and instance selection for making comparisons. There are different plots used for the categorical and quantitative responses. Figures <a href="5-ch-cheem.html#fig:classificationcase">5.2</a> and <a href="5-ch-cheem.html#fig:regressioncase">5.3</a> are screenshots showing the cheem viewer for the two primary tasks: classification (categorical response) and regression (quantitative response).</p>
<div id="global-view" class="section level3" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Global view</h3>
<!-- Purpose -->
<p>The global view provides context of all instances and facilitates the exploration of the separability of the data- and attribution-spaces. Both of these spaces are of dimension <span class="math inline">\(n\times p\)</span>, where <span class="math inline">\(n\)</span> is the number of instances and <span class="math inline">\(p\)</span> is the number of predictors. The attribution space corresponds to the local explanations for each instance, each a vector of <span class="math inline">\(p\)</span> values.</p>
<!-- Approximations of the spaces, PC1:2 -->
<p>A visualization is provided by the first two principal components of the data (left) and the attribution (middle) spaces. These single 2D projections will not reveal all of the structure of higher-dimensional space, but they are helpful visual summaries. In addition, a plot of the observed against predicted response values is also provided (Figures <a href="5-ch-cheem.html#fig:classificationcase">5.2</a>b, <a href="5-ch-cheem.html#fig:regressioncase">5.3</a>a) to help identify instances poorly predicted by the model. <!-- Interactions --> For classification tasks, misclassified instances are circled in red if applicable. Linked brushing between the plots is provided, and a tabular display of selected points helps to facilitate exploration of the spaces and the model (shown in Figures <a href="5-ch-cheem.html#fig:classificationcase">5.2</a>a,c).</p>
<p>While the comparison of these spaces is interesting, the primary purpose of the global view is to enable the selection of instances to explore the local explanations. The projection attribution of the primary instance (PI) is examined and typically viewed with an optional comparison instance (CI). These instances are highlighted as asterisk and <span class="math inline">\(\times\)</span>, respectively.</p>
</div>
<div id="radial-tour" class="section level3" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Radial tour</h3>
<!-- Local explanations as par coords -->
<p>The local explanations for all observations are normalized (squared sum of values adds to 1), and thus, the relative importance of features can be compared across all instances. These are depicted as a vertical parallel coordinate plot, where each line connects one instance’s feature attribution (Figures <a href="5-ch-cheem.html#fig:classificationcase">5.2</a>e and <a href="5-ch-cheem.html#fig:regressioncase">5.3</a>e). The attribution projections of the PI and CI are shown as dashed and dotted lines, respectively. From this plot, the range of importances across all instances can be interpreted. For classification, one would look at differences between groups on any feature. For example, Figure <a href="5-ch-cheem.html#fig:classificationcase">5.2</a>e suggests that <code>bl</code> is important for distinguishing the green class from the other two. For regression, one might generally observe which features have low values for all instances (not important), for example, <code>BMI</code> and <code>pwr</code> in Figure <a href="5-ch-cheem.html#fig:regressioncase">5.3</a>e, and which have a range of high and low values (e.g. <code>off</code>, <code>def</code>) suggesting they important for some instances and not important for other instances.</p>
<!-- Segue of PI to attribution projection -->
<p>The overlaid bars on the parallel coordinate plot represent the attribution projection of the PI. (Remember that the PI is interactively selected from the global view). The attribution projection is an approximation of the feature importance for predicting this instance. This combination of features best explains the difference between the mean response and an instance’s predicted value. It is not an indication of the local shape of the model surface. That is, it is not some indication of the tangent to the curve at this point.</p>
<!-- Tour animation -->
<p>The attribution projection of the PI is the initial 1D basis in a radial tour, displayed as a density plot for a categorical response (Figure <a href="5-ch-cheem.html#fig:classificationcase">5.2</a>f) and as scatterplots for a quantitative response (Figure <a href="5-ch-cheem.html#fig:regressioncase">5.3</a>f). The PI and CI are indicated by vertical dashed and dotted lines, respectively. The user uses the radial tour to vary the contribution of the selected feature between 0-1. Doing so tests the sensitivity of structure (class separation or strength of relationship) to the feature’s contribution. For classification, if the separation between classes diminishes when the feature contribution is reduced, this suggests that the feature is important for class separation. For regression, if the relationship scatterplot weakens when the feature contribution is reduced, then this indicates that the feature is important for accurately predicting the response.</p>
</div>
<div id="classification-task" class="section level3" number="5.2.3">
<h3><span class="header-section-number">5.2.3</span> Classification task</h3>
<p>Selecting a misclassified instance as PI and a correctly classified point nearby in data space as CI makes it easier to examine the features most responsible for the error. The global view (Figure <a href="5-ch-cheem.html#fig:classificationcase">5.2</a>c) displays the model confusion matrix. The radial tour is 1D and display as density where color indicates class. A scroll bar here enables the user to vary the contribution of each feature to explore the sensitivity of the separation to that feature.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:classificationcase"></span>
<img src="figures/ch5_fig2_app_classification.PNG" alt="Overview of the cheem viewer for classification tasks. Global view inputs, (a), set the PI, CI, and color statistic. Global view, (b) PC1 by PC2 approximations of the data space and attribution space. (c) prediction by observed y (visual of the confusion matrix for classification tasks). Points are colored by predicted class, and red circles indicate misclassified instances. Radial tour inputs (d) select features to include and which feature is changed in the tour. (e) shows a parallel coordinate display of the distribution of the feature attributions while bars depict contribution for the current basis. The black bar is the variable being changed in the radial tour. Panel (f) is the resulting data projection indicated as density in the classification case." width="100%"  />
<p class="caption">
Figure 5.2: Overview of the cheem viewer for classification tasks. Global view inputs, (a), set the PI, CI, and color statistic. Global view, (b) PC1 by PC2 approximations of the data space and attribution space. (c) prediction by observed y (visual of the confusion matrix for classification tasks). Points are colored by predicted class, and red circles indicate misclassified instances. Radial tour inputs (d) select features to include and which feature is changed in the tour. (e) shows a parallel coordinate display of the distribution of the feature attributions while bars depict contribution for the current basis. The black bar is the variable being changed in the radial tour. Panel (f) is the resulting data projection indicated as density in the classification case.
</p>
</div>
</div>
<div id="regression-task" class="section level3" number="5.2.4">
<h3><span class="header-section-number">5.2.4</span> Regression task</h3>
<p>Selecting an inaccurately predicted instance as PI and an accurately predicted instance, with similar feature values, as CI is a helpful way to understand how the model is failing or not. The global view (Figure <a href="5-ch-cheem.html#fig:regressioncase">5.3</a>a) shows a scatterplot of the observed vs predicted values, which should exhibit a strong relationship if the model is a good fit. The points can be colored by a statistic, residual, a measure of outlyingness (log Mahalanobis distance), or correlation aid understanding the structure identified in these spaces.</p>
<p>In the radial tour view, the observed response and the residuals (vertical) are plotted against the attribution projection of the PI (horizontal). The attribution projection can be interpreted similarly to the predicted value from the global view plot. It represents a linear combination of the features, and a good fit would be indicated when there is a strong relationship seen with the observed values. This can be viewed as a local linear approximation if the fitted model is nonlinear. As the contribution of a feature is varied, if the value of the PI does not change much, it would indicate that the prediction for this instance is NOT sensitive to that feature. Conversely, if the predicted value varies substantially, the prediction is very sensitive to that feature, suggesting that the feature is very important for the PI’s prediction.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:regressioncase"></span>
<img src="figures/ch5_fig3_app_regression_interactions.PNG" alt="Overview of the cheem viewer for regression task highlighting the differences from the classification task and interactive features. Panel (a) PCA of the data and attributions spaces, (b) residual plot, predictions by observed values. Four selected points are highlighted in the PC spaces and tabularly displayed. Coloring on a statistic (c) highlights structure organized in the attribution space. Interactive tabular display (d) populates when instances are selected. Contribution of the 1D basis affecting the horizontal position (e) parallel coordinate display of the feature attribution from all observations, and horizontal bars show the contribution to the current basis. Regression projection (f) uses the same horizontal projection and fixes the vertical positions to the observed y and residuals (left and right)." width="100%"  />
<p class="caption">
Figure 5.3: Overview of the cheem viewer for regression task highlighting the differences from the classification task and interactive features. Panel (a) PCA of the data and attributions spaces, (b) residual plot, predictions by observed values. Four selected points are highlighted in the PC spaces and tabularly displayed. Coloring on a statistic (c) highlights structure organized in the attribution space. Interactive tabular display (d) populates when instances are selected. Contribution of the 1D basis affecting the horizontal position (e) parallel coordinate display of the feature attribution from all observations, and horizontal bars show the contribution to the current basis. Regression projection (f) uses the same horizontal projection and fixes the vertical positions to the observed y and residuals (left and right).
</p>
</div>
</div>
<div id="interactive-features" class="section level3" number="5.2.5">
<h3><span class="header-section-number">5.2.5</span> Interactive features</h3>
<!-- Reactive vs exploration interactions -->
<p>The application has several reactive inputs that affect the data used, aesthetic display, and tour manipulation. These reactive inputs make the software flexible and extensible. The application also has more exploratory interactions to help link points across displays and reveal structures found in different spaces.</p>
<!-- Exploratory interactions -->
<p>A tooltip displays instance number/name and classification information while the cursor hovers over a point. Linked brushing allows the selection of points (left click and drag) where those points will be highlighted across plots. The information corresponding to the selected points is populated on a dynamic table. These interactions aid exploration of the spaces and, finally, identification of a primary and comparison instance.</p>
</div>
<div id="preprocessing" class="section level3" number="5.2.6">
<h3><span class="header-section-number">5.2.6</span> Preprocessing</h3>
<p>It is vital to mitigate the render time of visuals, especially when users may want to iterate many times. All computational operations should be prepared before runtime. The work remaining when an application is run solely reacts to inputs and rendering visuals and tables. Below discusses the steps and details of the reprocessing.</p>
(ref:citeRf) <span class="citation">(<a href="bibliography.html#ref-liaw_classification_2002" role="doc-biblioref">Liaw and Wiener 2002</a>)</span>
(ref:citeTs) <span class="citation">(<a href="bibliography.html#ref-kominsarczyk_treeshap_2021" role="doc-biblioref">Kominsarczyk et al. 2021</a>)</span>
<!-- Note on time of execution -->
<p>The time to preprocess the data will vary significantly with the model and local explanation. For reference, the FIFA data, 5000 instances of nine explanatory features, took 2.5 seconds to fit a random forest model of modest hyperparameters. Extracting the tree SHAP values of each instance took 270 seconds combined. PCA and statistics of the features and attributions took 2.8 seconds. These runtimes were from a non-parallelized R session on a modern laptop, but suffice to say that most of the time will be spent on the local attribution. An increase in model complexity or data dimensionality will quickly become an obstacle. Its reduced computational complexity makes tree SHAP an excellent candidate to start. (Alternatively, the package <strong>fastshap</strong> <span class="citation">(<a href="bibliography.html#ref-greenwell_fastshap_2020" role="doc-biblioref">Greenwell 2020</a>)</span> claims extremely low runtimes, attributed to fewer calls to the prediction function, partial implementation in C++, and efficient use of logical subsetting.)</p>
</div>
<div id="sec:infrastructure" class="section level3" number="5.2.7">
<h3><span class="header-section-number">5.2.7</span> Package infrastructure</h3>
<p>The above-described method and application are implemented as an open-source <strong>R</strong> package, <strong>cheem</strong> available on <a href="https://CRAN.R-project.org/package=cheem">CRAN</a>. Preprocessing was facilitated with models created via <strong>randomForest</strong> <span class="citation">(<a href="bibliography.html#ref-liaw_classification_2002" role="doc-biblioref">Liaw and Wiener 2002</a>)</span> and explanations calculated with <strong>treeshap</strong> <span class="citation">(<a href="bibliography.html#ref-kominsarczyk_treeshap_2021" role="doc-biblioref">Kominsarczyk et al. 2021</a>)</span>. The application was made with <strong>shiny</strong> <span class="citation">(<a href="bibliography.html#ref-chang_shiny_2021" role="doc-biblioref">Chang et al. 2021</a>)</span>. The tour visual is built with <strong>spinifex</strong> <span class="citation">(<a href="bibliography.html#ref-spyrison_spinifex_2020" role="doc-biblioref">Spyrison and Cook 2020</a>)</span>. Both views are created first with first with <strong>ggplot2</strong> <span class="citation">(<a href="bibliography.html#ref-wickham_ggplot2_2016" role="doc-biblioref">Hadley Wickham 2016</a>)</span> and then rendered as interactive <code>html</code> widgets with <strong>plotly</strong> <span class="citation">(<a href="bibliography.html#ref-sievert_interactive_2020" role="doc-biblioref">Sievert 2020</a>)</span>. <strong>DALEX</strong> <span class="citation">(<a href="bibliography.html#ref-biecek_dalex_2018" role="doc-biblioref">Biecek 2018</a>)</span> and the free ebook, <em>Explanatory Model Analysis</em> <span class="citation">(<a href="bibliography.html#ref-biecek_explanatory_2021" role="doc-biblioref">Biecek and Burzykowski 2021</a>)</span> were a boon to understanding local explanations and how to apply them.</p>
</div>
<div id="installation-and-getting-started" class="section level3" number="5.2.8">
<h3><span class="header-section-number">5.2.8</span> Installation and getting started</h3>
<p>The package can be installed from GitHub using the following <strong>R</strong> code:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="5-ch-cheem.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install remotes if absent</span></span>
<span id="cb3-2"><a href="5-ch-cheem.html#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span>(<span class="fu">require</span>(<span class="st">&quot;remotes&quot;</span>) <span class="sc">==</span> <span class="cn">FALSE</span>) <span class="fu">install.packages</span>(<span class="st">&quot;remotes&quot;</span>)</span>
<span id="cb3-3"><a href="5-ch-cheem.html#cb3-3" aria-hidden="true" tabindex="-1"></a>remotes<span class="sc">::</span><span class="fu">install.packages</span>(<span class="st">&quot;cheem&quot;</span>, <span class="at">dependencies =</span> <span class="cn">TRUE</span>)</span>
<span id="cb3-4"><a href="5-ch-cheem.html#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;cheem&quot;</span>)</span>
<span id="cb3-5"><a href="5-ch-cheem.html#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="fu">run_app</span>()</span></code></pre></div>
<p>To process your data, you will need to use the <code>treeshap</code> package, which can be installed from GitHub:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="5-ch-cheem.html#cb4-1" aria-hidden="true" tabindex="-1"></a>remotes<span class="sc">::</span><span class="fu">install_github</span>(<span class="st">&#39;ModelOriented/treeshap&#39;</span>)</span></code></pre></div>
<p>Follow the examples provided with the package to compute the local explainers (see <code>?cheem_ls</code>). The application uses the returned objects from a from <code>cheem_ls()</code> call. Alternatively, the cheem viewer shiny app can be directly accessed at <a href="https://ebsmonash.shinyapps.io/cheem_initial/">ebsmonash.shinyapps.io/cheem_initial/</a>.</p>
</div>
</div>
<div id="sec:casestudies" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Case studies</h2>
<p>To illustrate the cheem method, it is applied to modern datasets, two classification examples and then two of regression.</p>
<div id="palmer-penguin-species-classification" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Palmer penguin, species classification</h3>
<p>The Palmer penguins data <span class="citation">(<a href="bibliography.html#ref-gorman_ecological_2014" role="doc-biblioref">Gorman, Williams, and Fraser 2014</a>; <a href="bibliography.html#ref-horst_palmerpenguins_2020" role="doc-biblioref">Horst, Hill, and Gorman 2020</a>)</span> was collected on three species of penguins foraging near Palmer Station, Antarctica. The data was publicly available to substitute for the overly-used iris data and is quite similar in form. After removing incomplete instances, there are 333 instances of four physical measurements, bill length (<code>bl</code>), bill depth (<code>bd</code>), flipper length (<code>fl</code>), body mass (<code>bm</code>), for this illustration. A random forest model was fit with species as the response feature.</p>
(ref:casepenguins-cap) Examining the SHAP values for a random forest model classifying Palmer penguin species. The PI is a Gentoo (purple) Chinstrap (orange) penguin that is misclassified as a Chinstrap (orange), marked as an asterisk in (a), and the dashed vertical line in (b). The radial view shows varying the contribution of <code>fl</code> from the initial attribution projection (b, left), which produces a linear combination where the PI is more probably (higher density value) a Chinstrap than a Gentoo (b, right). (The animation of the radial tour is at <a href="https://vimeo.com/666431172">vimeo.com/666431172</a>.)
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:casepenguins"></span>
<img src="figures/ch5_fig4_case_penguins.png" alt="(ref:casepenguins-cap)" width="100%"  />
<p class="caption">
Figure 5.4: (ref:casepenguins-cap)
</p>
</div>
<p>Figure <a href="5-ch-cheem.html#fig:casepenguins">5.4</a> shows plots from the cheem viewer for exploring the random forest model on the penguins data. Panel (a) shows the global view, and panel (b) shows several 1D projections generated with the radial tour. Penguin 243, a Gentoo (purple), is the PI because it has been misclassified as a Chinstrap (orange).</p>
(ref:casepenguinsblfl-cap) Checking what is learned from the cheem viewer. This is a plot of flipper length (<code>fl</code>) and bill length (<code>bl</code>), where an asterisk highlights the PI. A Gentoo (purple) misclassified as a Chinstrap (orange). The PI has an unusually small <code>f_l</code> length which is why it is confused with a Chinstrap.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:casepenguinsblfl"></span>
<img src="figures/ch5_fig5_case_penguins_BlFl.png" alt="(ref:casepenguinsblfl-cap)" width="100%"  />
<p class="caption">
Figure 5.5: (ref:casepenguinsblfl-cap)
</p>
</div>
<p>There is more separation visible in the attribution space than the data space, as would be expected. The predicted vs observed plot reveals a handful of misclassified instances. A Gentoo has been wrongly labeled as a Chinstrap is selected for illustration. The PI is a misclassified point (represented by the asterisk in the global view and a dashed vertical line in the tour view). The CI is a correctly classified point (represented by an <span class="math inline">\(\times\)</span> and a vertical dotted line).</p>
<p>The radial tour starts from the attribution projection of the misclassified instance (b, left). The important features identified by SHAP in the (wrong) prediction for this instance are mostly <code>bl</code> and <code>bd</code> with small contributions of <code>fl</code> and <code>bm</code>. This projection is a view where the Gentoo (purple) looks much more likely for this instance than Chinstrap. That is, this combination of features is not particularly useful because the PI looks very much like other Gentoo penguins. To explore this, the radial tour is used to vary the contribution of flipper length (<code>fl</code>). (In our exploration, this was the third feature explored. It is typically useful to explore the features with more significant contributions, here <code>bl</code> and <code>bd</code>, but when doing this, nothing was revealed about how the PI differed from other Gentoos). On varying <code>fl</code> as it contributes more to the projection (b, right), more, and more, this penguin looks like a Chinstrap. This suggests that <code>fl</code> should be considered an important feature for explaining the (wrong) prediction.</p>
<p>Figure <a href="5-ch-cheem.html#fig:casepenguinsblfl">5.5</a> confirms that flipper length (<code>fl</code>) is vital for the confusion of the PI as a Chinstrap. Here, flipper length and body length are plotted, and the PI can be seen to be closer to the Chinstrap group in these two features, mainly because it has an unusually low value of flipper length relative to other Gentoos. From this view, it makes sense that its a hard instance to account for as decision trees can only partition only vertical and horizontal lines.</p>
</div>
<div id="chocolates-milkdark-chocolate-classification" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Chocolates, milk/dark chocolate classification</h3>
<p>The chocolates dataset consists of 88 instances of ten nutritional measurements determined from their labels and labeled as either milk or dark. Dark chocolate is considered healthier than milk. The data were collected by students during the Iowa State University class STAT503 from nutritional information from the manufacturer’s website and normalized to 100g equivalents. The data is available in the <strong>cheem</strong> package. A random forest model is used for the classification of chocolate types.</p>
<p>It could be interesting to examine the nutritional properties of any dark chocolates that have been misclassified as milk. A reason to do this is that a dark chocolate, nutritionally more like milk, should not be considered a healthy alternative. It is interesting to explore which nutritional features contribute most to misclassification.</p>
(ref:casechocolates-cap) Examining the local interpretation for a PI which is dark (orange) chocolate incorrectly predicted to be milk (green). From the attribution projection, this chocolate correctly looks more like dark than milk, which suggests that the local explanation does not help understand the prediction for this instance. So, the contribution of Sugar is varied – reducing it corresponds primarily with increasing Fiber. When Sugar is zero, Fiber contributes strongly towards the left. In this particular view, the PI is closer to the bulk of the milk chocolates, suggesting that the prediction put a lot of importance on Fiber. This chocolate is a rare dark chocolate without any Fiber leading to it being mistaken for a milk chocolate. (A video of the tour animation can be found at <a href="https://vimeo.com/666431143">vimeo.com/666431143</a>.)
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:casechocolates"></span>
<img src="figures/ch5_fig6_case_chocolates.png" alt="(ref:casechocolates-cap)" width="100%"  />
<p class="caption">
Figure 5.6: (ref:casechocolates-cap)
</p>
</div>
<p>This type of exploration is shown in Figure <a href="5-ch-cheem.html#fig:casechocolates">5.6</a>, where a chocolate labeled dark but predicted to be milk is chosen as the PI (instance 22). It is compared with a CI that is a correctly classified dark chocolate (instance 7). The PCA plot and the tree SHAP PCA plots (a) show a big difference between the two chocolate types but with confusion for a handful of instances. The misclassifications are more apparent in the observed vs predicted plot and can be seen to be mistaken in both ways: milk to dark and dark to milk.</p>
<p>The attribution projection for chocolate 22 suggests that Fiber, Sugars, and Calories are most responsible for its incorrect prediction. The way to read this plot is to see that Fiber has a large negative value while Sugars and Calories have reasonably large positive values. In the density plot, instances on the very left of the display would have high values of Fiber (matching the negative projection coefficient) and low values of Sugars and Calories. The opposite would be interpreting a point with high values in this plot. The dark chocolates (orange) are primarily on the left, and this is a reason why they are considered to be healthier: high fiber and low sugar. The density of milk chocolates is further to the right, indicating that they generally have low fiber and high sugar.</p>
<p>The instance of interest (dashed line) can be viewed against the comparison instance (dotted line). Now, one needs to pay attention to the parallel plot of the SHAP values, which are local to a particular instance, and the density plot, which is the same projection of all instances as specified by the SHAP values of the instance of interest.</p>
<p>The feature contributions to the two different predictions can be quickly compared in the parallel coordinate plot. The instance of interest differs with the comparison primarily on the Fiber feature, which suggests that this is the reason for the incorrect prediction.</p>
<p>From the density plot, which is the attribution projection corresponding to the instance of interest, both instances are more like dark chocolates. Varying the contribution of Sugars and altogether removing it from the projection is where the difference becomes apparent. When primarily Fiber is examined, instance 22 looks more like a milk chocolate.</p>
(ref:casechocolatesinverse-cap) Examining the local interpretation for a PI which is milk (green) chocolate incorrectly predicted to be dark (orange). In the attribution projection, the PI could be either milk or dark. Sodium and Fiber have the largest differences in attributed feature importance, with low values relative to other milk chocolates. The lack of importance attributed to these variables is suspected to contribute to the mistake, so the contribution of Sodium is varied. If Sodium had a larger contribution to the prediction (like in this view). the PI would look more like other milk chocolates. (A video of the tour animation can be found at <a href="https://vimeo.com/666431148">vimeo.com/666431148</a>.)
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:casechocolatesinverse"></span>
<img src="figures/ch5_fig7_case_chocolates_inverse.png" alt="(ref:casechocolatesinverse-cap)" width="100%"  />
<p class="caption">
Figure 5.7: (ref:casechocolatesinverse-cap)
</p>
</div>
<!-- segue and selection for chocolates inverse case -->
<p>It would also be interesting to explore the inverse case. This would describe which features lead to a milk chocolate being misclassified as dark and how those attributions differ from the previous misclassification. Chocolate 84 is just this case and is compared with a correctly predicted milk chocolate (instance 71). This exploration is shown in Figure <a href="5-ch-cheem.html#fig:casechocolatesinverse">5.7</a>.</p>
<!-- treeSHAP pca, PCP, tour -->
<p>The difference of position in the tree SHAP PCA with the previous case is quite significant; this gives an approximate feel that the attribution should be quite different. Looking at the initial contribution, this is found to be the case. Previously Fiber was essential while it is absent from the attribution in this case. Conversely, Calories from Fat and Total Fat are highly attributed here, while unimportant in the preceding case.</p>
<p>Comparing the attribution with the CI (dotted line), discrepancies in Sodium and Fiber are identified. The contribution of Sodium is selected to be varied. Even in the initial projection, the instance looks slightly more like its observed milk than predicted dark chocolate. The misclassification appears least supported when the basis reaches sodium attribution of typical dark chocolate.</p>
</div>
<div id="fifa-wage-regression" class="section level3" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> FIFA, wage regression</h3>
<p>The 2020 season FIFA data <span class="citation">(<a href="bibliography.html#ref-leone_fifa_2020" role="doc-biblioref">Leone 2020</a>; <a href="bibliography.html#ref-biecek_dalex_2018" role="doc-biblioref">Biecek 2018</a>)</span> contains many skill measurements of soccer/football players and wage information. Nine higher-level skill groupings were identified and aggregated from a correlation matrix plot. A random forest model is fit from these predictors and regresses player wages [2020 euros]. The model was fit from 5000 instances before being thinned to 500 players to mitigate occlusion and render time. Continuing from the exploration in section <a href="2-ch-background.html#sec:explanations">2.6</a>, we are interested to see the difference in attribution based on the exogenous player position. That is, the model should be able to use multiple linear profiles to better predict the wages from different field positions of players. A leading offensive fielder (L. Messi) is compared with a top defensive fielder (V. van Dijk). The same instances were used in figure <a href="5-ch-cheem.html#fig:shapdistrbd">5.1</a>.</p>
(ref:casefifa-cap) FIFA 2020 data, a random forest model, regresses wages [2020 Euros] from nine aggregated skill measurements. The PI is a star offensive player (L. Messi) compared with a top defensive player (V. van Dijk). Three features with low attribution from both players are removed. The attribution projection starts with the selected instance on the right. The contribution to defense (<code>def</code>) is varied. The star offensive player is not distinguished in the horizontal direction. At this point, defensive players have been rotated to the highest horizontal value. (A video of the animated radial tour can be found at <a href="https://vimeo.com/666431163">vimeo.com/666431163</a>.)
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:casefifa"></span>
<img src="figures/ch5_fig8_case_fifa.png" alt="(ref:casefifa-cap)" width="100%"  />
<p class="caption">
Figure 5.8: (ref:casefifa-cap)
</p>
</div>
<p>Figure <a href="5-ch-cheem.html#fig:casefifa">5.8</a>, tests the support of the local explanation. Offensive and reaction skills (<code>off</code> and <code>rct</code>) are both crucial to explaining a star offensive player. If either of them were rotated out, the other would be rotated into the frame, maintaining a far-right position. However, Increasing the contribution of variable with the low importance would rotate important features out of the frame.</p>
<p>The contribution from <code>def</code> will be varied to contrast with offensive skills. As the contribution of defensive skills increases, Messi’s is no longer separated from the group. Players with high values in defensive skills are now the rightmost points. In terms of what-if analysis, the difference between the data mean and his predicted wages would be halved if Messi’s tree SHAP attributions at these levels.</p>
</div>
<div id="ames-housing-2018-sales-price-regression" class="section level3" number="5.3.4">
<h3><span class="header-section-number">5.3.4</span> Ames housing 2018, sales price regression</h3>
<p>Ames 2018, housing data was subset to North Ames (the neighborhood with the most house sales). The remaining are 338 house sales. A random forest model has regressed this price with the features Lot Area (<code>LtA</code>), Overall Quality (<code>Qlt</code>), Year the house was Built (<code>YrB</code>), Living Area (<code>LvA</code>), number of Bathrooms (<code>Bth</code>), number of Bedrooms (<code>Bdr</code>), the total number of Rooms (<code>Rms</code>), Year the Garage was Built (<code>GYB</code>), and Garage Area (<code>GrA</code>). Using interaction from the global view, a house with an extreme negative residual and an accurate instance with a similar prediction are selected.</p>
(ref:caseames-cap) Exploring an instance with a large residual as the PI from fitting sales price [USD] to other variables in the Ames housing 2018. The sale price of the PI was under-predicted. The local explanation indicates a sizable attribution to Lot Area (<code>LtA</code>). The CI has a similar predicted sales price and smaller residual and has minimal attribution to Lot Area. In the attribution projection, the PI has a higher sales price than the CI. Reducing the contribution of lot area brings these two prices in line. This suggests if the model did not consider Lot Area, then the two houses would be quite similar. That is, the large residual is due to a lack of factoring in the lot area for the prediction of PI’s sales price. (A video showing the animation is at <a href="https://vimeo.com/666431134">vimeo.com/666431134</a>.)
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:caseames"></span>
<img src="figures/ch5_fig9_case_ames2018.png" alt="(ref:caseames-cap)" width="100%"  />
<p class="caption">
Figure 5.9: (ref:caseames-cap)
</p>
</div>
<p>Figure <a href="5-ch-cheem.html#fig:caseames">5.9</a> selects the house sale 74, a sizable under prediction with an enormous Lot Area contribution. The CI has a similar predicted price though the prediction was accurate and gives almost no attribution to lot size. The attribution projection places instances with high Living Areas to the right. The contribution of Living Area contrasts the contribution of this feature. As the contribution of lot area decreases, the predictive power decreases for the PI, while the CI remains stationary. This large of importance in the Living Area is relatively uncommon. Boosting tree models may be more resilient to such an under-prediction as up-weighting this residual would force its inclusion in the final model.</p>
</div>
</div>
<div id="sec:cheemdiscussion" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Discussion</h2>
<p>There is a clear need to assist with the interpretability of black-box models. This chapter provides a technique that builds on local interpretations to explore the feature importance local to an instance. The local interpretations form an attribution projection from which feature contributions are varied using a radial tour. Several diagnostic plots are provided to assist with understanding the sensitivity of the prediction to particular features. A global view shows the data space, explanation space, model fit. The user can interactively select instances to compare, contrast, and study further.</p>
<p>Th has been illustrated using four data examples using random forest models and using the tree SHAP local explanation. In the penguins example, we showed how the misclassification of a penguin arose due to it having an unusually small flipper size compared to others of its species. This was verified by making a follow-up plot of the data, including this variable. The chocolates example shows how a dark chocolate was misclassified primarily due to it value on Fiber, and a milk chocolate was misclassified as dark due to its lowish Sodium value. For the FIFA example, we show how low Messi’s salary would be if it depended on defensive skills. In the Ames housing data, an inaccurate prediction for a house was likely due to the Lot Area not being effectively used.</p>
<p>This analysis is manually intensive and thus only feasible for investigating a few instances. The approach select instances that the model has not done well and compare it with an instance where it did fit well. The radial tour launches from the attribution projection to enable exploration of the sensitivity of the prediction to any feature. It can be helpful to make additional plots of the features and responses to cross-check interpretations made from the cheem viewer. This methodology provides an additional tool in the box for studying model fitting.</p>
<p>An implementation is provided in the open-source <strong>R</strong> package <strong>cheem</strong>, available on <a href="https://CRAN.R-project.org/package=cheem">CRAN</a>. Example data sets are provided, and you can upload your data after model fitting and computing the local explanations. In theory, this approach would work with any black-box model, but the implementation currently only calculates tree SHAP for tree-based models supported by <strong>treeshap</strong> (tree based models from <strong>randomForest</strong>, <strong>ranger</strong>, <strong>gbm</strong>, <strong>xgboost</strong>, <strong>lightgbm</strong>, or <strong>catboost</strong>). Tree SHAP was selected because of its computational efficiency. The SHAP and oscillation explanations could be added with the use of the <code>DALEX::explain()</code> and would be an excellent direction to extend the work <span class="citation">(<a href="bibliography.html#ref-biecek_dalex_2018" role="doc-biblioref">Biecek 2018</a>; <a href="bibliography.html#ref-biecek_explanatory_2021" role="doc-biblioref">Biecek and Burzykowski 2021</a>)</span>.</p>
<!-- Attribution space does tend to be more separable than data space at least in the first couple principal components. Different statistics should be explored to convey a clearer understanding what structure the explanation distinguishes. It is curious that the attribution space the same dimensionality of the data with a seemingly better separation. It sounds like a candidate to fit another model. However, such an attempt should want to be vigilant not to over-fit the training data.-->
<!-- ## References -->
<!-- <div id="refs"></div> -->
<!-- Adds a bib section at the end of every chapter -->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4-ch-userstudy.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="6-ch-conclusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
