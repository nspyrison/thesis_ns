[["index.html", "Interactive and dynamic visualization of high-dimensional data via animated linear projections Welcome", " Interactive and dynamic visualization of high-dimensional data via animated linear projections Nicholas S Spyrison Welcome This is the website for my PhD thesis at Monash University, Australia. A pdf version can be found here, and the code creating it can be found on github. 2022-02-28 "],["abstract.html", "Abstract", " Abstract Visualizing data space is a crucial aspect of exploratory data analysis, checking assumptions, and validating model performance. However, visualization quickly becomes complex as the dimensionality of the data or features increases. Traditionally, linear projections have viewed discrete pairs of components to mitigate this complexity. Data visualization tours are a class of dynamic linear projections that animate many linear projections over small changes to the projection basis. The permanence of observations between nearby frames potentially conveys more information than discrete orthogonal frames alone. Tours are categorized by the path of their bases. Manual tours uniquely allow for user-controlled steering of a path of bases, where the contributions of individual variables can be changed. The radial tour is a specific case of the manual tour, that freezes the angle of movement but allows the magnitude to be varied. This is used in the work reported here. Chapter 3 clarifies the theoretical basis of the radial tour. The details of implementation and illustration of its use are provided. It introduces an open-source R package that facilitates creating these tours with a variety of display choices and user controls. Allowing the analyst to steer a path in the radial tour should enable a better understanding of the variable importance to any structure revealed in a projection. Chapter 4 discusses a within-participant user study comparing the radial tour with current practices: principal component analysis (PCA) and the original type of tour that has no steering. The \\(n=108\\) crowdsourced participants performed a variable attribution task describing the separation between clusters. The results find that the radial tour leads to more accurate variable attribution. Participants also reported that the radial tour was their preferred visualization method. Nonlinear modeling techniques are sometimes referred to as black-box models due to the uninterpretable nature of the model terms. Recent research in Explainable Artificial Intelligence (XAI) tries to bring these models interpretability through local explanations. Local explanations are a class of techniques that approximate the linear-variable importance for prediction at one point in the data. Chapter 5 provides a new approach for exploring the variable sensitivity of local explanations using the radial tour. Local explanations can be considered projection bases. The radial tour is used to vary the contribution of a variable to assess its importance for any particular prediction. This is illustrated using four contemporary data examples covering classification and regression. An accompanying R package provides a graphical user interface (GUI) for conducting this analysis. Nicholas Spyrison 2022-03-01 &gt; "],["acknowledgments.html", "Acknowledgments", " Acknowledgments I would like to express my sincere gratitude to my supervisors, Professor Dianne Cook and Professor Kimbal Marriott, for their support of my Ph.D studies and research, their subject expertise, and their careers supervising and teaching. Thank you for continuously pushing me and my research to new levels. I have enjoyed teaching data visualization, which has been strongly shaped by Dis practical, data-first, visualization-often approach. I will continue to hear Kims persistent question, what do we learn from this? as a reminder not to get lost in the implementation details and regularly step back and analyze if this is the correct object to change. Thanks to the member of supervisory panel, Associate Professor Bernhard Jenny, Dr. Maxime Cordeil, and Dr. Shirui Pan for their time, engagement, and suggestions throughout this research. Special thanks to Jieyang Chong and Julie Holden for their proofreading and suggested language clarifications. I thank my fellow Ph.D students, NUMBAT and DVIA lab members, and Pomodoro partners, primarily on the occasion of stimulating discussions and the positive peer pressure of knowing others are around and working. Thanks immensely to those who empathized with me and others, primarily through the hardships of studies and COVID-19. Gratitude to Ying Zhou for her enduring support through the thick of my studies and wavering mental health. Last but not least, I would like to thank my parents, Doug and Terry, for their continued support and for airing my grievances at odd hours of the day. Thanks to Alan and Claire for their companionship and support now and in our more formative years. This research was supported in part by an Australian Government Research Training Program (RTP) Scholarship. "],["preface.html", "Preface", " Preface This thesis has been written using R Markdown with the bookdown package (Xie 2016). All materials required to compile the thesis are available at github.com/nspyrison/thesis_monash_phd. Versions are made available as html and pdf at nspyrison.github.io/thesis_ns/ and github.com/nspyrison/thesis_ns/blob/master/docs/thesis_ns.pdf, respectively. "],["1-ch-introduction.html", "Chapter 1 Introduction 1.1 Research questions 1.2 Methodology 1.3 Contributions 1.4 Thesis structure", " Chapter 1 Introduction Exploratory Data Analysis (EDA) is the process of the initial summarization and visualization of a dataset. EDA is a critical first step that includes checking for realistic values, identifying improper data formats, and revealing insights (Tukey 1977). Wickham and Grolemund (2017) describes the analyst workflow as a series of discrete steps. The data is imported and cleaned before entering into an iterated cycle of transformation, visualization, and modeling. This work focuses on the visualization step, specifically for quantitative multivariate data. Multivariate data is ubiquitous in contemporary data sets. Multivariate data is found in physics, biology, social sciences, and manufacturing, to name a few (Wang et al. 2018; Huber et al. 2015; Brown 2015; Evans and Boreland 2017, respectively). As the number of variables in the data increases, it becomes increasingly difficult to visualize this space and reveal the structure contained in the data. This thesis addresses the visualization and analysis of multivariate data. One of the most common and successful approaches to visualize multivariate data is to use linear projection, which approximates \\(p\\)-dimensional data onto 1-3 display dimensions. In the same way that a 3D object casts a 2D shadow, these projections cast one data orientation onto a lower plane. Linear projections define new variables called components, which are a linear combination of the original variables. This is essentially an orientation of the direction of the axes. A basis is an orthonormal matrix that maps the variable to the component output space. The basis of a linear projection is frequently illustrated with a biplot (Gabriel 1971). The biplot shows the magnitude and angle each variable contributes to the resulting display dimensions inscribed in a unit circle, such as in Figure 1.1. Traditional axes-based visuals look at 1- to 3D views of orthogonal variables. In contrast, viewing linear combinations of these variables can reveal features that exist in more than three dimensions. (ref:ch1fig2-cap) Two linear projections of penguins data. Biplot circles depict the basis indicating the direction and magnitude of the variable contributions. In the left panel, the direction of separation between the orange and green clusters is in the direction that bl contributes, meaning that this variable is sensitive to the separation of this cluster. The purple clusters separation is attributed primarily to bd and smaller contributions from fl and bm. Many other linear orientations do not resolve structures of interest, such as cluster separation (right panel). The Palmer Penguin data (Gorman, Williams, and Fraser 2014; Horst, Hill, and Gorman 2020) measures four physical variables: bill length (bl), bill depth (bd), flipper length (fl), and body mass (bm) for three species of penguins. Figure 1.1: (ref:ch1fig2-cap) There are many features that an analyst may be interested in when analyzing multivariate data. Shape, spread, clusters separation, outliers, and irregularities are the most common. Not all basis orientations will reveal the features in the data. Thus the choice of basis is essential. Furthermore, the analyst is interested in understanding the variables that reveal features. For instance, Figure 1.1 shows two linear projections of penguin data. The color and shape of the data points distinguish the three penguin species. The linear projection used in the left panel contains a significant separation of the clusters, while the right panel does not show differences between the species clusters. Therefore, it makes sense for an analyst to explore many projections with very different bases. However, it can be challenging to link observations across other projections with no relationship to one another. The tour is a dynamic visualization that overcomes this difficulty (Cook et al. 2008; Lee et al. 2021). It is a class of linear projections that animate over small changes to the projection basis. A vital feature of the tour is the visual permanence and trackability of the points through the frames. In the shadow analogy, an object such as a barstool will cast a circular shadow if the light is directly above the seat. However, such a shadow does not give the observer sufficient information as it could arise from any number of shapes that contain circular profiles: spheres, cylinders, or circles. However, if the stool was rotated, its legs show in the shadow, giving an intuitive interpretation of the object. Similarly, the rotation of a data object yields information about its structure. Originally component spaces and the original tour have no means to influence the path of its bases. Cook and Buja (1997) introduced the manual tour, offering user control over the basis. By selecting a variable and initializing an additional manipulation dimension onto a projection, the contribution of the variable can be controlled through a rotation of the manipulation space. This user-controlled steering of the basis path enables the analyst to explore what happens to the structure when one variable is removed or the contribution of another is increased. The manual tour allows the analyst to control the angle and magnitude that a variable contributes to the projection. However, influencing the magnitude is more meaningful as angular manipulations effectively rotate a projection, changing the relative position but not the amounts of the variables. Because of this, this work focuses on a specific manual tour, the radial tour, where the manipulation angle is fixed, but a variables magnitude is changed along the radius. Figure 1.2 shows a radial tour of the penguins data varying the contribution of bill length. The left panel has a total contribution from bl, while the middle panel shows half contribution, and bill length has been removed from the right panel. The separation between orange and green clusters was in the direction that bl contributed and when this contribution is removed, so too is the separation of the clusters. Because of this, this variable is said to be sensitive to the separation of these two clusters. (ref:ch1fig3-cap) A radial tour changing the contribution of bill length (bl). When bill length has a considerable contribution, the clusters of orange and green are separated (left and middle). When its contribution is removed, the clusters overlap (right). Because of this, we say that bill length is sensitive to the separation of these two species. An animated version can be viewed at vimeo.com/676723431. Figure 1.2: (ref:ch1fig3-cap) 1.1 Research questions Discerning variable sensitivity to the structure is crucial to understanding which variables contribute to a revealed feature. We conjecture that the user interaction afforded by the radial tour should allow for a more precise exploration of this structure from testing the variable sensitivity to that structure. The over-arching question of interest can, therefore, be stated as: Can the radial tour, with user-steering of the basis, help analysts understand the variable sensitivity of structure in the projection? While Cook and Buja (1997) sketched the theoretical basis for the manual (and hence radial) tour, some details are missing. Furthermore, we lack a publicly available implementation, fully-featured interface design, implementation notes, and have no evaluation of its performance over alternatives. RQ 1. How do we define and implement a user interface and interactions for the radial tours to add and remove variables smoothly from 1- and 2D linear data projections? At present, the radial tour is not used by analysts. Instead, they would use a single projection to understand the structure, almost always principal components analysis (PCA, Pearson 1901), which chooses the basis that shows the most variation. Another approach is to use a grand tour (Asimov 1985). The grand tour animates many interpolated frames between randomly selected target bases. Neither PCA nor the grand tour provides a means for manually manipulating a desired variables contribution to the basis. We wish to investigate if the basis-steering of the radial tour facilitates a better understanding of variable sensitivity to the structure. RQ 2. Does the use of the interactive radial tour improve analysts understanding of the relationship between variables and structure in 2D linear projections compared to existing approaches? Complex nonlinear models are also being applied more frequently to predict or classify with many predictors. While these models lead to increased accuracy over linear models, they suffer from a loss of the interpretability of their variables. One aspect of eXplainable Artificial Intelligence (XAI, Adadi and Berrada 2018; Arrieta et al. 2020) tries to preserve the interpretability of such models through local explanations. These explanations are essentially linear variable importance in the vicinity of one observation of a model. That is, the extent that variables help the model explain the difference between the observed means and an observations prediction. The user control from the radial tour potentially allows an analyst to better understand the model and the support of these local explanations. RQ 3. Can the radial tours be used in conjunction with local explanations to improve the interpretability of black-box models? 1.2 Methodology The research corresponding to RQ 1 entails algorithm &amp; software design (Kleinberg and Tardos 2006), and adapts the algorithm from Cook and Buja (1997). To address RQ 2, experimental design (Winer 1962) is used. A task and measure must be defined that is suitable to evaluate the radial tour against alternatives. The experimental factors and their levels must be selected and randomly assigned as a randomized controlled trial to measure the efficacy of user-controlled radial tours compared with two benchmark methods. The research responding to RQ 3 involves design science (Hevner et al. 2004). It is not obvious how to combine a radial tour with a nonlinear model. A local explanation approximates the linear variable importance in the vicinity of one observation. Two novel interactive visualizations need to be developed. The first should visually facilitate the selection of observations to explore. The second should extend the biplot to show the distribution of local explanations from all observations while a radial tour is created starting from the attribution of one observation. This will examine the variable sensitivity to the structure identified in the local explanation. 1.3 Contributions The contributions resulting from the research to address these research questions can be split into scientific knowledge and software contributions: 1.3.1 Scientific knowledge Radial tour algorithm Refined and clarified the steps to producing a radial tour based on the use of the Rodrigues rotation formula extends the approach from Rodrigues (1840). The algorithm makes the radial tour in the animation and interactive systems simpler to implement. Provides new examples of usage. A user study comparing the radial tours efficacy against two alternatives  PCA and the grand tour, the first empirical evaluation of the radial tour. Creation of supervised classification task to assess the variable attribution to the separation of two clusters. As tested over experimental factors: location, shape, and dimensionality. Definition of an accuracy measure to evaluate this task. Results: strong evidence that the radial tour increases the accuracy of this task by a sizable amount and minor evidence to suggest a moderate increase in accuracy of the grand tour over PCA. Mixed model regression helps to attribution the source of the error accounting for the variability of participants skills and the difficulty of the simulation by chance. A novel interactive visualization using the radial tour to explore the variable sensitivity of local explanations from nonlinear models, part of XAI. A global view approximates the variable space, attribution space, and residual plot side-by-side, which serves to identify observations of interest. Selected observations normalized variable attribution is used as a projection basis. Explore the support of the local explanation using the radial tour; the variable sensitivity to the structure identified tests the range of contributions supporting the explanation. 1.3.2 Software spinifex, an R package for transforming data, performing manual tours, and extending any tours display and animation exportation. Facilitates the transformation of numeric variables in the data. Identify various bases finding various features of the data. Creation of manual tours allows analyst steering of the basis to explore the variable sensitivity to the structure. Layered composition of tour displays that mirrors the approach in ggplot2 (Wickham 2016), interoperable with tours made by tourr (Wickham et al. 2011). Exporting rendered animation either to interactive html widgets with plotly (Sievert 2020) or to gif, mp4, and other video formats with gganimate (Pedersen and Robinson 2020). Interactive shiny (W. Chang et al. 2021) application to preprocess data and explore. Users can choose from six supplied datasets or upload their own. Vignettes and code examples help users get up to speed. cheem, an R package that facilitates the exploration of local explanations of nonlinear models through the radial tour. Preprocessing: given a tree-based model, calculate the tree SHAP local explanation (via treeshap, Kominsarczyk et al. 2021) of all observations, and calculate statistics that accent the separability of this space. Visualization of approximations of the data space, attribution space, and residual plot side-by-side with linked brushing, hover tooltips, and tabular display facilitates the selection of observations to explore. Use of the radial tour changes variable contribution to test the support of the variable contribution in agreement with the explanation. Interactive application facilitates this analysis for several prepared datasets or user preprocessed data. A vignette and code examples help users get up to speed. 1.4 Thesis structure The remainder of the thesis is organized as follows: Chapter 2 covers various visualization techniques before introducing related studies, nonlinear models, and their interpretability issues. Chapter 3 discusses the theory and implementation of the radial tours in the package spinifex. Chapter 4 discusses a user study evaluating the radial tours efficacy compared with PCA and the grand tour. Chapter 5 discusses the cheem package, which extends radial tours to improve the interpretability of nonlinear models. Lastly, Chapter 6 concludes with some takeaways and a discussion of limitations and possible extensions. "],["2-ch-background.html", "Chapter 2 Background 2.1 Motivation 2.2 Multivariate visualization 2.3 Linear projections 2.4 Evaluating multivariate data visualization 2.5 Nonlinear models 2.6 Local explanations 2.7 Conclusion", " Chapter 2 Background The last chapter discussed the importance of data visualization despite the complexity of viewing high-dimensional data and outlined the research questions to be addressed around the user-control of the radial tour. This chapter first motivates data visualization in general and the importance of interaction. It continues on illustrating common visualizations for quantitative multivariate data before turning to dimension reduction, including further discussion of tours. Then empirical evaluations of multivariate visuals are covered. The chapter concludes with nonlinear models and the extention of their interpretation with local explanations. 2.1 Motivation Visualization is much more robust than numerical summarization alone (Anscombe 1973; Matejka and Fitzmaurice 2017). Figure 2.1 illustrates this. The data sets have a quite different structure revealed in their visualization but not in the summary statistics. All data sets have the same means, standard deviations, and correlation. (ref:ch2fig1-cap) The datasaurus dozen is a modern data set illustrating Anscombes point that summary statistics cannot always adequately summarize data content. Patterns in the scatterplots are evident despite the data having the same summary statistics. Unless the data is plotted, one would never know that there were such radical differences. Figure 2.1: (ref:ch2fig1-cap) Interaction is an essential aspect of modern data visualization (Batch et al. 2019; Card, Moran, and Newell 1983; Marriott et al. 2018). Interaction facilitates accessing increasing amounts of data and amplifies cognition through control and input (Dimara and Perin 2019). Munzner (2014) posits that responsive and fast computer graphics allow us to move beyond paper and static resources. Interaction is key to navigating within views of sizable data and linking observations and features across these views. We focus on multivariate data interactions with coordinated views, linked brushing, and tooltip display. Coordinated and multiple views (Roberts 2007) (also known as ensemble graphics, Unwin and Valero-Mora 2018) use several types of visuals that give a more comprehensive understanding than any one visual portrays in isolation. The linking of observations between different views and animation frames is facilitated by linked brushing (Becker and Cleveland 1987). In linked brushing, selected observations in one view are colored across all views, allowing these selections to be tracked and correlated across other views and frames. Linked brushing has proved to be helpful in animated tours (Arms, Cook, and Cruz-Neira 1999; Laa, Cook, and Valencia 2020; Lee, Laa, and Cook 2020). Tooltip displays upon the cursor hovering over observation can display identification information and other associated values to aid with point identification and more detailed information to be accessed. Interactive selection of parameters extends the breadth of analysis, and animation interactions such as play, pause, and frame selection are regularly used. 2.2 Multivariate visualization In this thesis, we are concerned with the visualization of multivariate data. Specifically, we are interested in quantitative multivariate data. We assume that our data consists of \\(n\\) observations of \\(p\\) variables. Generally \\(n&gt;p\\) with many more observations than variables. While written as though operating on the original variables, the discussion below could also be applied to reduced component spaces (such as PCA approximation in a few components) or feature decomposition of data not fitting this format. Kang, Hyndman, and Smith-Miles (2017) provide an excellent example of decomposing time series data in a quantitative feature matrix. Grinstein, Trutschl, and Cvek (2002) illustrate many multivariate visualization methods. In particular, this work shows examples of actual visuals. Liu et al. (2017) give a good classification and taxonomy of such methods. The content below focuses on the most common and the most relevant visuals. Illustrations are provided with the Palmer penguins data that was used in the Introduction. This data contains 333 observations of three penguin species across four physical measurements: bill length, bill depth, flipper length, and body mass. Observations were collected between 2007 and 2009 near Palmer Station, Antarctica. The penguins data is a good substitute for the over-used iris data. 2.2.1 Scatterplot matrices An analyst could look at \\(p\\)-univariate histograms or density curves. Extending this idea, pairs of variables can be exhaustively viewed. Combining these brings us to scatterplot matrices, also known as SPLOM (Chambers et al. 1983). In a scatterplot matrix, variables are arranged across the columns and rows. The diagonal elements show univariate densities, while off-diagonal positions show scatterplot pairs, as in Figure 2.2. This is useful for getting a handle on the range of the variables but is not going to scale well when the number of variables \\(p\\) is large. Such visualization will only partially resolve features that could be better show information from more dimensions. (ref:penguinsplom-cap) A scatterplot matrix shows univariate densities and all pairs of bivariate scatterplots. The panels show partial cluster separation, indicating that these variables contain discerning information. This approach is suitable for quickly exploring the range of the data but will not reveal features in more than two dimensions. It is a good exploratory visual but does not scale well with increasing variables. Figure 2.2: (ref:penguinsplom-cap) As \\(n\\) increases, scatterplot displays also suffer from occlusion as the points overlay each other. This is typically addressed in a few ways. One method is to decrease points opacity, allowing more layers to be seen. Another approach is to change the geometric display, such as, 2D density contour or an aggregated heatmap (illustrated in Figure 2.5). Aggregated displays typically render faster and scale better with increasing observations. Or, if needed, visualization can be performed on a representative subset of the data. 2.2.2 Parallel coordinate plots In scatterplot matrices, each observation repeated across all panels. In contrast, observation-linked visuals have a single line or glyph for each observation. In parallel coordinate plots (Ocagne 1885), variables are arranged horizontally, and lines connect observations after being transformed to a common scale such as quantiles or z-score (standard deviations away from the mean). Figure 2.3 illustrates this method. Parallel coordinate plots scale much better with dimensions than scatterplot matrices but more poorly with observations. They also suffer from an asymmetry with the variable order. That is, changing the order of the variables may lead to very different conclusions. The \\(x\\)-axis is also used to display variables rather than the values of the observations. This restricts the amount of information that can be interpreted between variables. Munzner (2014) asserts that position is the more human-perceptible channel for encoding information; we should prefer to reserve it for distinguishing between values of the observations rather than the arrangement of variables. (ref:penguinpcp-cap) Parallel coordinate plots put variables on a common scale and position them side-by-side with lines connecting observations. Some variation between the clusters can be seen corroborating their importance to explaining cluster separation. This approach scales relatively well with the number of variables but poorly with the number of observations. Figure 2.3: (ref:penguinpcp-cap) The same issues persist across displays that map observations into \\(n\\) glyphs or pixel heatmaps. Examples of these include star plots (Chambers et al. 1983), pixel-based visuals (Keim 2000), and Chernoff faces (Chernoff 1973). Like parallel coordinate plots, these other visuals scale quite poorly with increasing observations. However, because these visuals scale well with the number of variables, they may be candidate visualizations for low \\(n\\), high \\(p\\) data. 2.2.3 Dimension reduction The other main approach for visualizing quantitative multivariate data is dimension reduction. This involves a function mapping \\(p\\)-space onto a lower \\(d\\)-dimensional space. Dimension reduction is separated into two categories, linear and nonlinear. The linear case spans all affine mathematical transformations, essentially any function where parallel lines stay parallel. Nonlinear transformations complement the linear case, think transformations containing exponents or interacting terms. Examples in low dimensions are relatable. For instance, shadows are linear projections of a 3-dimensional object down to a 2D shadow. Linear perspective drawings are another instance. An example of a nonlinear transformation is that of 2D maps of the globe. A common example is the Mercator projection, a rectangular display where the area is proportionally distorted with the distance away from the equator (Snyder 1987). Other distortions are created when the surface is unwrapped into an elongated ellipse. Yet others create non-continuous gaps on land or oceans to minimize the distortion of targeted areas. Snyder lists over 200 different projections that distort the surface to display as a map, each with unique properties distorting the 3D surface. However, despite familiarity with map projections, users find it difficult to understand the distortions they introduce (Hennerdal 2015). As illustrated by map projections, nonlinear projections can be challenging to understand. Various computational quality metrics, such as Trustworthiness, Continuity, Normalized stress, and Average local error, have been introduced to describe the distortion of the space (Espadoto et al. 2021; Gracia et al. 2016; Maaten, Postma, and Van den Herik 2009; Venna and Kaski 2006). To quote Panagiotelis (2020), All nonlinear projections are wrong, but some are useful, a play on George Boxs quote about models (All models are wrong, but some are useful, Box 1976). The distortions are hard to interpret and, thus, communicate. Furthermore, nonlinear projections have hyperparameters (absent from linear methods) that control how the spaces are distorted to fit into fewer dimensions. These introduce a degree of subjectivity into the resulting projection. Opinions differ on how to best deal with hyperparameters. Probst, Wright, and Boulesteix (2019) discuss tuning strategies. Others compare implementation default values (Gijsbers et al. 2021; Pfisterer et al. 2021). Probst, Boulesteix, and Bischl (2019) look at the sensitivity of hyperparameters to the performance of a model. Automated machine learning takes a programmatic approach to hyperparameter tuning (Feurer et al. 2015; Hutter, Kotthoff, and Vanschoren 2019; Yao et al. 2019). Due to the difficulty of interpreting nonlinear mappings and the added subjectivity of hyperparameter selection, this thesis focuses on linear visualization techniques. 2.2.4 Intrinsic data dimensionality One way dimension reduction is used is to project multivariate data onto 1-, 2-, or 3D space and visualize the results. However, it can also be used as a preprocessing step for analysis in \\(d\\)-dimensions. The intrinsic dimensionality of data is the number of variables needed to minimally represent the data (Grinstein, Trutschl, and Cvek 2002). Intrinsic data dimensionality is an essential consideration of dimension reduction. Consider a Psychology survey consisting of 100 questions about the Big Five personality traits. The data consists of 100 response variables, while the theory would suggest the intrinsic dimensionality is five. It thus makes sense to project onto five dimensions and analyze and visualize this embedded space rather than the original space as this mitigates the exponentially increasing volume of the view space and the computational load with minimal information loss. 2.3 Linear projections Linear projections map a higher \\(p\\)-dimensional space onto a smaller \\(d\\)-space with an affine mapping (where parallel lines stay parallel). A projection is the resulting space of the data multiplied by a basis \\(Y_{n \\times d} = X_{n \\times p} \\cdot A_{p \\times d}\\). This basis is an orthonormal matrix that explains the orientation and magnitudes that variables contribute to the resulting space. This basis is often illustrated as a biplot (Gabriel 1971), where variable contributions are inscribed in a unit circle showing the direction and the magnitude of contribution as presented in the previous chapter and in Figure 2.4. A standard linear projection is principal component analysis (PCA, Pearson 1901), which creates a component space ordered by descending variation. It uses eigenvalue decomposition to identify the basis. These components are typically viewed as discrete orthogonal pairs, commonly approximated as several components. The exact number of components to keep is subjective but typically guided by a screeplot (Cattell 1966). Scree plots illustrate the decreasing variation contained in subsequent components. The analyst then identifies an elbow in this plot. PCA is also commonly used in preprocessing to reduce the number of dimensions to embed the data in a space corresponding to the intrinsic dimensionality. 2.3.1 Tours, animated linear projections In a static linear projection, there is only one basis. However, a single projection may not reveal the structure of interest. In contrast to this, a data visualization tour shows multiple projections by animating between small changes in the basis. In the shadow analogy, structural information of an object is gained by watching its shadow change due to its rotation. An analyst similarly gains structural information by watching a continuous projection over changes to the basis (essentially a rotation of the data). There are various types of tours that are classified by the generation of their basis paths. We enumerate a few related to this work. A more comprehensive discussion and review of tours can be found in the works of Cook et al. (2008) and Lee et al. (2021). Regardless of the type of tour, they generate a sequence of target bases, and then the tour must interpolate intermediate bases between consecutive bases. This interpolation is performed along a geodesic path between the bases. Geodesic refers to the shortest path (on a \\(p\\)-sphere of possible bases). This ends up being slightly curved in 2D representation for the same reason that flight paths appear curved on maps. The interpolation of bases at small enough angles is foundational for the trackability of observations between frames. Originally in a grand tour (Asimov 1985), several target bases are randomly selected. Figure 2.4 illustrates six frames of a grand tour. The grand tour is suitable for exploratory data analysis in that it will show bases with widely varying contributions but lacks a means of steering the tour and controlling the choice of bases. (ref:ch2fig4-cap) Frames from a grand tour. Biplots (grey circles) illustrate the direction and magnitude that variables contribute. In the grand tour, target bases are selected randomly. Tours animate linear projections over small changes in the basis. The observation permanence between frames is an essential distinction in tours. An animation can be viewed at vimeo.com/676723441. Figure 2.4: (ref:ch2fig4-cap) In contrast, the manual tour (Cook and Buja 1997) allows the analyst to change the contribution of a selected variable. It does so by initializing a manipulation dimension on a 1- or 2D projection which can then be rotated to alter the contribution of a selected variable. This work focuses on the radial tour sub-variant, where the contribution angle is fixed, and the magnitude of contribution along the radius is controlled. We saw an example of this Figure 1.2. Figure 2.5 illustrates the same radial tour shown in the previous chapter across three geometric displays designed to overcome occlusion issues when there are a large number of observations. The use of density contours and aggregated heatmap displays help to mitigate the occlusion of dense observations and is compatible with any scatterplot display. (ref:ch2fig5-cap) Radial tour across three geometric displays. Changing the point opcacity or geometric display to density contours or aggregated heatmap are common ways to mitigate occlusion caused by dense observations. The radial tour removes the contribution of bl and, with it, the separation between the orange and green clusters. Heatmap display is not the best choice to show supervised cluster separation but can be helpful to see structure in dense data. Figure 2.5: (ref:ch2fig5-cap) 2.4 Evaluating multivariate data visualization Definitions and surveys of quality metrics for the distortion of nonlinear reduction are given in Bertini, Tatu, and Keim (2011). The latest, most comprehensive quantitative surveys are discussed in Espadoto et al. (2021) and Nonato and Aupetit (2018). The former compares 44 dimension reduction techniques across 18 datasets over seven metrics. While the former papers mention tours, they are absent from quantitative surveys. Some studies compare visualizations across complete contributions of variables. C. Chang, Dwyer, and Marriott (2018) conduct an \\(n=51\\) participant study comparing parallel coordinate plots and SPLOM either in isolation, sequentially, or as a coordinated view. Accuracy, completion time, and eye focus were measured for six tasks. Three tasks were more accurate with SPLOM and three with parallel coordinates while coordinated view was usually marginally more accurate than the max of the separate visuals. Cao et al. (2018) compare unstandardized line-gylph and star-gylphs with standardized variants (with and without curve fill). Each of the \\(n=18\\) participant perform 72 trials across the six visuals, two levels of dimensions, and two levels of observations. Visuals with variable standardization outperform the unstandardized variants, and the radial star-gylph reportedly outperformed line-variant. Other studies have investigated the relative benefits of projecting to 2- or 3D scatterplots in PCA-reduced spaces. Gracia et al. (2016) conducted an \\(n=40\\) user study comparing 2- and 3D scatterplots on traditional 2D monitors. Participants perform point classification, distance perception, and outlier identification tasks. The results are mixed and primarily have small differences. There is some evidence to suggest a lower error in distance perception from a 3D scatterplot. Wagner Filho et al. (2018) performed an \\(n=30\\) within participants study on PCA reduced space using scatterplot displays between 2D on monitors, 3D on monitors, and 3D display with a head-mounted display. None of the tasks on any dataset lead to a significant difference in accuracy. However, the immersive display reduced effort and navigation, resulting in higher perceived accuracy and engagement. Some studies use an expert or cohort coding. Sedlmair, Munzner, and Tory (2013) instead use two expert coders to evaluate 75 datasets and four dimension reduction techniques across 2D scatterplots, 2D scatterplot matrices, and interactive 3D scatterplots. They suggest a tiered guidance approach finding that 2D scatterplots are often sufficient to resolve a feature. If not, try an alternative dimension reduction technique before going to scatterplot matrix display or concluding a true negative. They find that interactive 3D scatterplots help in relatively rare cases. Lewis, Van der Maaten, and Sa (2012) compare across three cohorts: experts, uninformed novices, and informed novices (\\(n=5+15+16=36\\)). Participants were asked their opinion of the quality of nine different embedding for each of sever data sets. Expert opinion in reportedly more consistent than the novice groups, though this is confounded with the different sample sizes. Interestingly, cohort responses correlated with different quality metrics. Positive ratings from the expert group correlated strongest with the Trustworthiness metric. Tours are absent from studies calculable quality measures. However, Nelson, Cook, and Cruz-Neira (1998) compare scatterplots of grand tours on a 2D monitor with 3D display (stereoscopic, not head-mounted) over \\(n=15\\) participants. Participants perform clusters detection, dimensionality, and radial sparseness tasks on six-dimensional data. They find that stereoscopic 3D leads to more accuracy for cluster identification, though interaction time greatly increased in the 3D case. 2.5 Nonlinear models So far, the thesis has focused on exploratory data analysis. Another core part of data analysis is fitting regression models of quantitative data (Galton 1886) and classification models for discrete categories (Fisher 1935). There are different reasons and emphases when considering to fit a model. Breiman (2001b), reiterated by Shmueli (2010), taxonomize models based on their purpose. Explanatory modeling is done for some inferential purpose, while predictive modeling focuses more narrowly on the performance of some objective function. The intended use has important implications for model selection and development. In explanatory modeling, interpretability is vital for drawing inferential conclusions. Nonlinear models range from additive models with at least one polynomial term to more complex machine learning models such as random forests, gradient boosting, or neural network, to name a few (Breiman 2001a; Friedman 2002; Anderson 1995, receptively). Nonlinear models have many or complex interactive terms, which cause an opaqueness to the interpretation of the variables. This difficulty in interpreting the terms in complex nonlinear models sometimes lead to them being referred to as black box models. However, the use and prevalence of black box models is not without controversy (ONeil 2016; Kodiyan 2019). And the loss of interpretation presents a challenge. Interpretability is vital for exploring and protecting against potential biases in any model (e.g. sex  Dastin (2018); Duffy (2019), race  Larson et al. (2016), and age  Díaz et al. (2018)). For instance, models regularly pick up on biases in the training data where such classes correlate with changes in the response variable. This bias is then built into the model. Variable-level (feature-level) interpretability of models is essential in evaluating and addressing such biases. Another concern is data drift, where a shift in the range of the explanatory variables (features or predictors). Some nonlinear models are sensitive to this and do not extrapolate well outside the support of the training data. Maintaining variable interpretability is also essential to address issues arising from data drift. 2.6 Local explanations Explainable Artificial Intelligence (XAI) is an emerging field of research that aims to increase the interpretability of nonlinear models. A common approach is local explanations, which attempt to approximate linear variable importance in the vicinity of one observation (instance). This is a linear measure indicating which variables are essential for distinguishing between the mean of the data and the prediction near one observation. Because these are point-specific, the challenge is to comprehensively visualize them to understand a model. Consider a highly nonlinear model. It can be hard to determine which variables in the data are sensitive to changes in the classification or sizable changes in a residual. Local explanations shed light on these situations by approximating linear variable importance in the vicinity of a single observation. Figure 2.6 motivates local explanations where the analyst wants to know the variable attribution for a particular observation close to the classification boundary in a nonlinear model. (ref:ch2fig6-cap) Illustration of a nonlinear classification model. An analyst may want to know the variable importance in the vicinity of the highlighted red cross. Understanding this attribution elucidates how the variables influence this point that is precariously close to the classification boundary. Local explanations approximate this linear attribution in the vicinity of one observation. Figure from Ribeiro, Singh, and Guestrin (2016). Figure 2.6: (ref:ch2fig6-cap) A comprehensive summary of the taxonomy and literature of explanation techniques is provided in Figure 6 of Arrieta et al. (2020). It includes a large number of model-specific explanations such as deepLIFT (Shrikumar et al. 2016; Shrikumar, Greenside, and Kundaje 2017), a popular recursive method for estimating importance in neural networks. There are fewer model-agnostic explanations, of which LIME, (Ribeiro, Singh, and Guestrin 2016) SHAP, (Lundberg and Lee 2017), and their variants are popular. These observation-level explanations are used in various ways depending on the context. In image classification, a saliency map indicate necessary pixels for the resulting classification (Simonyan, Vedaldi, and Zisserman 2014). For example, snow may be highlighted when distinguishing if a picture contains a wolf or husky (Besse et al. 2019). In text analysis, word-level contextual sentiment analysis can be used to highlight the sentiment and magnitude of influential words (Vanni et al. 2018). In the case of numeric regression, they are used to explain variable additive contributions from the observed mean to the observations prediction (Ribeiro, Singh, and Guestrin 2016). 2.7 Conclusion This chapter discussed the motivation for data visualization showing more information than statistic summarization alone. User interaction is important for linking observation and auxiliary information between coordinated views and animation frames. Several visuals were discussed before turning to dimension reduction. Because of the opaqueness of the distortions and added subjectivity from nonlinear dimension reduction this thesis focused more on viewing many linear projection as the basis changes in tours. The radial tour with user-control steering is of particular interest. Metric and empirical evaluations of multivariate data were discussed. Nonlinear models, and XAIs use of local explanations to extend the interpretability of models was also covered. There is an absence of studies comparing animated tours with alternative visualizations. The following chapters respectively address the three research questions covered in the Introduction. Chapter 3 discusses the implementation of a package that facilitates the creation of radial tours and extends the display and exporting of tours in general. Chapter 4 covers the first user study evaluating the radial tour compared with two common alternatives. Chapter 5 introduces a novel analysis that explores local explanations of nonlinear models with the radial tour. "],["3-ch-spinifex.html", "Chapter 3 A User-Controlled Manual Tour for Animated Linear Projections 3.1 Algorithm 3.2 Oblique cursor movement 3.3 Package structure 3.4 Use cases 3.5 Discussion", " Chapter 3 A User-Controlled Manual Tour for Animated Linear Projections Dynamic low-dimensional linear projections of multivariate data known as tours are essential for exploring multivariate data and models. The R package tourr facilitates several types of tours: grand, guided, little, local, and frozen. Each of these can be viewed in a development environment, or their basis path can be saved for later consumption. This chapter describes a new package, spinifex, which creates manual tours (Cook et al. 1995) of multivariate data. In a manual tour, an analyst controls the contribution of a variable to the projection. Controlled manipulation is important to explore a variables sensitivity to the structure of an identified feature. The radial tour is a sub-variant that fixes the contribution angle and alters the magnitude along the radius. The radial tour is applied to particle physics data to illustrate the structures sensitivity in a projection to specific variable contributions. Additionally, this package creates a ggproto API for composing any tour that mirrors the layered additive approach of ggplot2. Tours can then be animated and exported to various formats with plotly or gganimate. The chapter is organized as follows. Section 3.1 provides an illustrated detailed description of a 2D manual tour and fills in the previously absent details to solve the 3D rotation matrix used in 2D manual tours. This proves a scaffolding for the extension to solving a 4D rotation matrix for a 3D manual tour. Section 3.2 includes compact Algorithms for capturing oblique movements from a cursor. Section 3.3 discusses the functions and code usage to perform radial tours and layered display of tours. Section 3.4 illustrates a use case of the radial tour facilitating sensitivity analysis on high-energy physics experiments (Wang et al. 2018). Section 3.5 concluded with a discussion of this chapter. 3.1 Algorithm The types of manipulations of the manual tour can be thought of in several ways: radial: fix the direction of contribution, and allow the magnitude to change. angular: fix the magnitude, and allow the angle or direction of the contribution to vary. horizontal, vertical: changing the contribution in horizontal or vertical directions. oblique: paths deviating from these movements, such as being captured from the movement of a cursor. Angular manipulations are homomorphic in that they show the same information while rotating the frame. More interesting is a change in the magnitude of the contribution, changing the radius along the original contribution angle. For this reason, we implement the radial tour as the default for the manual tour. Below we describe the manual tour illustrated in detail. After that, the algorithms for oblique cursor movements in 1- and 2D are covered. 3.1.1 Notation The notation used to describe the algorithm for a 2D radial manual tour is as follows: \\(\\textbf{X}\\), the data, an \\(n \\times p\\) numeric matrix to be projected. \\(\\textbf{A}\\), any orthonormal projection basis, \\(p \\times d\\) matrix, describing the projection from \\(\\mathbb{R}^p \\Rightarrow \\mathbb{R}^d\\). \\(k\\), is the index of the manipulation variable or manip var for short. \\(\\textbf{e}\\), a 1D basis vector of length \\(p\\), with 1 in the \\(k\\)-th position and 0 elsewhere. \\(\\textbf{R}\\), the \\(d+1\\)-D rotation matrix, performs unconstrained 3D rotations within the manip space, \\(\\textbf{M}\\). \\(\\theta\\), the angle of in-projection rotation, for example, on the reference axes; \\(c_\\theta, s_\\theta\\) are its cosine and sine. \\(\\phi\\), the angle of out-of-projection rotation, into the manip space; \\(c_\\phi, s_\\phi\\) are its cosine and sine. The initial value for animation purposes is \\(\\phi_1\\). \\(\\textbf{U}\\), the axis of rotation for out-of-projection rotation orthogonal to \\(\\textbf{e}\\). \\(\\textbf{Y} = \\textbf{X} \\times \\textbf{A}\\), the resulting data projection through the manip space, \\(\\textbf{M}\\), and rotation matrix, \\(\\textbf{R}\\). The algorithm operates entirely on projection bases and incorporates the data only when making the projected data plots in light of efficiency. 3.1.2 Steps 3.1.2.1 Step 0) Setup The flea data (Lubischew (1962)), available in the tourr package (Wickham et al. 2011), is used to illustrate the algorithm. The data contains 74 observations of six variables, physical measurements of flea beetles. Each observation belongs to one of three species. Each variable is normalized to a common range of [0, 1]. An initial 2D projection basis must be provided. A suggested way to start is to identify an interesting projection using a projection pursuit guided tour. The holes index is used to find a 2D projection of the flea data, which shows three separated species groups. Figure 3.1 shows the initial projection of the data. On the left, a biplot (Gabriel 1971) illustrates the projection basis (\\(\\textbf{A}\\)), showing each variables magnitude and direction of contribution to the projection. The right side shows the projected data, \\(\\textbf{Y}_{[n,~2]} ~=~ \\textbf{X}_{[n,~p]} \\textbf{A}_{[p,~2]}\\). The color and shape of points are mapped to the flea species. Figure 3.1: Biplot of the initial 2D projection of normalized flea data. The basis is depicted on the left and resulting data projection is on the right. The color and shape of data points are mapped to the flea beetle species. The basis was produced by a projection pursuit guided tour with the holes index. The contribution of the variables aede2 and tars1 approximately contrasts the other variables. The visible structure in the projection is the three species clusters. 3.1.2.2 Step 1) Choose manip variable In figure 3.1 the contribution of the variables tars1 and aede2 mostly contrast the contribution of the other four variables. These two variables combined contribute in the direction where the purple cluster is separated from the other two clusters. The variable aede2 is selected as the manip var, the variable whoes contribution is being changed. The aspect being explored is: how important is this variable to separating the clusters in this projection? 3.1.2.3 Step 2) Create the 3D manip space Initialize a zero vector, \\(\\textbf{e}\\), of length \\(p\\), and set the \\(k\\)-th element to 1. In the example data, aede2 is the fifth variable in the data, so \\(k=5\\), set \\(e_5=1\\). Append this vector to the current basis as the third column. Use a Gram-Schmidt process to orthonormalize the coordinate basis vector on the original 2D projection to describe a 3D manip space, \\(\\textbf{M}\\). \\[\\begin{align*} e_k &amp;\\leftarrow 1 \\\\ \\textbf{e}^*_{[p,~1]} &amp;= \\textbf{e} - \\langle \\textbf{e}, \\textbf{A}_1 \\rangle \\textbf{A}_1 - \\langle \\textbf{e}, \\textbf{A}_2 \\rangle \\textbf{A}_2 \\\\ \\textbf{M}_{[p,~3]} &amp;= (\\textbf{A}_1,\\textbf{A}_2,\\textbf{e}^*) \\end{align*}\\] The manip space provides a 3D projection from \\(p\\)-dimensional space, where the coefficient of the manip var can range completely between [0, 1]. This 3D space serves as the medium to rotate the projection basis relative to the selected manipulation variable. Figure 3.2 illustrates this 3D manip space with the manip var highlighted. This representation is produced by calling the view_manip_space() function. This diagram is purely used to help explain the algorithm. Figure 3.2: Illustration of a 3D manip space, the projection plane is shown as a blue circle extending into and out of the display. A manipulation direction is initialized, the red circle, orthogonal to the projection plane. This allows the selected variable, aede2, to change its contribution to the projection plane. The other variables contributions rotate into this space, preserving the orthogonal basis, but are omitted in the manipulation dimension for simplicity. 3.1.2.4 Step 3) Defining a 3D rotation The basis vector corresponding to the manip var (red line in Figure 3.2) can be operated like a lever anchored at the origin. This manual control process rotates the manip variable into and out of the 2D projection (Figure 3.3). As the variable contribution is controlled, the manip space turns, and the projection onto the horizontal plane correspondingly changes. This is a manual tour. Generating a sequence of values for the rotation angles produces a path for the rotation of the manip space. For a radial tour, fix \\(\\theta\\), the angle describing rotation within the horizontal projection plane, and compute a sequence for \\(\\phi\\), defining movement out of this plane. This will change \\(\\phi\\) from the initial value, \\(\\phi_1\\), the angle between \\(\\textbf{e}\\) and its shadow in \\(\\textbf{A}\\), to a maximum of \\(0\\) (manip var entirely in projection), then to a minimum of \\(\\pi/2\\) (manip var orthogonal to the projection), before returning to \\(\\phi_1\\). Rotations in 3D can be defined by the axes they pivot on. Rotation within the projection, \\(\\theta\\), is rotation around the \\(Z\\)-axis. Out-of-projection rotation, \\(\\phi\\), is the rotation around an axis on the \\(XY\\) plane, \\(\\textbf{U}\\), orthogonal to \\(\\textbf{e}\\). Given these axes, the rotation matrix, \\(\\textbf{R}\\), can be written as follows, using Rodrigues rotation formula (originally published in Rodrigues (1840)): \\[\\begin{align*} \\textbf{R}_{[3,~3]} &amp;= \\textbf{I}_3 + s_\\phi\\*\\textbf{U} + (1-c_\\phi)\\*\\textbf{U}^2 \\\\ &amp;= \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} + \\begin{bmatrix} 0 &amp; 0 &amp; c_\\theta s_\\phi \\\\ 0 &amp; 0 &amp; s_\\theta s_\\phi \\\\ -c_\\theta s_\\phi &amp; -s_\\theta s_\\phi &amp; 0 \\\\ \\end{bmatrix} + \\begin{bmatrix} -c_\\theta (1-c_\\phi) &amp; s^2_\\theta (1-c_\\phi) &amp; 0 \\\\ -c_\\theta s_\\theta (1-c_\\phi) &amp; -s^2_\\theta (1-c_\\phi) &amp; 0 \\\\ 0 &amp; 0 &amp; c_\\phi-1 \\\\ \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} c_\\theta^2 c_\\phi + s_\\theta^2 &amp; -c_\\theta s_\\theta (1 - c_\\phi) &amp; -c_\\theta s_\\phi \\\\ -c_\\theta s_\\theta (1 - c_\\phi) &amp; s_\\theta^2 c_\\phi + c_\\theta^2 &amp; -s_\\theta s_\\phi \\\\ c_\\theta s_\\phi &amp; s_\\theta s_\\phi &amp; c_\\phi \\end{bmatrix} \\\\ \\end{align*}\\] where \\[\\begin{align*} \\textbf{U} &amp;= (u_x, u_y, u_z) = (s_\\theta, -c_\\theta, 0) \\\\ &amp;= \\begin{bmatrix} 0 &amp; -u_z &amp; u_y \\\\ u_z &amp; 0 &amp; -u_x \\\\ -u_y &amp; u_x &amp; 0 \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 0 &amp; -c_\\theta \\\\ 0 &amp; 0 &amp; -s_\\theta \\\\ c_\\theta &amp; s_\\theta &amp; 0 \\\\ \\end{bmatrix} \\\\ \\end{align*}\\] 3.1.2.5 Step 4) Creating an animation of the radial rotation The steps outlined above can be used to create any arbitrary rotation in the manip space. To use this for sensitivity analysis, the radial rotation is built into an animation where the manip var is rotated fully into the projection, completely out, and then back to the initial value. This involves varying \\(\\phi\\) between \\(0\\) and \\(\\pi/2\\), call the steps \\(\\phi_i\\). Figure 3.3: Select frames of a radial tour manipulating aede2: (1) original projection, (2) full contribution, (3) zero contribution. After zeroing the contribution the animation continues to return to the initial contribution. Set the initial value of \\(\\phi_1\\) and \\(\\theta\\): \\(\\phi_1 = \\cos^{-1}{\\sqrt{A_{k1}^2+A_{k2}^2}}\\), \\(\\theta = \\tan^{-1}\\frac{A_{k2}}{A_{k1}}\\). Where \\(\\phi_1\\) is the angle between \\(\\textbf{e}\\) and its shadow in \\(\\textbf{A}\\). Set an angle increment (\\(\\Delta_\\phi\\)) that sets the step size for the animation, to rotate the manip var into and out of the projection. The package spinifex uses of angle increment, rather than some steps to control the movement to be consistent with the tour algorithm as implemented in tourr. Step towards \\(0\\), where the manip var is entirely in the projection plane. Step towards \\(\\pi/2\\), where the manip variable has no contribution to the projection. Step back to \\(\\phi_1\\). In each of the steps 3-5, a small step may be added to ensure that the endpoints of \\(\\phi\\) (\\(0\\), \\(\\pi/2\\), \\(\\phi_1\\)) is reached. 3.1.2.6 Step 5) Projecting the data The operation of a manual tour is defined on the projection bases. Only when the data plot needs to be made, the data is projected through the relevant basis. \\[\\begin{align*} \\textbf{Y}^{(i)}_{[n,~3]} &amp;= \\textbf{X}_{[n,~p]} \\textbf{M}_{[p,~3]} \\textbf{R}^{(i)}_{[3,3]} \\end{align*}\\] where \\(\\textbf{R}^{(i)}_{[3,3]}\\) is the incremental rotation matrix, using \\(\\phi_i\\). To make the data plot, use the first two columns of . Show the projected data for each frame in sequence to form an animation. Tours are typically viewed as an animation. The animation of this tour can be viewed online on GitHub. 3.2 Oblique cursor movement In a move abbreviated way, we can think about the algorithm for 1D and 2D oblique manual tours as: 3.3 Package structure In addition to facilitating the manual tour, the other primary function is to facilitate the layered composition of tours interoperability with tours from tourr. This package attempts to abstract the complexity of dealing with a varying number of frames and replicating the length of arguments. We use a layered composition approach to tours similar to ggplot2 (Wickham 2016). The resulting displays then be animated with plotly (Sievert 2020) or gganimate (Pedersen and Robinson 2020). This section describes the functions available in the package, their usage, and how to install and get up and running. 3.3.1 Usage The penguins data (Gorman, Williams, and Fraser 2014; Horst, Hill, and Gorman 2020), also available in spinifex , is used to illustrate the creation of a radial tour, layered composition, and animation. Similar to the flea data, this is exploring the variable contribution that leads to the separation of clusters. library(spinifex) ## Process penguins data dat &lt;- scale_sd(penguins_na.rm[1:4]) clas &lt;- penguins_na.rm$species ## Start basis and manual tour path bas &lt;- basis_olda(data = dat, class = clas) mt_path &lt;- manual_tour(basis = bas, manip_var = 1, data = dat) ## Compose the tour display ggt &lt;- ggtour(mt_path, angle = .15) + proto_point(aes_args = list(color = clas, shape = clas), identity_args = list(alpa = .8, size = 1.5)) + proto_basis() + proto_origin() ## Animate animate_plotly(ggt, fps = 5) Composition and animation options are interoperable with tourr produced tours. The code below similarly composes a 1D tour from a saveed grand tour path. ## A grand tour from tourr library(tourr) gt_path &lt;- save_history(data = dat, tour_path = grand_tour(), max_bases = 10) ## Compose 1D tour display ggt2 &lt;- ggtour(gt_path, angle = .15) + proto_density(aes_args = list(color = clas, fill = clas)) + proto_basis1d() + proto_origin1d() ## Animate animate_plotly(ggt2, fps = 5) 3.3.2 Functions Table 3.1 lists the primary functions and their purpose. These are grouped into four classes: processing the data, production of tour path, the composition of the tour display, and its animation. Table 3.1: Summary of primary functions. Family Function Related to Description processing scale_01/sd scale each column to [0,1]/std dev away from the mean processing basis_pca/olda/ Rdimtools::do.* basis of orthogonal component spaces processing basis_half_circle basis with uniform contribution across half of a circle processing basis_guided tourr::guided_tour silently return the basis from a guided tour tour path manual_tour basis and interpolation information for a manual tour tour path save_history tourr::save_history silent, extended wrapper returning other tour arrays display ggtour ggplot2::ggplot canvas and initialization for a tour animation display proto_point/text geom_point/text adds observation points/text display proto_density/2d geom_density/2d adds density curve/2d contours display proto_hex geom_hex adds hexagonal heatmap of observations display proto_basis/1d adds adding basis visual in a unit-circle/-rectangle display proto_origin/1d adds reference mark in the center of the data display proto_default/1d wrapper for proto_* point + basis + origin display facet_wrap_tour ggplot2::facet_wrap facets on the levels of variable display append_fixed_y add/overwrite a fixed vertical position animation animate_plotly plotly::ggplotly render as an interactive hmtl widget animation animate_gganimate gganimate::animate render as a gif, mp4, or other video formats animation filmstrip static gpplot faceting on the frames of the animation 3.3.3 Installation The spinifex is available from CRAN. The following code will help to get up and running: # Installation: install.package(&quot;spinifex&quot;) ## Install from CRAN library(&quot;spinifex&quot;) ## Load into session # Getting started: ## Shiny app for visualizing basic application run_app(&quot;intro&quot;) ## View the code vignette vignette(&quot;getting_started_with_spinifex&quot;) ## More about proto_* functions vignette(&quot;ggproto_api&quot;) 3.4 Use cases Wang et al. (2018) introduces a new tool, PDFSense, to visualize the sensitivity of hadronic experiments to nucleon structure. The parameter-space of these experiments lies in 56 dimensions, and are approximated as the ten first principal components. Cook, Laa, and Valencia (2018) illustrate using the grand tour to explore this component reduced space. Tours can better resolve the shape of clusters, intra-cluster detail, and lead to better outlier detection than PDFSense &amp; TFEP (TensorFlow embedded projections) or traditional static embeddings. Building on this, the manual tour is used to examine the sensitivity of structure in a projection to different parameters. Bases from the grand tour identified in the previous work are used as the initial basis. The data has a hierarchical structure with top-level clusters; DIS, VBP, and jet. Each cluster is a particular class of experiments, each with many experimental datasets. In consideration of data occlusion, we conduct manual tours on subsets of the DIS and jet clusters. This explores the structures sensitivity to each of the variables in turn, and we present the subjectively best and worst variable to manipulate for identifying dimensionality of the clusters and describing the range of the clusters. 3.4.1 Jet cluster The jet cluster resides in a smaller dimensionality than the full set of experiments, with four principal components explaining 95% of the variation in the cluster (Cook, Laa, and Valencia 2018). The data within this 4D embedding is a further subset to ATLAS7old and ATLAS7new, focuses on two groups that occupy different parts of the subspace. Radial manual tours controlling contributions from PC4 and PC3 are shown in Figures 3.4 and 3.5, respectively. The difference in shape can be interpreted as the experiments probing different phase spaces. Back-transforming the principal components to the original variables can be done for a more detailed interpretation. When PC4 is removed from the projection (Figure 3.4), the difference between the two groups is removed, indicating that PC4 is essential for separating types of experiments. However, eliminating PC3 from the projection (Figure 3.5) does not affect the structure, meaning PC3 is not essential for distinguishing experiments. Animations for the remaining PCs can be viewed at the following links: PC1, PC2, PC3, and PC4. It can be seen that only PC4 is vital for viewing the difference in these two experiments. Figure 3.4: Select frames from a radial tour of PC4 within the jet cluster, with color indicating experiment type: ATLAS7new (green) and ATLAS7old (orange). When PC4 is removed from the projection (frame 10), there is little difference between the clusters, suggesting that PC4 is vital for distinguishing the experiments. Figure 3.5: Frames from the radial tour manipulating PC3 within the jet cluster, with color indicating experiment type: ATLAS7new (green) and ATLAS7old (orange). When the contribution from PC3 is changed, there is little change in the separation of the clusters, suggesting that PC3 is not important for distinguishing the experiments. 3.4.2 DIS cluster Following Cook, Laa, and Valencia (2018), PCA is used to explore the DIS cluster, and the first six principal components are used, which contain 48% of the full sample variation. The contributions of PC6 and PC2 are explored in Figures 3.6 and 3.7, respectively. Three experiments are examined: DIS HERA1+2 (green), dimuon SIDIS (purple), and charm SIDIS (orange). Both PC2 and PC6 contribute to the projection similarly. When PC6 is rotated into the projection, variation in the DIS HERA1+2 is greatly reduced. When PC2 is removed from the projection, dimuon SIDIS becomes more distinct. Even though both variables contribute similarly to the original projection, their contributions have quite different effects on the structure of each cluster and the distinction between clusters. Animations of all of the principal components can be viewed from the links: PC1, PC2, PC3, PC4, PC5, and PC6. Figure 3.6: Select frames from a radial tour exploring the sensitivity that PC6 has on the structure of the DIS cluster, with color indicating experiment type: DIS HERA1+2 (green), dimuon SIDIS (purple), and charm SIDIS (orange). DIS HERA1+2 is distributed in a cross-shaped plane, and charm SIDIS occupies the center of this cross, and dimuon SIDIS is a linear cluster crossing DIS HERA1+2. As the contribution of PC6 is increased, DIS HERA1+2 becomes almost singular in one direction (frame 5), indicating that this cluster has minimal variability in the direction of PC6. Figure 3.7: Frames from the radial tour exploring the sensitivity PC2 to the structure of the DIS cluster, with color indicating experiment type: DIS HERA1+2 (green), dimuon SIDIS (purple), and charm SIDIS (orange). As the contribution of PC2 is decreased, dimuon SIDIS becomes more distinguishable from the other two clusters, indicating that the absence of PC2 is vital for separating this cluster from the others. 3.5 Discussion Dynamic linear projections of numeric multivariate data, tours, play an important role in data visualization; they extend the dimensionality of visuals to peek into high-dimensional data and parameter spaces. This research has taken the manual tour algorithm, specifically the radial rotation, used in GGobi (Swayne et al. 2003) to interactively rotate a variable into or out of a 2D projection and modified it to create an animation that performs the same task. It is most helpful in examining the importance of variables and how the structure in the projection is sensitive or not to specific variables. This functionality is made available in the package spinifex. Which also extends the geometric display and export formats interoperable with the tourr package. The original conception of the grand tour was motivated by problems in physics (Asimov 1985) Thus, the use case was fitting. The radial tour was used to explore the sensitivity of variable contributions to the separation of different experiments in high-energy particle physics. These tools can be applied quite broadly to many multivariate data analysis problems. The manual tour is constrained because the effect of one variable depends on the contributions of other variables in the manip space. However, this can simplify a projection by removing variables without affecting the visible structure. Defining a manual rotation in high dimensions is possible using Givens rotations and Householder reflections as outlined in Buja et al. (2005). This would provide more flexible manual rotation but more difficult for a user because they have the choice (too much choice) of which directions to move. "],["4-ch-userstudy.html", "Chapter 4 A Study on the Benefit of a User-Controlled Radial Tour for Variable Importance for Structure in High-Dimensional Data 4.1 User study 4.2 Results 4.3 Conclusion", " Chapter 4 A Study on the Benefit of a User-Controlled Radial Tour for Variable Importance for Structure in High-Dimensional Data The previous chapter introduced the package spinifex, which gave us the means to perform radial tours. However, there is no evidence suggesting that the radial tours user-controlled steering leads to a better perception than traditional methods. Therefore, this chapter discusses the user study to elucidate the efficacy of the radial tour. Chapters 1 and 2 introduced PCA, the grand tour, and the radial tour. This chapter describes a within-participants user study evaluating the efficacy of these techniques. A supervised classification task is designed where participants evaluate variable attribution of the separation between two classes. An accuracy measure is defined as a response variable. Data were collected from 108 crowdsourced participants, who performed two trials with each visual for 648 trials in total. The user influence over a basis is crucial to testing variable sensitivity to the structure visible in projection and uniquely available in the radial tour. If the contribution of a variable is reduced, and the feature disappears, then it is said that the variable is sensitive to that structure. For example, Figure 4.1 shows two projections of simulated data. Panel (a) has identified separation between the two clusters and primarily in the direction of the contribution from V2. Many other bases do not reveal this cluster separation, such as panel (b) with a small contribution from V2. Because of this, it is said that V2 is sensitive to the separation of the clusters. Figure 4.1: Illustration of cluster separation. Panel (a) shows clear separation in the direction of V2 and no separation in the direction of V3. While the contributions of V1 and v4 are relatively small and contributed little to this orientation. Panel (b) has a minimal contribution from V2, and no separation between the cluster means is resolved. Understanding which variables to use is also important to statistical modeling and their interpretations. Models are becoming increasingly complex, and the nonlinear interactions of their terms cause an opaqueness to model interpretability. Exploratory Artificial Intelligence (XAI, Adadi and Berrada 2018; Arrieta et al. 2020) is an emerging field that extends the interpretability of such models. Multivariate data visualization is essential for exploring features spaces and communicating interpretations of models (Biecek 2018; Biecek and Burzykowski 2021; Wickham, Cook, and Hofmann 2015). This chapter is structured as follows. Section 4.1 describes the implementation of the user study; its visual methods, experimental factors, task, accuracy measure used standardization and randomization of the experiment. The results of the study are discussed in Section 4.2. Conclusions and potential future directions are discussed in Section 4.3. An accompanying application and extended analysis are provided in the appendix under Section A.1. 4.1 User study An experiment was constructed to assess the performance of the radial tour relative to the grand tour and PCA for interpreting the variable attribution contributing to separation between two clusters. Data were simulated across three experimental factors: location of the cluster separation, cluster shape, and data dimensionality. Participant responses were collected using a web application and crowdsourced through prolific.co, (Palan and Schitter 2018) an alternative to MTurk. 4.1.1 Objective PCA will be used as a baseline for comparison as it is the most commonly used linear embedding. It will use static, discrete jumps between orthogonal components. The grand tour will act as a secondary control that will help evaluate the benefit of observation trackability between nearby animation frames but without user-control of its path. Lastly, the radial tour will be compared, which benefits from the continuity of animation and user control of the basis. Then for some subset of tasks, we expect to find that the radial tour performs most accurately. Conversely, we are less sure about the accuracy of such limited grand tours as there is no objective function in selecting the bases; it is possible that the random selection of the target bases altogether avoids bases showing cluster separation. However, given that the data dimensionality is modest, it seems plausible that the grand tour coincidentally regularly crossed bases with the correct information for the task. Experimental factors and the definition of an accuracy measure are given below. The null hypothesis can be stated as: \\[\\begin{align*} &amp;H_0: \\text{accuracy does not change across the visual methods} \\\\ &amp;H_\\alpha: \\text{accuracy does change across the visual methods} \\end{align*}\\] 4.1.2 Experimental factors In addition to the visual method, data are simulated across three experimental factors. First, the location of the separation between clusters is controlled by mixing a signal and a noise variable at different ratios. Secondly, the shape of the clusters reflects varying distributions of the data. And third, the dimension-ality of the data is also tested. The levels within each factor are described below, and Figure 4.2 gives a visual representation. Figure 4.2: Illustration of the experimental factors, the parameter space of the independent variables. The location of the separation between the clusters is at the heart of the measure. It would be good to test a few varying levels. To test the sensitivity, a noise, and signal-containing variable are mixed. The separation between clusters are mixed at the following percentages: 0/100% (not mixed), 33/66%, 50/50% (evenly mixed). In selecting the shape of the clusters, the convention given by Scrucca et al. (2016) is followed. They describe 14 variants of model families containing three clusters. The model family name is the abbreviation of the clusters respective volume, shape, and orientation. The levels are either Equal or Vary. The models EEE, EEV, and EVV are used. For instance, in the EEV model, the volume and shape of clusters are constant, while the shapes orientation varies. The EVV model is modified by moving four-fifths of the data out in a &gt; or banana-like shape. Dimension-ality is tested at two modest levels: four dimensions containing three clusters and six with four clusters. Such modest dimensionality is required to limit the difficulty and search space to make the task realistic for crowdsourcing. 4.1.3 Task and evaluation With our hypothesis formulated and data at hand, let us turn our attention to the task and how to evaluate it. Regardless of the visual method, the display elements are held constant, shown as a 2D scatterplot with a biplot (Gabriel 1971) to its left. A biplot is a visual depiction of the variable contributions from the basis inscribed in a unit circle. Observations were supervised with cluster membership mapped to (colorblind safe) colors and shape. Participants were asked to check any/all variables that contribute more than average to the cluster separation green circles and orange triangles, which was further explained in the explanatory video as mark any and all variable that carries more than their fair share of the weight, or one quarter in the case of four variables. The instructions were iterated several times in the video was: 1) use the input controls to find a frame that contains separation between the clusters of green circles and orange triangles, 2) look at the orientation of the variable contributions in the gray circle (biplot axes orientation), and 3) select all variables that contribute more than uniformed distributed cluster separation in the scatterplot. Independent with experimental level, participants were limited to 60 seconds for each evaluation of this task. This restriction did not impact many participants as the 25th, 50th, 75th quantiles of the response time were about 7, 21, and 30 seconds, respectively. The accuracy measure of this task was designed with a couple of features in mind. 1) symmetric about the expected value, that is, without preference to under- or over-guessing. 2) heavier than linear weight with an increasing difference from the expected value. The following measure is defined for evaluating the task. Let the data \\(\\textbf{X}_{n,~p,~k}\\) be a simulation containing clusters of observations of different distributions. Where \\(n\\) is the number of observations, \\(p\\) is the number of variables, and \\(k\\) indicates the observations cluster. Cluster membership is exclusive; an observation cannot belong to more than one cluster. The weights, \\(w\\), is a vector, the variable-wise difference between the mean of two clusters of less \\(1/p\\), the expected cluster separation if it were uniformly distributed. Accuracy, \\(A\\) is defined as the signed square of these weights if selected by the participant. Participant responses are a logical value for each variable  whether or not the participant thinks each variable separates the two clusters more than uniformly distributed separation. \\[\\begin{equation*} w_{j} = \\frac{ (\\overline{X}_{\\cdot, j=1, k=1} - \\overline{X}_{\\cdot, 1, 2}, ~...~ (\\overline{X}_{\\cdot, p, 1} - \\overline{X}_{\\cdot, p, 2})} {\\sum_{j=1}^{p}(|\\overline{X}_{\\cdot, j, k=1} - \\overline{X}_{\\cdot, j, 2}|)} - \\frac{1}{p} \\end{equation*}\\] \\[\\begin{equation*} A = \\sum_{j=1}^{p}I(j) \\cdot sign(w_j) \\cdot w^2 \\end{equation*}\\] Where \\(I(j)\\) is the indicator function, the binary response for variable \\(j\\). Figure 4.3 shows one projection of a simulation with its observed variable separation (wide bars), expected uniform separation (dashed line), and accuracy if selected (thin lines). Figure 4.3: Illustration of how accuracy is measured. (L), Scatterplot and biplot of PC1 by PC4 of a simulated data set (R) illustrates cluster separation between the green circles and orange triangles. Bars indicate observed cluster separation, and (red/green) lines show the accuracy weight of the variable if selected. The horizontal dashed line is \\(1 / p\\), the expected value of cluster separation. The accuracy weights equal the signed square of the difference between each variable value and the dashed line. 4.1.4 Visual design standardization The visual methods are tested within-participant, with each visual being evaluated twice by each participant. The order that experimental factors are experienced is randomized with the assignment, as illustrated in Figure 4.4. Below discusses the design standardization and unique input associated with each visual. The visualization methods were standardized wherever possible. Data were displayed as 2D scatterplots with biplots. All aesthetic values (colors, shapes, sizes, absence of legend, and axis titles) were constant. Variable contributions were always shown left of the scatterplot embeddings with their aesthetic values consistent. What did vary between visuals were their inputs. PCA allowed users to select between the top four principal components for each axes regardless of the data dimensionality (four or six). Upon changing an axis, the visual would change to the new view of orthogonal components without displaying intermediate bases. There was no user input for the grand tour; users were instead shown a 15-second animation of the same randomly selected path (variables containing cluster separation were shuffled after simulation). Participants could view the same clip up to four times within the time limit. Radial tours allowed participants to select the manipulation variable. The starting basis was initialized to a half-clock design, where the variables were evenly distributed in half of the circle. This design was created to be variable agnostic while maximizing the independence of the variables. Selecting a new variable resets the animation where the new variable is manipulated to a complete contribution, zeroed contribution, and then back to its initial contribution. Animation and interpolation parameters were held constant across grand and radial tour (five frames per second with a step size of 0.1 radians between interpolated frames). 4.1.5 Data simulation Each dimension is distributed initially as \\(\\mathcal{N}(0, 1)\\), given the covariance set by the shape factor. Clusters were initially separated by a distance of two before location mixing. Signal variables had a correlation of 0.9 when they had equal orientation and -0.9 when their orientations varied. Noise variables were restricted to zero correlation. Each cluster is simulated with 140 observations and is offset in a variable that did not distinguish previous variables. Clusters of the EVV shape are transformed to the banana-chevron shape (illustrated in figure 4.2, shape row). Then location mixing is applied by post-multiplying a rotation matrix to the signal variable and a noise variable for the clusters in question. All variables are then standardized by standard deviations away from the mean. The columns are then shuffled randomly. Each of these replications is then iterated with each level of the visual. For PCA, projections were saved (to png) for each of the 12 pairs of the top four principal components. A grand tour basis path is saved for each dimensionality level. The data from each simulation is then projected through its corresponding bases path and saved to gif file. The radial tour starts at either the four or six-variable half-clock basis. A radial tour is then produced for each variable and saved as a gif. 4.1.6 Randomized assignment Now, with simulation and their artifacts in hand, this section covers how the experimental factors are assigned and demonstrate how this is experienced from the participants perspective. The study is sectioned into three periods. Each period is linked to a randomized level of visual and location. The order of dimension and shape are of secondary interest and are held constant in increasing order of difficulty; four then six dimensions and EEE, EEV, then EVV-banana, respectively. Each period starts with an untimed training task at the simplest remaining experimental levels; location = 0/100%, shape = EEE, and four dimensions with three clusters. This serves to introduce and familiarize participants with input and visual differences. After the training, the participant performs two trials with the same visual and location level across the increasing difficulty of dimension and shape. The plot was removed after 60 seconds, though participants rarely reached this limit. The order of the visual and location levels is randomized with a nested Latin square where all levels of the visuals are exhausted before advancing to the next level of location. This requires \\(3!^2 = 36\\) participants to evaluate all permutations of the experimental factors once. This randomization controls for potential learning effects the participant may receive. Figure 4.4 illustrates how an arbitrary participant experiences the experimental factors. Figure 4.4: Illustration of how a hypothetical participant 63 is assigned experimental factors. Each of the six visual order permutations is exhausted before iterating to the next permutation of location order. Through pilot studies sampled by convenience (information technology and statistics Ph.D. students attending Monash University), it was estimated that three full evaluations are needed to power the study properly, a total of \\(N = 3 \\times 3!^2 = 108\\) participants. 4.1.7 Participants \\(N = 108\\) participants were recruited via prolific.co (Palan and Schitter 2018). Participants are restricted based on their claimed education requiring that they have completed at least an undergraduate degree (some 58,700 of the 150,400 users at the time). This restriction is used on the premise that linear projections and biplot displays will not be regularly used for consumption by general audiences. There is also the implicit filter that Prolific participants must be at least 18 years of age and implicit biases of timezone, location, and language. Participants were compensated for their time at 7.50 per hour, whereas the mean duration of the survey was about 16 minutes. Previous knowledge or familiarity was minimal, as validated in the follow-up survey. The appendix Section A.2.1 contains a heatmap distribution of age and education paneled across preferred pronouns of the participants that completed the survey, who are relatively young, well educated, and slightly more likely to identify as males. 4.1.8 Data collection Data were recorded in shiny application and written to a Google Sheet after each third of the study. Especially at the start of the study, participants experienced adverse network conditions due to the volume of participants hitting the application with modest allocated resources. In addition to this, API read/write limitations further hindered data collection. To mitigate this, the number of participants were throttled and over-collect survey trials until three evaluations were collected for all permutation levels. The processing steps were minimal. The data were formatted and then filtered to the latest three complete studies of each experimental factor, which should have experienced the least adverse network conditions. The bulk of the studies removed were partial data and a few over-sampled permutations. This brings us to the 108 studies described in the paper, from which models and aggregation tables were built. The post-study surveys were similarly decoded to human-readable format and then filtered to include only those 84 associated with the final 108 studies. The code, response files, their analyses, and the study application are publicly available at . 4.2 Results To recap, the primary response variable is accuracy, as defined in Section 4.1.3. The parallel analysis of the log response time is provided in the appendix, Section A.2.2. Two primary data sets were collected; the user study evaluations and the post-study survey. The former is the 108 participants with the experimental factors: visual, location of the cluster separation signal, the shape of variance-covariance matrix, and the dimensionality of the data. Experimental factors and randomization were discussed in section 4.1.2. A followup survey was completed by 84 of these 108 people. It collected demographic information (preferred pronoun, age, and education), and subjective measures for each visual (preference, familiarity, ease of use, and confidence). Below a battery of mixed regression models is built to examine the degree of the evidence and the size of the effects from the experimental factors. Then, Likert plots and rank-sum tests to compare the subjective measures between the visuals. 4.2.1 Accuracy To quantify the contribution of the experimental factors to the accuracy, a mixed-effects models were fit. All models have a random effect term on the participant and the simulation. These terms explain the amount of error that can be attributed to the individual participants effect and variation due to the random sampling data. In building a set of models to test, a base model with only the visual term is compared with the full linear model term and progressively interacting an additional experimental factor. The models with three and four interacting variables are rank deficient; there is not enough varying information in the data to explain all interacting terms. \\[ \\begin{array}{ll} \\textbf{Fixed effects} &amp;\\textbf{Full model} \\\\ \\alpha &amp;\\widehat{Y} = \\mu + \\alpha_i + \\textbf{Z} + \\textbf{W} + \\epsilon \\\\ \\alpha + \\beta + \\gamma + \\delta &amp;\\widehat{Y} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\delta_l + \\textbf{Z} + \\textbf{W} + \\epsilon \\\\ \\alpha \\times \\beta + \\gamma + \\delta &amp;\\widehat{Y} = \\mu + \\alpha_i \\times \\beta_j + \\gamma_k + \\delta_l + \\textbf{Z} + \\textbf{W} + \\epsilon \\\\ \\alpha \\times \\beta \\times \\gamma + \\delta &amp;\\widehat{Y} = \\mu + \\alpha_i \\times \\beta_j \\times \\gamma_k + \\delta_l + \\textbf{Z} + \\textbf{W} + \\epsilon \\\\ \\alpha \\times \\beta \\times \\gamma \\times \\delta &amp;\\widehat{Y} = \\mu + \\alpha_i \\times \\beta_j \\times \\gamma_k \\times \\delta_l + \\textbf{Z} + \\textbf{W} + \\epsilon \\end{array} \\] \\[ \\begin{array}{ll} \\text{where } &amp;\\mu \\text{, the intercept of the model} \\\\ &amp;\\alpha_i \\text{, fixed term for visual}~|~i\\in (\\text{pca, grand, radial}) \\\\ &amp;\\beta_j \\text{, fixed term for location}~|~j\\in (\\text{0/100\\%, 33/66\\%, 50/50\\%}) \\text{ noise/signal mixing} \\\\ &amp;\\gamma_k \\text{, fixed term for shape}~|~k\\in (\\text{EEE, EEV, EVV banana}) \\text{ model shapes} \\\\ &amp;\\delta_l \\text{, fixed term for dimension}~|~l\\in (\\text{4 variables \\&amp; 3 cluster, 6 variables \\&amp; 4 clusters}) \\\\ &amp;\\textbf{Z} \\sim \\mathcal{N}(0,~\\tau), \\text{ the error of the random effect of participant} \\\\ &amp;\\textbf{W} \\sim \\mathcal{N}(0,~\\upsilon), \\text{ the error of the random effect of simulation} \\\\ &amp;\\epsilon \\sim \\mathcal{N}(0,~\\sigma), \\text{ the remaining error in the model} \\\\ \\end{array} \\] Table 4.1: Model performance of random effect models regressing accuracy. Complex models perform better in terms of \\(R^2\\) and RMSE, yet AIC and BIC penalize their large number of fixed effects in favor of the much simpler model containing only the visuals. Conditional \\(R^2\\) includes error explained by the random effects, while marginal does not. Fixed effects No. levels No. terms AIC BIC R2 cond. R2 marg. RMSE a 1 3 -71 -44 0.303 0.018 0.194 a+b+c+d 4 8 -45 4 0.334 0.056 0.194 a*b+c+d 5 12 -26 41 0.338 0.064 0.193 a*b*c+d 8 28 28 167 0.383 0.108 0.19 a*b*c*d 15 54 105 360 0.37 0.222 0.185 Table 4.2: The task accuracy model coefficients for \\(\\widehat{Y} = \\alpha \\times \\beta + \\gamma + \\delta\\), with visual = pca, location = 0/100%, shape = EEE, and dim = 4 held as baselines. Visual being radial is the fixed term with the strongest evidence supporting the hypothesis. Interacting with the location term, there is evidence suggesting radial performs with minimal improvement for 33/66% location mixing. Estimate Std. Error df t value Pr(&gt;|t|) (Intercept) 0.10 0.06 16.1 1.54 0.143 Visual Visualgrand 0.06 0.04 622.1 1.63 0.104 Visualradial 0.14 0.04 617.0 3.77 0.000 *** Fixed effects Location33/66% -0.02 0.07 19.9 -0.29 0.777 Location50/50% -0.04 0.07 20.0 -0.66 0.514 ShapeEEV -0.05 0.06 11.8 -0.82 0.427 Shapebanana -0.09 0.06 11.8 -1.54 0.150 Dim6 -0.01 0.05 11.8 -0.23 0.824 Interactions Visualgrand:Location33/66% -0.02 0.06 588.9 -0.29 0.774 Visualradial:Location33/66% -0.12 0.06 586.5 -2.13 0.033 Visualgrand:Location50/50% -0.03 0.06 591.6 -0.47 0.641 Visualradial:Location50/50% -0.06 0.06 576.3 -1.16 0.248 Table 4.1 compares the model summaries across increasing complexity. The \\(\\alpha \\times \\beta + \\gamma + \\delta\\) model to is selected to examine in more detail as it has relatively high condition \\(R^2\\) and not overly complex interacting terms. Table 4.2 looks at the coefficients for this model. There is strong evidence suggesting a relatively large increase in accuracy from the radial tour, though there is evidence that almost of increase is lost under 33/66% mixing. We also want to visually examine the conditional variables in the model. Figure 4.5 examines violin plots of accuracy by visual with panels distinguishing location (vertical) and shape (horizontal). Figure 4.5: Violin plots of terms of the model \\(\\widehat{Y} = \\alpha \\times \\beta + \\gamma + \\delta\\). Overlaid with global significance from the Kruskal-Wallis test and pairwise significance from the Wilcoxon test, both are non-parametric, ranked-sum tests. Viewing marginal accuracy of the terms corroborate the primary findings that use of the radial tour leads to a significant increase in accuracy, at least over PCA, and this effect is particularly well support when no location mixing is applied. 4.2.2 Subjective measures The 84 evaluations of the post-study survey also collect four subjective measures for each visual. Figure 4.6 shows the Likert plots, or stacked percentage bar plots, alongside violin plots with the same non-parametric, ranked sum tests previously used. Participants preferred to use radial for this task. Participants were also more confident of their answers and found radial tours easier than grand tours. All visuals have reportedly low familiarity, as expected from crowdsourced participants. Figure 4.6: The subjective measures of the 84 responses of the post-study survey, five discrete Likert scale levels of agreement (L) Likert plots (stacked percent bar plots) with (R) violin plots of the same measures. Violin plots are overlaid with global significance from the Kruskal-Wallis test and pairwise significance from the Wilcoxon test. Both are non-parametric, ranked sum tests. Participants are more confident using the radial tour and find it easier to use than the grand tour. The radial tour is the most preferred visual. 4.3 Conclusion Data visualization is an integral part of understanding relationships in data and how models are fitted. However, thorough exploration of data in high dimensions becomes difficult. Previous methods offer no means for an analyst to impact the projection basis. The manual tour provides a mechanism for changing the contribution of a selected variable to the basis. Giving analysts such control should facilitate the exploration of variable-level sensitivity to the identified structure. This paper discussed a with-in participant user study (\\(n=108\\)) comparing the efficacy of three linear projection techniques: PCA, grand tour, and radial tour. The participants performed a supervised cluster task, explicitly identifying which variables contribute to the separation of two target clusters. This was evaluated evenly over four experimental factors. In summary, mixed model regression finds strong evidence that using the radial tour sizably increases accuracy, especially with the location of cluster separation is not mixed at 33/66%. The effect sizes on accuracy are large relative to the change from the other experimental factors and the random effect of data simulation, though smaller than the random effect of the participant. The radial tour was most preferred of the three visuals. There are several ways that this study could be extended. In addition to expanding the support of the experimental factors, more exciting directions include: introducing a new task, visualizations used, and experience level of the target population. It is difficult to achieve good coverage given the number of possible experimental factors. "],["5-ch-cheem.html", "Chapter 5 Exploring Local Explanations of Nonlinear Models Using the Radial tour 5.1 SHAP and tree SHAP local explanations 5.2 The cheem viewer 5.3 Case studies 5.4 Discussion", " Chapter 5 Exploring Local Explanations of Nonlinear Models Using the Radial tour The previous chapter discussed the within-participants user study comparing PCA, the grand tour, and the radial tour in a supervised variable attribution task. There was strong evidence that the radial tour led to a large increase in accuracy. The analyst can be more confident that the radial tour leads to better analysis of variable-level attribution to variables identified in a projection. Given the interpretability crisis of nonlinear models, it would be interesting to see if the radial tour can help. Specifically, this chapter investigates using the radial tour to explore variable sensitivity to the structure identified in linear local explanations of nonlinear models. That is, under what range of variable importance does an explanation make sense, and what contributions fail to support the prediction. This can be used to examine which variables lead to the misclassification of observation or an extreme residual. The radial tour can also test how susceptible a variables contribution is to discerning the predictions of two observations. The increased predictive power of nonlinear models comes at the cost of interpretability of its terms. This trade-off has led to the emergence of eXplainable AI (XAI). XAI attempts to shed light on how models use predictors to arrive at a prediction with local explanations, a point estimate of the linear variable importance in the vicinity of one observation. These can be considered linear projections and can be further explored to understand better the interactions between variables used to make predictions across the predictive model surface. Here we describe interactive linear interpolation used for exploration at any observation and illustrate with examples with categorical (penguin species, chocolate types) and quantitative (soccer/football salaries, house prices) output. The methods are implemented in the R package cheem, available on CRAN. Chapter 2 introduced predictive modeling, the interpretability crisis of nonlinear models, and local explanations  approximations of linear variable importance in the vicinity of one observation. The remainder of this chapter is organized as follows. Section 5.1 induces the SHAP and tree SHAP local explanation. Section 2.3.1 explains the animations of continuous linear projections. Section 5.2 discusses the visual layout in the graphical user interface, how it facilitates analysis, data preprocessing, and package infrastructure. Then Section 5.3 illustrates the application to supervised learning with categorical and quantitative output. Section 5.4 concludes with the insights gained and possible directions to explore in the future. 5.1 SHAP and tree SHAP local explanations SHaply Additive exPlanations (SHAP) quantifies the variable contributions of one observation by examining the effect of other variables on the predictions. The explanations of SHAP refer to Shapley (1953)s method to evaluate an individuals contribution to cooperative games by assessing this players performance in the presence or absence of other players. Strumbelj and Kononenko (2010) introduced SHAP for local explanations in ML models. The attribution of variable importance depends on the sequence of the included variables. The SHAP values are the mean contributions over different variable sequences. The approach is related to partial dependence plots (Molnar 2020), used to explain the effect of a variable by predicting the response for a range of values on this variable after fixing the value of all other variables to their mean. Though partial dependence plots are a global approximation of the variable importance, while SHAP is specific to one observation. It could also be considered similar to examining the coefficients from all subsets regression, as described in Wickham, Cook, and Hofmann (2015), helps to understand the relative importance of each variable in the context of all other candidate variables. Figure 5.1: Illustration of SHAP values for a random forest model FIFA 2020 player wages from nine skill predictors. A star offensive and defensive player are compared, L. Messi and V. van Dijk, respectively. Panel (a) shows breakdown plots of three sequences of the variables. The sequence of the variables impacts the magnitude of their attribution. Panel (b) shows the distribution of attribution for each variable across 25 sequences of predictors, with the mean displayed as a dot for each player. Reaction skills are important for both players. Offense and movement are important for Messi but not van Dijk, and conversely, defense and power are important for van Dijk but not Messi. Following the use case Explanatory Model Analysis (Biecek and Burzykowski 2021), FIFA data is used to illustrate SHAP. Consider soccer data from the FIFA 2020 season (Leone 2020). There are 5000 observations of 9 skill measures (after aggregating highly correlated variables). A random forest model is fit regressing players wages [2020 Euros] from their skill measurements. The SHAP values are compared for a star offensive player (L. Messi) and defensive player (V. van Dijk). The results are displayed in Figure 5.1. A difference in the attribution of the variable importance across the two positions of the players can be expected. This would be interpreted as how a players salary depends on this combination of skill sets. Panel (a) is a modified breakdown plot (Gosiewska and Biecek 2019) where three sequences of variables are presented, so the two observations can be more easily compared. The magnitude of the contributions depends on the sequence in which they appear. Panel (b) shows the differences of the players median values of over 25 such sequences. In summary, these plots highlight how local explanations bring interpretability to a model, at least in the vicinity of their observations. In this observation, two players with different positions receive different profiles of variable importance to explain the prediction of their wages. For the application, we use tree SHAP, a variant of SHAP that enjoys a lower computational complexity (Lundberg, Erion, and Lee 2018). Instead of aggregating over sequences of the variables, tree SHAP calculates observation-level variable importance by exploring the structure of the decision trees. Tree SHAP is only compatible with tree-based models; random forests are used for illustration. The following section will use normalized SHAP values as a projection basis (call this the attribution projection) that will be used to explore the sensitivity of the variable contributions. 5.2 The cheem viewer To explore the local explanations, coordinated views (Roberts 2007) (also known as ensemble graphics, Unwin and Valero-Mora 2018) are provided in the cheem viewer application. There are two primary plots: the global view to give the context of all of the SHAP values and the radial tour view to explore the local explanations with user-controlled rotation. There are numerous user inputs, including variable selection for the radial tour and observation selection for making comparisons. There are different plots used for the categorical and quantitative responses. Figures 5.2 and 5.3 are screenshots showing the cheem viewer for the two primary tasks: classification (categorical response) and regression (quantitative response). 5.2.1 Global view The global view provides a context of all observations and facilitates the exploration of the separability of the data- and attribution-spaces. Both of these spaces are of dimension \\(n\\times p\\), where \\(n\\) is the number of observations and \\(p\\) is the number of predictors. The attribution space corresponds to the local explanations for each observation, each a vector of \\(p\\) values. Visualization is provided by the first two principal components of the data (left) and the attribution (middle) spaces. These single 2D projections will not reveal all of the structure of higher-dimensional space, but they are helpful visual summaries. In addition, a plot of the observed against predicted response values is also provided (Figures 5.2b, 5.3a) to help identify observations poorly predicted by the model. For classification tasks, misclassified observations are circled in red. Linked brushing between the plots is provided, and a tabular display of selected points helps to facilitate exploration of the spaces and the model (shown in Figures 5.3d). While the comparison of these spaces is interesting, the primary purpose of the global view is to enable the selection of observations to explore the local explanations. The projection attribution of the primary observation (PO) is examined and typically viewed with an optional comparison observation (CO). These observations are highlighted as asterisk and \\(\\times\\), respectively. 5.2.2 Radial tour The local explanations for all observations are normalized (squared sum of values adds to 1), and thus, the relative importance of variables can be compared across all observations. These are depicted as vertical parallel coordinate plots (Ocagne 1885) on the basis biplot (Gabriel 1971). 1D biplot display depicts the values of the current basis as bars. The parallel coordinate overlays lines connecting one observations variable attribution (Figures 5.2e and 5.3e). The attribution projections of the PO and CO are shown as dashed and dotted lines. From this plot, the range and density of the importance across all observations can be interpreted. For classification, one would look at differences between groups on any variable. For example, Figure 5.2e suggests that bl is important for distinguishing the green class from the other two. For regression, one might generally observe which variables have low values for all observations (not important), for example, BMI and pwr in Figure 5.3e, and which have a range of high and low values (e.g. off, def) suggesting they are important for some observations and not important for other observations. The overlaid bars on the parallel coordinate plot represent the attribution projection of the PO. (Remember that the PO is interactively selected from the global view). The attribution projection is approximates the variable importance for predicting this observation. This combination of variables best explains the difference between the mean response and an observations predicted value. It is not an indication of the local shape of the model surface. That is, it is not some indication of the tangent to the curve at this point. The attribution projection of the PO is the initial 1D basis in a radial tour, displayed as a density plot for a categorical response (Figure 5.2f) and as scatterplots for a quantitative response (Figure 5.3f). The PO and CO are indicated by vertical dashed and dotted lines, respectively. The radial tour varies the contribution of the selected variable between 0-1. This is viewed as an animation of the projections from many intermediate bases. Doing so tests the sensitivity of structure (class separation or strength of relationship) to the variables contribution. For classification, if the separation between classes diminishes when the variable contribution is reduced, this suggests that the variable is important for class separation. For regression, if the relationship scatterplot weakens when the variable contribution is reduced, indicating that the variable is important for accurately predicting the response. 5.2.3 Classification task Selecting a misclassified observation as PO and a correctly classified point nearby in data space as CO makes it easier to examine the variables most responsible for the error. The global view (Figure 5.2c) displays the model confusion matrix. The radial tour is 1D and displays as density where color indicates class. An animation slider enables the user to vary the contribution of each variable to explore the sensitivity of the separation to that variable. Figure 5.2: Overview of the cheem viewer for classification tasks. Global view inputs, (a), set the PO, CO, and color statistic. Global view, (b) PC1 by PC2 approximations of the data space and attribution space. (c) prediction by observed y (visual of the confusion matrix for classification tasks). Points are colored by predicted class, and red circles indicate misclassified observations. Radial tour inputs (d) select variables to include and which variable is changed in the tour. (e) shows a parallel coordinate display of the distribution of the variable attributions while bars depict contribution for the current basis. The black bar is the variable being changed in the radial tour. Panel (f) is the resulting data projection indicated as density in the classification case. 5.2.4 Regression task Selecting an inaccurately predicted observation as PO and an accurately predicted observation with similar variable values as CO is a helpful way to understand how the model is failing or not. The global view (Figure 5.3a) shows a scatterplot of the observed vs predicted values, which should exhibit a strong relationship if the model is a good fit. The points can be colored by a statistic, residual, a measure of outlyingness (log Mahalanobis distance), or correlation aid in understanding the structure identified in these spaces. In the radial tour view, the observed response and the residuals (vertical) are plotted against the attribution projection of the PO (horizontal). The attribution projection can be interpreted similarly to the predicted value from the global view plot. It represents a linear combination of the variables, and a good fit would be indicated when there is a strong relationship with the observed values. This can be viewed as a local linear approximation if the fitted model is nonlinear. As the contribution of a variable is varied, if the value of the PO does not change much, it would indicate that the prediction for this observation is NOT sensitive to that variable. Conversely, if the predicted value varies substantially, the prediction is very sensitive to that variable, suggesting that the variable is very important for the POs prediction. Figure 5.3: Overview of the cheem viewer for regression and illustration of interactive variables. Panel (a) PCA of the data and attributions spaces, (b) residual plot, predictions by observed values. Four selected points are highlighted in the PC spaces and tabularly displayed. Coloring on a statistic (c) highlights structure organized in the attribution space. Interactive tabular display (d) populates when observations are selected. Contribution of the 1D basis affecting the horizontal position (e) parallel coordinate display of the variable attribution from all observations, and horizontal bars show the contribution to the current basis. Regression projection (f) uses the same horizontal projection and fixes the vertical positions to the observed y and residuals (middle and right). 5.2.5 Interactive variables The application has several reactive inputs that affect the data used, aesthetic display, and tour manipulation. These reactive inputs make the software flexible and extensible (Figure 5.2a &amp; d). The application also has more exploratory interactions to help link points across displays, reveal structures found in different spaces, and access the original data. A tooltip displays observation number/name and classification information while the cursor hovers over a point. Linked brushing allows the selection of points (left click and drag) where those points will be highlighted across plots (Figure 5.2a &amp; b). The information corresponding to the selected points is populated on a dynamic table (Figure 5.2d). These interactions aid exploration of the spaces and, finally, the identification of primary and comparison observations. 5.2.6 Preprocessing It is vital to mitigate the render time of visuals, especially when users may want to iterate many explorations. All computational operations should be prepared before runtime. The work remaining when an application is run solely reacts to inputs and rendering visuals and tables. Below discusses the steps and details of the reprocessing. (ref:citeRf) (Liaw and Wiener 2002) (ref:citeTs) (Kominsarczyk et al. 2021) The time to preprocess the data will vary significantly with the complexity of the model and local explanation. For reference, the FIFA data contained 5000 observations of nine explanatory variables took 2.5 seconds to fit a random forest model of modest hyperparameters. Extracting the tree SHAP values of each observation took 270 seconds total. PCA and statistics of the variables and attributions took 2.8 seconds. These runtimes were from a non-parallelized R session on a modern laptop, but suffice to say that most of the time will be spent on the local attribution. An increase in model complexity or data dimensionality will quickly become an obstacle. Its reduced computational complexity makes tree SHAP an excellent candidate to start. (Alternatively, the package fastshap (Greenwell 2020) claims extremely low runtimes, attributed to fewer calls to the prediction function, partial implementation in C++, and efficient use of logical subsetting.) 5.2.7 Package infrastructure The above-described method and application are implemented as an open-source R package, cheem available on CRAN. Preprocessing was facilitated with models created via randomForest (Liaw and Wiener 2002) and explanations calculated with treeshap (Kominsarczyk et al. 2021). The application was made with shiny (W. Chang et al. 2021). The tour visual is built with spinifex (Spyrison and Cook 2020). Both views are created first with ggplot2 (Wickham 2016) and then rendered as interactive html widgets with plotly (Sievert 2020). DALEX (Biecek 2018) and the free ebook, Explanatory Model Analysis (Biecek and Burzykowski 2021) were a boon to understanding local explanations and how to apply them. 5.2.8 Installation and getting started The package can be installed from GitHub using the following R code: install.packages(&quot;cheem&quot;, dependencies = TRUE) library(&quot;cheem&quot;) run_app() Alternatively, a version of the cheem viewer shiny app can be directly accessed at ebsmonash.shinyapps.io/cheem_initial/. the development version of the package is available at https://github.com/nspyrison/cheem, and documentation of the package can be found at https://nspyrison.github.io/cheem/. Follow the examples provided with the package to compute the local explainers (using ?cheem_ls). The application expects the output returned by cheem_ls(), saved to an .rds file with saveRDS(), to be uploaded. 5.3 Case studies To illustrate the cheem method, it is applied to modern datasets, two classification examples and then two of regression. 5.3.1 Palmer penguin, species classification The Palmer penguins data (Gorman, Williams, and Fraser 2014; Horst, Hill, and Gorman 2020) was collected on three species of penguins foraging near Palmer Station, Antarctica. The data was publicly available to substitute for the overly-used iris data and is quite similar in form. After removing incomplete observations, there are 333 observations of four physical measurements, bill length (bl), bill depth (bd), flipper length (fl), body mass (bm), for this illustration. A random forest model was fit with species as the response variable. (ref:casepenguins-cap) Examining the SHAP values for a random forest model classifying Palmer penguin species. The PO is a Gentoo (purple) penguin that is misclassified as a Chinstrap (orange), marked as an asterisk in (a), and the dashed vertical line in (b). The radial view shows varying the contribution of fl from the initial attribution projection (b, left), which produces a linear combination where the PO is more probably (higher density value) a Chinstrap than a Gentoo (b, right). (The animation of the radial tour is at vimeo.com/666431172.) Figure 5.4: (ref:casepenguins-cap) Figure 5.4 shows plots from the cheem viewer for exploring the random forest model on the penguins data. Panel (a) shows the global view, and panel (b) shows several 1D projections generated with the radial tour. Penguin 243, a Gentoo (purple), is the PO because it has been misclassified as a Chinstrap (orange). (ref:casepenguinsblfl-cap) Checking what is learned from the cheem viewer. This is a plot of flipper length (fl) and bill length (bl), where an asterisk highlights the PO. A Gentoo (purple) misclassified as a Chinstrap (orange). The PO has an unusually small fl length which is why it is confused with a Chinstrap. Figure 5.5: (ref:casepenguinsblfl-cap) There is more separation visible in the attribution space than the data space, as would be expected. The predicted vs observed plot reveals a handful of misclassified observations. A Gentoo that has been wrongly labeled as a Chinstrap is selected for illustration. The PO is a misclassified point (represented by the asterisk in the global view and a dashed vertical line in the tour view). The CO is a correctly classified point (represented by an \\(\\times\\) and a vertical dotted line). The radial tour starts from the attribution projection of the misclassified observation (b, left). The important variables identified by SHAP in the (wrong) prediction for this observation are mostly bl and bd with small contributions of fl and bm. This projection is a view where the Gentoo (purple) looks much more likely for this observation than Chinstrap. That is, this combination of variables is not particularly useful because the PO looks very much like other Gentoo penguins. To explore this, the radial tour is used to vary the contribution of flipper length (fl). (In our exploration, this was the third variable explored. It is typically helpful to explore the variables with more significant contributions, here bl and bd. Still, when doing this, nothing was revealed about how the PO differed from other Gentoos). On varying fl as it contributes increasingly to the projection (b, right), more and more, this penguin looks like a Chinstrap. This suggests that fl should be considered an important variable for explaining the (wrong) prediction. Figure 5.5 confirms that flipper length (fl) is vital for the confusion of the PO as a Chinstrap. Here, flipper length and body length are plotted, and the PO can be seen to be closer to the Chinstrap group in these two variables, mainly because it has an unusually low value of flipper length relative to other Gentoos. From this view, it makes sense that it is a hard observation to account for as decision trees can only partition only vertical and horizontal lines. 5.3.2 Chocolates, milk/dark chocolate classification The chocolates dataset consists of 88 observations of ten nutritional measurements determined from their labels and labeled as either milk or dark. Dark chocolate is considered healthier than milk. The data were collected by students during the Iowa State University class STAT503 from nutritional information from the manufacturers website and normalized to 100g equivalents. The data is available in the cheem package. A random forest model is used for the classification of chocolate types. It could be interesting to examine the nutritional properties of any dark chocolates that have been misclassified as milk. A reason to do this is that a dark chocolate, nutritionally more like milk, should not be considered a healthy alternative. It is interesting to explore which nutritional variables contribute most to misclassification. (ref:casechocolates-cap) Examining the local explanation for a PO which is dark (orange) chocolate incorrectly predicted to be milk (green). From the attribution projection, this chocolate correctly looks more like dark than milk, which suggests that the local explanation does not help understand the prediction for this observation. So, the contribution of Sugar is varied  reducing it corresponds primarily with increasing Fiber. When Sugar is zero, Fiber contributes strongly towards the left. In this particular view, the PO is closer to the bulk of the milk chocolates, suggesting that the prediction put a lot of importance on Fiber. This chocolate is a rare dark chocolate without any Fiber leading to it being mistaken for a milk chocolate. (A video of the tour animation can be found at vimeo.com/666431143.) Figure 5.6: (ref:casechocolates-cap) This type of exploration is shown in Figure 5.6, where a chocolate labeled dark but predicted to be milk is chosen as the PO (observation 22). It is compared with a CO that is a correctly classified dark chocolate (observation 7). The PCA plot and the tree SHAP PCA plots (a) show a big difference between the two chocolate types but with confusion for a handful of observations. The misclassifications are more apparent in the observed vs predicted plot and can be seen to be mistaken in both ways: milk to dark and dark to milk. The attribution projection for chocolate 22 suggests that Fiber, Sugars, and Calories are most responsible for its incorrect prediction. The way to read this plot is to see that Fiber has a large negative value while Sugars and Calories have reasonably large positive values. In the density plot, observations on the very left of the display would have high values of Fiber (matching the negative projection coefficient) and low values of Sugars and Calories. The opposite would be interpreting a point with high values in this plot. The dark chocolates (orange) are primarily on the left, and this is a reason why they are considered to be healthier: high fiber and low sugar. The density of milk chocolates is further to the right, indicating that they generally have low fiber and high sugar. The PO (dashed line) can be viewed against the CO (dotted line). Now, one needs to pay attention to the parallel plot of the SHAP values, which are local to a particular observation, and the density plot, which is the same projection of all observations as specified by the SHAP values of the PO. The variable contributions to the two different predictions can be quickly compared in the parallel coordinate plot. The PO differs with the comparison primarily on the Fiber variable, which suggests that this is the reason for the incorrect prediction. From the density plot, which is the attribution projection corresponding to the PO, both observations are more like dark chocolates. Varying the contribution of Sugars and altogether removing it from the projection is where the difference becomes apparent. When primarily Fiber is examined, observation 22 looks more like a milk chocolate. It would also be interesting to explore an inverse misclassification. In this case, a milk chocolate is selected while it was misclassified as a dark chocolate. Chocolate 84 is just this case and is compared with a correctly predicted milk chocolate (observation 71). The corresponding global view and radial tour frames are shown in Figure 5.7. (ref:casechocolatesinverse-cap) Examining the local interpretation for a PO which is milk (green) chocolate incorrectly predicted to be dark (orange). In the attribution projection, the PO could be either milk or dark. Sodium and Fiber have the largest differences in attributed variable importance, with low values relative to other milk chocolates. The lack of importance attributed to these variables is suspected of contributing to the mistake, so the contribution of Sodium is varied. If Sodium had a larger contribution to the prediction (like in this view). the PO would look more like other milk chocolates. (A video of the tour animation can be found at vimeo.com/666431148.) Figure 5.7: (ref:casechocolatesinverse-cap) The difference of position in the tree SHAP PCA with the previous case is quite significant; this gives a higher-level sense that the attributions should be quite different. Looking at the attribution projection, this is found to be the case. Previously, Fiber was essential while it is absent from the attribution in this case. Conversely, Calories from Fat and Total Fat have high attributions here, while they were unimportant in the preceding case. Comparing the attribution with the CO (dotted line), large discrepancies in Sodium and Fiber are identified. The contribution of Sodium is selected to be varied. Even in the initial projection, the observation looks slightly more like its observed milk than predicted dark chocolate. The misclassification appears least supported when the basis reaches sodium attribution of typical dark chocolate. 5.3.3 FIFA, wage regression The 2020 season FIFA data (Leone 2020; Biecek 2018) contains many skill measurements of soccer/football players and wage information. Nine higher-level skill groupings were identified and aggregated from highly correlated variables. A random forest model is fit from these predictors, regressing player wages [2020 euros]. The model was fit from 5000 observations before being thinned to 500 players to mitigate occlusion and render time. Continuing from the exploration in section 2.6, we are interested to see the difference in attribution based on the exogenous player position. That is, the model should be able to use multiple linear profiles to better predict the wages from different field positions of players despite not having this information. A leading offensive fielder (L. Messi) is compared with a top defensive fielder (V. van Dijk). The same observations were used in figure 5.1. (ref:casefifa-cap) Exploring the wages (euros) relative to skill measurements in the FIFA 2020 data. Star offensive player (L. Messi) is the PO and he is compared with a top defensive player (V. van Dijk). The attribution projection is shown at left, and it can be seen that this combination of variables produces a view where Messi has very high predicted (and observed) wages. Defense (def) is the chosen variable to vary. It starts off very low, and as its contribution is increased (right plot) it can be seen that Messis predicted wages decrease dramatically. The increased contribution in defense comes at the expense of offensive and reaction skills. The interpretation is that indeed, Messis high wages are most attributable to his offensive and reaction skills, as initially provided by the local explanation. (A video of the animated radial tour can be found at vimeo.com/666431163.) Figure 5.8: (ref:casefifa-cap) Figure 5.8, tests the support of the local explanation. Offensive and reaction skills (off and rct) are both crucial to explaining a star offensive player. If either of them were rotated out, the other would be rotated into the frame, maintaining a far-right position. However, Increasing the contribution of a variable with low importance would rotate both variables out of the frame. The contribution from def will be varied to contrast with offensive skills. As the contribution of defensive skills increases, Messis is no longer separated from the group. Players with high values in defensive skills are now the rightmost points. In terms of what-if analysis, the difference between the data mean and his predicted wages would be halved if Messis tree SHAP attributions at these levels. 5.3.4 Ames housing 2018, sales price regression Ames 2018, housing data was subset to North Ames (the neighborhood with the most house sales). The remaining are 338 house sales. A random forest model has fitted this price [USD] with the property variables: Lot Area (LtA), Overall Quality (Qlt), Year the house was Built (YrB), Living Area (LvA), number of Bathrooms (Bth), number of Bedrooms (Bdr), the total number of Rooms (Rms), Year the Garage was Built (GYB), and Garage Area (GrA). Using interaction from the global view, a house with an extreme negative residual and an accurate instance with a similar prediction are selected. (ref:caseames-cap) Exploring an observation with a large residual as the PO from fitting sales price [USD] to other variables in the Ames housing 2018. The sale price of the PO was under-predicted. The local explanation indicates a sizable attribution to Lot Area (LtA). The CO has a similar predicted sales price and smaller residual and has minimal attribution to Lot Area. In the attribution projection, the PO has a higher sales price than the CO. Reducing the contribution of Lot Area brings these two prices in line. This suggests if the model did not consider Lot Area, then the two houses would be quite similar. That is, the large residual is due to a lack of factoring in the Lot Area for the prediction of POs sales price. (A video showing the animation is at vimeo.com/666431134.) Figure 5.9: (ref:caseames-cap) Figure 5.9 selects the house sale 74, a sizable under prediction with an enormous Lot Area contribution. The CO has a similar predicted price though the prediction was accurate and gives almost no attribution to lot size. The attribution projection places observations with high Living Areas to the right. The contribution of Living Area contrasts the contribution of this variable. As the contribution of Lot Area decreases, the predictive power decreases for the PO, while the CO remains stationary. This large of importance in the Living Area is relatively uncommon. Boosting tree models may be more resilient to such an under-prediction as they would up-weighting this residual and force its inclusion in the final model. 5.4 Discussion There is a clear need to extend the interpretability of black-box models. This chapter provides a technique that builds on local interpretations to explore the variable importance local to an observation. The local interpretations form an attribution projection from which variable contributions are varied using a radial tour. Several diagnostic plots are provided to assist with understanding the sensitivity of the prediction to particular variables. A global view shows the data space, explanation space, residual plot. The user can interactively select observations to compare, contrast, and study further. Then the radial tour is used to explore the variable sensitivity identified by the attribution projection. This approach has been illustrated using four data examples of random forest models with the tree SHAP local explanation. In the penguins example, we showed how the misclassification of a penguin arose due to it having an unusually small flipper size compared to others of its species. This was verified by making a follow-up plot of the data. The chocolates example shows how a dark chocolate was misclassified primarily due to its attribution to Fiber, and a milk chocolate was misclassified as dark due to its lowish Sodium value. In the FIFA example, we show how low Messis salary would be if it depended on defensive skills. In the Ames housing data, an inaccurate prediction for a house was likely due to the Lot Area not being effectively used by the random forest model. This analysis is manually intensive and thus only feasible for investigating a few observations. The approach selects observations that the model has not done well to predict and compare it with an observation where it did fit well. The radial tour launches from the attribution projection to enable exploration of the sensitivity of the prediction to any variable. It can be helpful to make additional plots of the variables and responses to cross-check interpretations made from the cheem viewer. This methodology provides an additional tool in the box for studying model fitting. An implementation is provided in the open-source R package cheem, available on CRAN. Example data sets are provided, and you can upload your data after model fitting and computing the local explanations. In theory, this approach would work with any black-box model, but the implementation currently only calculates tree SHAP for tree-based models supported by treeshap (tree based models from gbm, lightgbm, randomForest, ranger, or xgboost Greenwell et al. 2020; Shi et al. 2022; Liaw and Wiener 2002; Wright and Ziegler 2017; Chen et al. 2021, respectively). Tree SHAP was selected because of its computational efficiency. The SHAP and oscillation explanations could be added with the use of the DALEX::explain() and would be an excellent direction to extend the work (Biecek 2018; Biecek and Burzykowski 2021). "],["6-ch-conclusion.html", "Chapter 6 Conclusion 6.1 Contributions 6.2 Limitations 6.3 Future work", " Chapter 6 Conclusion Most data are multivariate. This thesis makes important contributions to the interactive visualization of quantitative data. Data visualization conveys more information than numerical summarization alone. Yet visualization of multivariate quickly becomes challenging to view comprehensively. After considering common visualizations, we continue with linear dimension reduction. The methods used are built on dynamic animation of linear projections, called tours. Specifically, the radial tour is central to this work. The Introduction introduced the over-arching question: Can the radial tour, with user-steering of the basis, help analysts understand the variable sensitivity of structure in the projection? This was broken into three research questions that were correspondingly addressed in the three content chapters. Chapter 3 developed a package addressing RQ 1 (How do we define and implement a user interface and interactions for the radial tours to add and remove variables smoothly from 1- and 2D linear data projections?). Chapter 4 discussed a user study to answer RQ 2; does the use of the interactive radial tour improve analysts understanding of the relationship between variables and structure in 2D linear projections compared to existing approaches? Chapter 5 covers the novel analysis and corresponding package responding to RQ 3, can the radial tours be used in conjunction with local explanations to improve the interpretability of black-box models? 6.1 Contributions The contributions of this thesis can be split into scientific knowledge and software development. 6.1.1 Scientific knowledge Chapter 3 clarifies the radial tour methodology, specifically using Rodrigues rotation formula (Rodrigues 1840) to solve the rotation matrix defined for 1- and 2D tours. This use of the rotation formula sets up a scaffolding to extend the manual tour to three dimensions with another rotation angle to span the manipulation space. This work also supports radial tours by illustrating use cases on high-energy physics data sets. Chapter 4 discusses a user study to evaluate the efficacy of the radial tour as compared with PCA and the grand tour. It defines a task and accuracy measure for a variable attribution of the separation of two clusters. The \\(n=108\\) crowdsourced user study compares the performance of these visuals across three experimental factors of simulated data. Mixed model regression finds considerable evidence for a sizable improvement in accuracy from the radial tour, which participants also subjectively prefer. Chapter 5 introduces cheem analysis. This extends the interpretability of nonlinear models by exploring local explanations with the radial tour. The global view gives a full observation summary of data space, attribution space, and residual plot side-by-side as a coordinated view. From this, an analyst identifies a primary observation to explore its explanation in detail and optionally compares it against another observation. The primary observations normalized attribution becomes the starting basis for a radial tour. By varying the contributions of variables, an analyst tests the contributions sensitivity to the predictive power identified in the explanation. We provide usage and discussion from four contemporary datasets. 6.1.2 Software The R package spinifex facilitates the creation of manual tours, which allow an analyst to steer the contributions of a variable. It creates a framework for the layered display of tours interoperable with the tourr package. This extension of the geometric display of tours will feel at home to ggplot2 users. After composition, tours can be animated and exported as interactive or fixed animations. Vignettes and an interactive application help users rapidly understand the concepts facilitated by this work. The impact of spinifex can be seen in two ways. My contributions to spinifex and tourr won the ACEMS Impact and Engagement Award, 2018. Furthermore, the package is available on CRAN with vignettes and version notes on its pkgdown site. It has been downloaded over 15,800 times from CRAN between 09 April 2019 and 27 February 2022. The cheem package facilitates the tree SHAP local explanations from tree-based models. Given a compatible model, functions perform the preprocessing to calculate the local explanations of all observations with several statistics that help describe the separability of data and attribution space. This processed object is then used to produce two novel visuals. The global view allows analysts to identify an observation to look at in more detail interactively. A custom display of the radial tour then helps evaluate the sensitivity of the variable contributions to the structure identified in the explanation. An interactive graphical user interface uses these visuals to facilitate the outlined cheem analysis. An interactive graphical user interface facilities this analysis. Several preprocessed datasets are included and allow analysts to upload their data after processing. This package was recently published to CRAN and has a corresponding pkgdown site. 6.2 Limitations Below, we discuss several limitations with this work in chapter order. The following section echos this order and discusses possible directions to address these limitations. Manual tours (and radial tours) are limited in several ways. For instance, the work currently lists rotation matrices for 1- and 2D manual tours. Another limitation is that they only change one variable at a time. This can be cumbersome and timely if many variables need to be zeroed or otherwise modified. The radial tour moves in three segments (total contribution, no contribution, initial contribution). It may be more approachable to directly relate the fraction of the slider to the magnitude of the variables contribution. Setting aside the manual tour, there are several extensions to the layered composition of tours in spinifex, such as extending the geometric display type. Because the manual tour is currently defined for \\(d \\in [1,2]\\) projections, the geometric display for \\(d&gt;2\\) tourr-made tours are under-supported. Most tour implementations are made with 2D monitors in mind. It would be nice to have implementations for extended reality with a stereoscopic head-mounted display. The user study evaluating the radial tour has several intrinsic limitations from the choice of visuals and levels of the experimental factors. It only considered discrete 2D PCA, grand tour, and manual tour for supervised cluster separation on mostly linear clusters in four and six dimensions. Trials we collected online due to the ongoing COVID-19 pandemic. The cheem analysis was illustrated using the tree SHAP explanations on random forest models. The cheem package handles several other tree-based models, though the analysis should be generalized to a broader base of models and compatible explanations. We also focus on quantitative matrices. This could be generalized to accommodate text, image, or time-series data. There is also no comparison evaluating the value of cheem analysis. It would be interesting to measure the benefit of cheem over local explanations or other observation-level evaluations of a model. 6.3 Future work This section purposes possible directions to address or extend from the limitations outlined above. Chapter 3 included the scaffolding to extend manual tours beyond 2D. Namely, this would require the use Rodrigues rotation formula (Rodrigues 1840) to define another rotation angle. The 3D projection would initialize a 4D manipulation space and use three input angles to control the contribution in the first three components. Another display dimension may benefit the detection and understanding of the higher dimensional structure, while we would also expect longer interaction time and less intuitive input. In addition to the manual tour controlling the contribution of a single variable, it may be insightful to change the contributions of several variables at once (effectively manipulating a linear combination of variables). Varying combinations of variables may increase manipulation speed or prove to be unintuitive to input. Alternatively, an automated approach may help clean up variables with small contributions. A sort of dimension reduction tour, could append several manual tours that sequentially zero the contributions of variables contributing less than some threshold, which may prove to expedite analysis, especially for approaching a features intrinsic dimensionality. This approach may be better as an initialization step before render time. This would abstract away some of the business and monotony of dealing with many variables allowing an analyst to focus on the variables that tangibly impact the projection. Currently, the radial tour creates three segments (increase-decrease-increase of magnitude). An ink tank display for a manual tour may be more intuitive than the current approach. For this, the extent of contribution would match the progress of an animation slider. The first frame would contain zero variable contribution, and the last would have a total contribution. The starting frame could be the original contribution that would also be exaggerated or annotated to refer. This last point is adjacent to the pedagogical side of tours and linear projections more broadly. The user-steering of the manual tour provides a way to play with projections and test hypotheses making it a good candidate as a learning tool. An interactive application built for teaching and exploring projection techniques would be a boon to scholars. Extending the output dimensions of some tours is relatively easy to do. The display in spinifex has focused on \\(d \\in [1, 2]\\). Additional functions could be added to facilitate the geometric display of parallel coordinates, Chernoff faces, glyph-based, or pixel-based visuals. For the reasons mentioned in Chapter 2, these displays are potentially best used with data with relatively few observations and many variables. There are also several extensions to the display of 1 &amp; 2D tours, including tabular displaying the numeric values of the basis, drawing of lines around convex and alpha hulls, and a high-density region displays with progressive density rugs, or a combination of point and density contour display where the bulk of the data is shown as density contour, while the outermost observations are displayed as points (Hyndman 1996; OHara-Wild et al. 2022). It may be interesting to experience tours as 3D scatterplots in extended reality with stereoscopically accurate head-tracking may be fruitful. Nelson, Cook, and Cruz-Neira (1998) explore 2D tours in virtual reality. Other works view 3D scatterplot tours on 2D displays (Yang 1999, 2000). It would be interesting to see modern implementations using WebGL, Mozilla A-frame, or Unity. One concern would be keeping hardware and software as generalized as possible. There are some ways that the user-study evaluating the radial tour could be extended. The data could be changed to involve nonlinear shapes, outliers, or varying density and dimensionality. Besides changing the support of the experimental factors to the user study, it may be more interesting to evaluate across different tasks or compare other visualization techniques  scatterplot matrices, parallel coordinate plots, or ridge plots of principal component approximations may be realistic. The performed user study crowdsourced participants with little exposure to linear projections. It would be interesting to compare the results from more experienced participants. The outlined cheem analysis can be generally applied models and local explanations. While the package cheem currently calculates tree SHAP for tree-based models supported by treeshap (Kominsarczyk et al. 2021). This could be generalized more broadly to other models and local explanations. Those facilitated by DALEX::explain() seem to be an excellent direction to extend (Biecek 2018; Biecek and Burzykowski 2021). However, processing runtime is a looming obstacle that is exasperated when moving away from computationally efficient explanations. Alternatively, other statistics may better show the structure identified in the attribution space from the explanations. In the cheem analysis, we had focused primarily on continuous numeric predictors. Perhaps this analysis could be extended to image, text, or time series analysis. The global view may remain helpful in summary and identification while observation-level exploration would likely need to change to fit the context of the data, be it saliency map, word-level contextual sentiment analysis, or other. A user study would help elucidate the benefit of cheem analysis over local explanations or observation-specific analysis of a model. Our analysis can highlight the unique attribution of a selected observation against peers and test the sensitivity of that attribution. Perhaps a task identifying variable sensitivity to a prediction may be appropriate. "],["bibliography.html", "Bibliography", " Bibliography "],["A-supplementary-material-for-radial-tour-user-study.html", "A Supplementary material for radial tour user study A.1 Accompanying radial tour application A.2 Extended analysis", " A Supplementary material for radial tour user study A.1 Accompanying radial tour application An accompanying application illustrates the radial tour. The R package, spinifex, (Spyrison and Cook 2020) is an open-source and now contains a shiny (W. Chang et al. 2021) application allowing users to apply various preprocessing tasks and interactively explore their data via interactive radial tour. Example datasets are provided with the ability to upload data. The html widget produced is a more interactive variant relative to the one used in the user study. Screen captures and more details are provided in the appendix. Run the following R code will run the application locally. install.packages(&quot;spinifex&quot;, dependencies = TRUE) spinifex::run_app() Figure A.1: Process data tab, interactively loads or select data, check which variables to project, and optionally scale columns by standard deviation. In the initial tab, Figure A.1, users upload their own (.csv, .rds, or .rda) data or select from predefined data sets. The numeric columns appear as a list of variables to include in the projection. Below that, a line displays whether or not missing rows were removed. Scaling by standard deviation is included by default, as this is a common transformation used to explore linear projections of spaces. Summaries of the raw data and processed numeric data are displayed to illustrate how the data was read and its transformation. Figure A.2: Radial tour tab, interactively create radial tours, changing the manipulation variable, color, or shape of the resulting manual tour. Here, the palmer penguins data is being explored, bill length was selected to manipulate as it is the only variable separating the green cluster from the orange. By mapping shape to island of observation, the green species can be noted to live on all three islands, while the other species live on only one island. The second tab, Figure A.2 contains interaction for selecting the manipulation variable, and non-numeric columns can be used to change the color and shape of the data points in the projection. The radial tour is created in real-time, animated as an interactive plotly html widget. The application offers users a fast, intuitive introduction elucidating what the radial tour does and some of the features offered. A.2 Extended analysis This section covers extended analysis. First, the demographics of the participants is covered. Then a parallel modeling analysis on log response time is conducted. Lastly, the effect ranges and marginal effects of the random effect of the participants and data simulation are examined. A.2.1 Survey participant demographics The target population is relatively well-educated people, as linear projections may prove difficult for generalized consumption. Hence Prolific.co participants are restricted to those with an undergraduate degree (58,700 of the 150,400 users at the study time). From this cohort, 108 performed a complete study. Of these participants, 84 submitted the post-study survey, represented in the following heatmap. All participants were compensated for their time at 7.50 per hour, with a mean time of about 16 minutes. Figure A.3 shows a heat map of the demographics for these 84 participants. Figure A.3: Heatmaps of survey participant demographics; counts of age group by completed education as faceted across preferred pronoun. Our sample tended to be between 18 and 35 years of age with an undergraduate or graduate degree. A.2.2 Response time As a secondary explanatory variable response time is considered. Response time is first log transformed to remove its right skew. The same modeling procedure is repeated for this response. 1) Compare the performance of a battery of all additive and multiplicative models. Table A.1 shows the higher level performance of these models over increasing model complexity. 2) Select the model with the same effect terms, \\(\\alpha \\times \\beta + \\gamma + \\delta\\), with relatively high conditional \\(R^2\\) without becoming overly complex from interaction. The coefficients of this model are displayed in Table A.2. Table A.1: Model performance regressing on log response time [seconds], \\(\\widehat{Y_2}\\) random effect models. Conditional \\(R^2\\) includes the random effects, while marginal does not. The model \\(\\alpha \\times \\beta + \\gamma + \\delta\\) model is selected to examine further as it has relatively high marginal \\(R^2\\) while having much less complexity than the complete interaction model. Fixed effects No. levels No. terms AIC BIC R2 cond. R2 marg. RMSE a 1 3 &lt;span style=\" font-weight: bold; \" &gt;1448&lt;/span&gt; &lt;span style=\" font-weight: bold; \" &gt;1475&lt;/span&gt; 0.645 0.007 0.553 a+b+c+d 4 8 1467 1516 0.647 0.017 0.552 a*b+c+d 5 12 1474 1541 0.656 0.024 0.548 a*b*c+d 8 28 1488 1627 0.673 0.054 0.536 a*b*c*d 15 54 1537 1792 &lt;span style=\" font-weight: bold; \" &gt;0.7&lt;/span&gt; &lt;span style=\" font-weight: bold; \" &gt;0.062&lt;/span&gt; &lt;span style=\" font-weight: bold; \" &gt;0.523&lt;/span&gt; Table A.2: Model coefficients for log response time [seconds] \\(\\widehat{Y_2} = \\alpha \\times \\beta + \\gamma + \\delta\\), with factor = pca, location = 0/100%, shape = EEE, and dim = 4 held as baselines. Location = 50/50% is the fixed term with the strongest evidence and takes less time. In contrast, the interaction term location = 50/50%:shape = EEV has the most evidence and takes much longer on average. Estimate Std. Error df t value Pr(&gt;|t|) (Intercept) 2.71 0.14 42.6 19.06 0.000 *** Visual Visualgrand -0.23 0.12 567.6 -1.97 0.049 Visualradial 0.16 0.12 573.5 1.34 0.181 Fixed effects Location33/66% 0.05 0.14 40.9 0.34 0.737 Location50/50% -0.05 0.14 42.1 -0.35 0.729 ShapeEEV -0.15 0.09 8.3 -1.61 0.145 Shapebanana -0.13 0.09 8.3 -1.42 0.192 Dim6 0.14 0.08 8.3 1.90 0.093 Interactions Visualgrand:Location33/66% 0.24 0.18 580.9 1.34 0.181 Visualradial:Location33/66% -0.24 0.18 582.4 -1.32 0.188 Visualgrand:Location50/50% 0.12 0.18 578.6 0.69 0.491 Visualradial:Location50/50% 0.05 0.18 584.4 0.25 0.800 Random effect ranges The random effect terms further clarify the source of the error. Below is a comparison the effect ranges attributed to the participant and to the simulations next to their marginal effect on the response, a sort of upper bound of the error they could explain. This was performed for the models regressing accuracy and then log response time. The residual plots have no noticeable nonlinear trends and contain striped patterns as an artifact from regressing on discrete variables. Figure A.4 illustrates (T) the effect size of the random terms participant and simulation, or more accurately, the 95% CI from Gelman simulation of their posterior distribution. The effect size of the participant is much larger than simulation. The most extreme participants are statistically significant at \\(\\alpha = .95\\), while none of the simulation effects significantly deviate from the null of having no effect size on the marks. In comparison, (B) 95% confidence intervals participation and simulation mean accuracy, respectively. Residual plots have no noticeable nonlinear trends and contain striped patterns as an artifact from regressing on discrete variables. Figure A.4 illustrates (T) the effect size of the random terms participant and simulation, or more accurately, the 95% CI from Gelman simulation of their posterior distribution. The effect size of the participant is much larger than simulation. The most extreme participants are statistically significant at \\(\\alpha = .95\\), while none of the simulation effects significantly deviate from the null of having no effect size on the marks. In comparison, (B) 95% confidence intervals participation and simulation mean accuracy, respectively. Figure A.4: Accuracy model: (T) Estimated effect ranges of the random effect terms participant and data simulation of the accuracy model, \\(\\widehat{Y_1} = \\alpha \\times \\beta + \\gamma + \\delta\\). Confidence intervals are created with Gelman simulation on the effect posterior distributions. The effect size of the participant is relatively large, with several significant extrema. None of the simulations deviate significantly. (B) The ordered distributions of the CI of mean marks follow the same general pattern and give the additional context of how much variation is in the data, an upper limit to the effect range. The effect ranges capture about two-thirds of the range of the data without the model. All intervals for \\(\\alpha = .95\\) confidence. Similarly, figure A.5 shows the Gelman simulations and marginal effects of the simulation and participants for the model with the same terms regressing on log response time. Figure A.5: Log response time model: (T) The effect ranges of Gelman resimulation on posterior distributions for the time model, \\(\\widehat{Y_2} = \\alpha \\times \\beta + \\gamma + \\delta\\). These show the magnitude and distributions of particular participants and simulations. Simulation has a relatively small effect on response time. (B) Confidence intervals for mean log time by participant and simulation. The marginal density shows that the response times are left-skewed after log transformation. Interpreting back to linear time there is quite the spread of response times: \\(e^{1} = 2.7\\), \\(e^{2.75} = 15.6\\), \\(e^{3.75} = 42.5\\) seconds. Of the simulations on the right, the bottom has a large variation in response time, relative to the effect ranges which means that the variation is explained in the terms of the model and not by the simulation itself. "],["B-overall-appendix.html", "B Overall appendix B.1 Glossary B.2 Animated tour links B.3 Supplementary material", " B Overall appendix B.1 Glossary This thesis was written with statistics terminology. This glossary helps bridge the language used here to be more accessible to machine learning audiences. Table B.1: Glossary of terms Term Alias observation instance, item, case, row (of data) variable feature, column (of data) explanatory variables independant/input variable, predictors, covariates response variable predicted/dependant/target/output variable B.2 Animated tour links Table B.2: Animated tour links Reference Description Link Figure 1.2 radial tour, penguins https://vimeo.com/676723431 Figure 2.4 grand tour, penguins https://vimeo.com/676723441 Figure 5.4 cheem, penguins https://vimeo.com/666431172 Figure 5.6 cheem, chocolates https://vimeo.com/666431143 Figure 5.7 cheem, chocolates (inverse) https://vimeo.com/666431148 Figure 5.8 cheem, fifa https://vimeo.com/666431163 Figure 5.9 cheem, ames housing 2018 https://vimeo.com/666431134 B.3 Supplementary material Table B.3: Supplementary material Description Link spinifex vignette, Getting started with spinifex https://shorturl.at/dgBOT spinifex vignette, Ggproto api https://shorturl.at/mwBP6 cheem vignette, Getting started with cheem https://shorturl.at/kqsF0 thesis repository https://shorturl.at/clqvB thesis, pdf format https://shorturl.at/uBYZ5 thesis, html format https://shorturl.at/dftCN "]]
