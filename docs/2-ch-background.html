<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Background | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models</title>
  <meta name="description" content="Chapter 2 Background | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Background | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Background | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models" />
  
  
  

<meta name="author" content="Nicholas S Spyrison" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="1-ch-introduction.html"/>
<link rel="next" href="3-ch-spinifex.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="template/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./"> </a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i>Abstract</a>
<ul>
<li class="chapter" data-level="" data-path="abstract.html"><a href="abstract.html#publications-and-papers-during-candidature-not-part-of-this-thesis"><i class="fa fa-check"></i>Publications and papers during candidature not part of this thesis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#research-questions"><i class="fa fa-check"></i><b>1.1</b> Research questions</a></li>
<li class="chapter" data-level="1.2" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#methodology"><i class="fa fa-check"></i><b>1.2</b> Methodology</a></li>
<li class="chapter" data-level="1.3" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#contributions"><i class="fa fa-check"></i><b>1.3</b> Contributions</a></li>
<li class="chapter" data-level="1.4" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#thesis-structure"><i class="fa fa-check"></i><b>1.4</b> Thesis structure</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-background.html"><a href="2-ch-background.html"><i class="fa fa-check"></i><b>2</b> Background</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2-ch-background.html"><a href="2-ch-background.html#scatterplot-matrices"><i class="fa fa-check"></i><b>2.1</b> Scatterplot matrices</a></li>
<li class="chapter" data-level="2.2" data-path="2-ch-background.html"><a href="2-ch-background.html#parallel-coordinate-plots"><i class="fa fa-check"></i><b>2.2</b> Parallel coordinate plots</a></li>
<li class="chapter" data-level="2.3" data-path="2-ch-background.html"><a href="2-ch-background.html#dimension-reduction"><i class="fa fa-check"></i><b>2.3</b> Dimension reduction</a></li>
<li class="chapter" data-level="2.4" data-path="2-ch-background.html"><a href="2-ch-background.html#sec:tour"><i class="fa fa-check"></i><b>2.4</b> Tours, animated linear projections</a></li>
<li class="chapter" data-level="2.5" data-path="2-ch-background.html"><a href="2-ch-background.html#evaluating-multivariate-data-visualization"><i class="fa fa-check"></i><b>2.5</b> Evaluating multivariate data visualization</a></li>
<li class="chapter" data-level="2.6" data-path="2-ch-background.html"><a href="2-ch-background.html#nonlinear-models"><i class="fa fa-check"></i><b>2.6</b> Nonlinear models</a></li>
<li class="chapter" data-level="2.7" data-path="2-ch-background.html"><a href="2-ch-background.html#sec:explanations"><i class="fa fa-check"></i><b>2.7</b> Local explanations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html"><i class="fa fa-check"></i><b>3</b> spinifex: an R package for creating user-controlled animated linear projections</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:algorithm"><i class="fa fa-check"></i><b>3.1</b> Algorithm</a></li>
<li class="chapter" data-level="3.2" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#oblique-cursor-movement"><i class="fa fa-check"></i><b>3.2</b> Oblique cursor movement</a></li>
<li class="chapter" data-level="3.3" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:pkgstructure"><i class="fa fa-check"></i><b>3.3</b> Package structure</a></li>
<li class="chapter" data-level="3.4" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:usecases"><i class="fa fa-check"></i><b>3.4</b> Use cases</a></li>
<li class="chapter" data-level="3.5" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:discussion"><i class="fa fa-check"></i><b>3.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html"><i class="fa fa-check"></i><b>4</b> A Study on the Benefit of a User-Controlled Radial Tour for Variable Importance for Structure in High-Dimensional Data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:userstudy"><i class="fa fa-check"></i><b>4.1</b> User study</a></li>
<li class="chapter" data-level="4.2" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:results"><i class="fa fa-check"></i><b>4.2</b> Results</a></li>
<li class="chapter" data-level="4.3" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:conclusion"><i class="fa fa-check"></i><b>4.3</b> Conclusion</a></li>
<li class="chapter" data-level="4.4" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:spinifex"><i class="fa fa-check"></i><b>4.4</b> Accompanying tool: radial tour application</a></li>
<li class="chapter" data-level="4.5" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:appendix"><i class="fa fa-check"></i><b>4.5</b> Extended analysis</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html"><i class="fa fa-check"></i><b>5</b> Exploring Local Explanations of Nonlinear Models Using Animated Linear Projections</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:shap"><i class="fa fa-check"></i><b>5.1</b> SHAP and tree SHAP local explanations</a></li>
<li class="chapter" data-level="5.2" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:cheemviwer"><i class="fa fa-check"></i><b>5.2</b> The cheem viewer</a></li>
<li class="chapter" data-level="5.3" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#classification-task"><i class="fa fa-check"></i><b>5.3</b> Classification task</a></li>
<li class="chapter" data-level="5.4" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:casestudies"><i class="fa fa-check"></i><b>5.4</b> Case studies</a></li>
<li class="chapter" data-level="5.5" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:cheemdiscussion"><i class="fa fa-check"></i><b>5.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html"><i class="fa fa-check"></i><b>6</b> Conclusion</a>
<ul>
<li class="chapter" data-level="6.1" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html#contributions-1"><i class="fa fa-check"></i><b>6.1</b> Contributions</a></li>
<li class="chapter" data-level="6.2" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html#limitations-and-future-work"><i class="fa fa-check"></i><b>6.2</b> Limitations and future work</a></li>
</ul></li>
<li class="divider"></li>
<li><strong><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></strong></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-background" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Background</h1>
<!-- Structure -->
<p>This chapter first motivates data visualization in general and the importance of interaction. Then we cover orthogonal and observation-based visuals before turning to dimension reduction, including a further discussion of tours. Then we turn to empirical evaluations of multivariate visuals. Lastly, it concludes with nonlinear models and extending their interpretation with local explanations.</p>
<!-- Better than numerical summarization alone -->
<p>Visualization is much more robust than numerical summarization alone <span class="citation">(<a href="6-ch-conclusion.html#ref-anscombe_graphs_1973" role="doc-biblioref">Anscombe 1973</a>; <a href="6-ch-conclusion.html#ref-matejka_same_2017" role="doc-biblioref">Matejka and Fitzmaurice 2017</a>)</span>. Several datasets have the same summary statistics in these studies yet contain obvious visual shapes that could go utterly unheeded if plotting is foregone. Figure <a href="2-ch-background.html#fig:ch2fig1">2.1</a> illustrates this, where data is allowed to drift toward different patterns provided that the mean and standard deviations stay within some tolerance of the original data.</p>
(ref:ch2fig1-cap) Starting from the profile of a dinosaur, observations are allowed to drift (by iterated simulated annealing) toward 12 patterns provided that they stay close to the original statistics <span class="citation">(<a href="6-ch-conclusion.html#ref-matejka_same_2017" role="doc-biblioref">Matejka and Fitzmaurice 2017</a>)</span>. Visualization of data yields stark designs that are easy to miss in numerical summarization.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch2fig1"></span>
<img src="figures/ch2_fig1_matejka17fig.PNG" alt="(ref:ch2fig1-cap)" width="100%"  />
<p class="caption">
Figure 2.1: (ref:ch2fig1-cap)
</p>
</div>
<!-- User interaction -->
<p>Interaction is known to be an essential aspect of modern data visualization <span class="citation">(<a href="6-ch-conclusion.html#ref-batch_there_2019" role="doc-biblioref">Batch et al. 2019</a>; <a href="6-ch-conclusion.html#ref-card_psychology_2018" role="doc-biblioref">Card, Moran, and Newell 2018</a>; <a href="6-ch-conclusion.html#ref-marriott_immersive_2018" role="doc-biblioref">Marriott et al. 2018</a>)</span>. Specific interactions are closely linked to the visual method, display dimensionality, the hardware used. Linked brushing <span class="citation">(<a href="6-ch-conclusion.html#ref-becker_brushing_1987" role="doc-biblioref">Becker and Cleveland 1987</a>)</span> has proved to be helpful in tours <span class="citation">(<a href="6-ch-conclusion.html#ref-arms_benefits_1999" role="doc-biblioref">Arms, Cook, and Cruz-Neira 1999</a>; <a href="6-ch-conclusion.html#ref-laa_slice_2020" role="doc-biblioref">Laa, Cook, and Valencia 2020</a>; <a href="6-ch-conclusion.html#ref-lee_casting_2020" role="doc-biblioref">Lee, Laa, and Cook 2020</a>)</span> and ensemble graphics. Tooltip display of observation identification and displaying associated values. The novel user-steering of the projection basis in the radial tour is the primary feature of interest to discern in the user study conducted in Chapter <a href="4-ch-userstudy.html#ch-userstudy">4</a>.</p>
<!-- Data scope and context -->
<p>Before jumping into visualization, we cover the context of the data in mind. Consider the case where data is a complete numeric matrix <span class="math inline">\(X_{nxp}\)</span> containing <span class="math inline">\(n\)</span> observations of <span class="math inline">\(p\)</span> variables. Ideally, values are continuous numeric (not ordinal levels) and <span class="math inline">\(n&gt;p\)</span> with many more observations than variables. While written as though always operating on the original variables, the visualization discussed could similarly be applied to component spaces or feature decomposition of data not fitting this format.</p>
<!-- Penguins data -->
<p>The work in <span class="citation"><a href="6-ch-conclusion.html#ref-grinstein_high-dimensional_2002" role="doc-biblioref">Grinstein, Trutschl, and Cvek</a> (<a href="6-ch-conclusion.html#ref-grinstein_high-dimensional_2002" role="doc-biblioref">2002</a>)</span> gives a good taxonomy of high-dimensional visualization. Below covers a guided discussion while we generally consider the question, “How can an analyst visualize arbitrary <span class="math inline">\(p-\)</span>dimensions?” We continue to use the Palmer penguins data in illustrations. This data contains 333 observations of 3 penguin species across four physical measurements: bill length, bill depth, flipper length, and body mass. Observations were collected between 2007 and 2009 near Palmer Station, Antarctica.</p>
<div id="scatterplot-matrices" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Scatterplot matrices</h2>
<!-- SPLOM -->
<p>Viewing as many univariate histograms or density curves is one method. Similarly, one could look at all variable pairing as scatter plots. This forms the crux of the scatterplot matrices, also known as SPLOM <span class="citation">(<a href="6-ch-conclusion.html#ref-chambers_graphical_1983" role="doc-biblioref">Chambers et al. 1983</a>)</span>. In a scatterplot matrix, variables are displayed across the columns and rows. The diagonal elements show univariate densities, while off-diagonal positions show scatterplot pairs, as Figure <a href="2-ch-background.html#fig:penguinsplom">2.2</a>. This is useful for getting a handle on the support of the variables but is not going to scale well with dimension and is not a suitable audience-ready display. It is hectic and doesn’t draw attention to any one spot. <span class="citation"><a href="6-ch-conclusion.html#ref-munzner_visualization_2014" role="doc-biblioref">Munzner</a> (<a href="6-ch-conclusion.html#ref-munzner_visualization_2014" role="doc-biblioref">2014</a>)</span> reminds us to abstract the cognitive work out of the visual, allowing the audience to focus on the evidence supporting the claim.</p>
(ref:penguinsplom-cap) Scatterplot matrix of penguins data. This is a good exploratory step but does not direct the audience’s attention and will not scale well as <span class="math inline">\(p\)</span> increases.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:penguinsplom"></span>
<img src="figures/ch2_fig2_penguin_splom.png" alt="(ref:penguinsplom-cap)" width="100%"  />
<p class="caption">
Figure 2.2: (ref:penguinsplom-cap)
</p>
</div>
</div>
<div id="parallel-coordinate-plots" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Parallel coordinate plots</h2>
<!-- PCP & observation based visuals -->
<p>Alternatively, we could consider a class of observation-based visuals. In parallel coordinate plots <span class="citation">(<a href="6-ch-conclusion.html#ref-ocagne_coordonnees_1885" role="doc-biblioref">Ocagne 1885</a>)</span>, variables are arranged horizontally, and lines connect observations after being transformed to a common scale quantile or z-value (standard deviations away from the mean). Figure <a href="2-ch-background.html#fig:penguinpcp">2.3</a> illustrates this method. This scales much better with dimensions but poorly with observations. It also suffers from an asymmetry with the variable order. That is, changing the order of the variable will lead people to very different conclusions. The x-axis is also used to display variables rather than the values of the observations. This restricts the amount of information that can be interpreted between variables. Munzner asserts that position is the more human-perceptible channel for encoding information; we should prefer to reserve it for the values of the observations. The same issues persist across other observation-based displays such as radial variants, pixel-based visuals, and Chernoff faces <span class="citation">(<a href="6-ch-conclusion.html#ref-keim_designing_2000" role="doc-biblioref">Keim 2000</a>; <a href="6-ch-conclusion.html#ref-chernoff_use_1973" role="doc-biblioref">Chernoff 1973</a>)</span>. These visuals are better suited for the <span class="math inline">\(n&lt;p\)</span> case with more variables than observations.</p>
(ref:penguinpcp-cap) Parallel coordinate plots of penguins data. This does not scale well with observations, suffers from asymmetry with the variable ordering, and horizontal position is used for distinguishing between variables rather than values within the variable.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:penguinpcp"></span>
<img src="figures/ch2_fig3_penguin_pcp.png" alt="(ref:penguinpcp-cap)" width="100%"  />
<p class="caption">
Figure 2.3: (ref:penguinpcp-cap)
</p>
</div>
<!-- cavet; observation-based visuals -->
<p>However, in the <span class="math inline">\(n&lt;p\)</span> case, these visuals will perform better with fewer observations than variables. Alternatively, as <span class="math inline">\(n\)</span> increases, scatterplots displays suffer from dense points occluding each other. This is typically addressed in a few ways. One method would decrease
points’ opacity, allowing more layers to be seen. Less interestingly, visualizing a representative subset of the data is another option. Another would change the geometric display to an aggregated heatmap or 2D density contours. These aggregations typically render faster and scale better with increasing observations.</p>
</div>
<div id="dimension-reduction" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Dimension reduction</h2>
<!-- Dimension reduction -->
<p>Ultimately, we will need to turn to dimension reduction to create a compelling visual allowing audiences to focus on features with contributions from multiple variables. Dimension reduction is separated into two categories, linear and nonlinear. The linear case spans all affine mathematical transformations, essentially mappings where parallel lines stay parallel. Nonlinear transformations complement the linear case, think transformations containing exponents or interacting terms. Examples in low dimensions are relatable. For instance, shadows are an example of linear projections where a 3-dimensional object casts a 2D projection, the shadow. Our vision (at any one instance) and pictures are similarly 2D projections. An example of a nonlinear transformation is that of 2D maps of the globe. There are many different methods to distort the surface to display as a map. The most common may be rectangular displays where the area is proportionally distorted with the distance away from the equator. Other distortions are created when the surface is unwrapped into a long ellipse. Yet others create non-continuous gaps in oceans to minimize the distortion of countries.</p>
<!-- Nonlinear -->
<p>Nonlinear techniques often have hyperparameters that affect how the spaces are distorted to fit into fewer dimensions. Various computational quality metrics, such as Trustworthiness, Continuity, Normalized stress, and Average local error, describe the distortion of the space <span class="citation">(<a href="6-ch-conclusion.html#ref-espadoto_toward_2021" role="doc-biblioref">Espadoto et al. 2021</a>; <a href="6-ch-conclusion.html#ref-gracia_new_2016" role="doc-biblioref">Gracia et al. 2016</a>; <a href="6-ch-conclusion.html#ref-van_der_maaten_dimensionality_2009" role="doc-biblioref">Maaten, Postma, and Van den Herik 2009</a>; <a href="6-ch-conclusion.html#ref-venna_visualizing_2006" role="doc-biblioref">Venna and Kaski 2006</a>)</span> To quote Anastasios Panagiotelis, “All nonlinear projections are wrong, but some are useful,” a play on George Box’s quote about models. The distortions are hard to interpret and, thus hard to communicate. Moreover, they can introduce features not in the data depending on the selection of hyperparameters. Then plausibly, the presence of structure in a nonlinear model is necessary but not sufficient to conclude the existence of a structure in the data.</p>
<!-- No free lunch trade off -->
<p>Unfortunately, there is no free lunch here. An increase in the original data dimensions will lead to a <span class="math inline">\(p-d\)</span>-dimensional viewing space in the linear case or an increasingly perturbed and distorted space in nonlinear techniques. Neither is a panacea for visualizing multivariate spaces. However, we will continue with only linear projections due to the opaque distortions of nonlinear methods.</p>
<!-- Intrinsic data dimensionality -->
<p>The intrinsic dimensionality of data is the number of variables needed to minimally represent the data <span class="citation">(<a href="6-ch-conclusion.html#ref-grinstein_high-dimensional_2002" role="doc-biblioref">Grinstein, Trutschl, and Cvek 2002</a>)</span>. Intrinsic data dimensionality is an essential aspect of dimension reduction that does not have to end in visual but is also a standard part of factor analysis and preprocessing data. Consider a Psychology survey consisting of 100 questions about the Big Five personality traits. The data consists of 100 response variables, while the theory would suggest the intrinsic dimensionality is five. However, Its likely that these personalities do not fully span the five dimensions. Regardless, reducing the disparity of these volumes would be necessary to gate the exponentially increasing space to view.</p>
<!-- PCA -->
<p>One example of linear projections is that of principal component analysis <span class="citation">(PCA, <a href="6-ch-conclusion.html#ref-pearson_liii._1901" role="doc-biblioref">Pearson 1901</a>)</span>, which creates a component space ordered by descending variation. It uses eigenvalue decomposition to identify the basis reorientation. These components are typically viewed as discrete orthogonal pairs, commonly approximated in fewer components than the original dimensionality. <!--The exact number of components to keep is subjective, but typically guided by use of a screeplot [@cattell_scree_1966]. Scree plots which illustrate the decreasing variation contained in subseqanty components. The analyst then identifies an elbow in this plot;--> This is commonly used in preprocessing and model initialization step when there are many more variables than the intrinsic data dimensionality, such as the Big Five personality example. These reduced spaces can be interpreted as the original variables, albeit with the added abstraction of another linear mapping.</p>
</div>
<div id="sec:tour" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Tours, animated linear projections</h2>
<!-- Segue from barstool -->
<p>In one static linear projection let, <span class="math inline">\(Y_{nxd} = X_{nxp} \cdot A_{pxd}\)</span> be the embedding of the data mapped by the basis <span class="math inline">\(A\)</span>, where <span class="math inline">\(d&lt;p\)</span>. In contrast to one such projection, a data visualization tour animates many such projections through small changes in the basis. In the shadow analogy, structural features of an object are gained by watching its shadow change due to its rotation. An analyst similarly gains information about the data object by watching continuous changes to the basis (the orientation of the data). There are various types of tours that are classified by the generation of their basis paths. We enumerate a few related to this work. A more comprehensive discussion and review of tours can be found in the works of <span class="citation"><a href="6-ch-conclusion.html#ref-cook_grand_2008" role="doc-biblioref">Cook et al.</a> (<a href="6-ch-conclusion.html#ref-cook_grand_2008" role="doc-biblioref">2008</a>)</span> and <span class="citation"><a href="6-ch-conclusion.html#ref-lee_state_2021" role="doc-biblioref">Lee et al.</a> (<a href="6-ch-conclusion.html#ref-lee_state_2021" role="doc-biblioref">2021</a>)</span>.</p>
<!-- Grand tour -->
<p>Originally in a <em>grand</em> tour <span class="citation">(<a href="6-ch-conclusion.html#ref-asimov_grand_1985" role="doc-biblioref">Asimov 1985</a>)</span>, several target frames are randomly selected. Figure <a href="2-ch-background.html#fig:ch2fig4">2.4</a> illustrates six frames from a grand tour. The grand tour is good for EDA in that it will show frames with widely varying contributions but lacks a destination, objective function, or means of steering.</p>
(ref:ch2fig4-cap) Frames from a grand tour of Palmer Penguin data. Tours animate linear projections over small changes in the basis. The observation permanence between frames is an essential distinction in tours. Target frames are selected randomly in the grand tour. An animation can be viewed at <a href="https://vimeo.com/670936494">vimeo.com/670936494</a>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch2fig4"></span>
<img src="figures/ch2_fig4_penguin_grandtour.png" alt="(ref:ch2fig4-cap)" width="100%"  />
<p class="caption">
Figure 2.4: (ref:ch2fig4-cap)
</p>
</div>
(ref:ch2fig5-cap) Illustration of geodesic interpolation between a randomly generated manual tour target frames (grey) and the resulting intermediate frames (white). Figure from <span class="citation"><a href="6-ch-conclusion.html#ref-buja_computational_2005" role="doc-biblioref">Buja et al.</a> (<a href="6-ch-conclusion.html#ref-buja_computational_2005" role="doc-biblioref">2005</a>)</span>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch2fig5"></span>
<img src="figures/ch2_fig5_buja05fig.PNG" alt="(ref:ch2fig5-cap)" width="100%"  />
<p class="caption">
Figure 2.5: (ref:ch2fig5-cap)
</p>
</div>
<!-- Interpolation frames -->
<p>Regardless of the type of tour target basis identified, tours must interpolate frames between distant randomly generated target bases. Figure <a href="2-ch-background.html#fig:ch2fig5">2.5</a> illustrates the geodesic interpolation between distant target frames. Animating through small distances between these interpolated frames is important for the trackability of observations.</p>
</div>
<div id="evaluating-multivariate-data-visualization" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Evaluating multivariate data visualization</h2>
<!-- reason for discussing nonlinear models -->
<p>We have discussed several different multivariate visualizations. Yet, the analyst must determine which method to use. Chapter <a href="4-ch-userstudy.html#ch-userstudy">4</a>, conducts a user study comparing competing visualizations to inform this selection.</p>
<!-- 2D vs D3, mostly PCA reduced -->
<p><span class="citation"><a href="6-ch-conclusion.html#ref-gracia_new_2016" role="doc-biblioref">Gracia et al.</a> (<a href="6-ch-conclusion.html#ref-gracia_new_2016" role="doc-biblioref">2016</a>)</span> conducted an <span class="math inline">\(n=40\)</span> user study comparing between 2D and 3D scatterplots on traditional 2D monitors. Participants perform point classification, distance perception, and outlier identification tasks. The results are mixed and mostly small differences. There is some evidence to suggest a lower error in distance perception from 3D scatterplot. <span class="citation"><a href="6-ch-conclusion.html#ref-wagner_filho_immersive_2018" role="doc-biblioref">Wagner Filho et al.</a> (<a href="6-ch-conclusion.html#ref-wagner_filho_immersive_2018" role="doc-biblioref">2018</a>)</span> performed an <span class="math inline">\(n=30\)</span> within participants using scatterplot display between 2D, 3D displays on monitors and 3D display with a head-mounted display on PCA reduced spaces. None of the tasks on any dataset lead to a significant difference im accuracy. However, the immersive display reduced effort and navigation while resulting in higher perceived accuracy and engagement. <span class="citation"><a href="6-ch-conclusion.html#ref-sedlmair_empirical_2013" role="doc-biblioref">Sedlmair, Munzner, and Tory</a> (<a href="6-ch-conclusion.html#ref-sedlmair_empirical_2013" role="doc-biblioref">2013</a>)</span> instead uses two expert coders to evaluate 75 datasets and four dimension reduction techniques for 2D scatterplots, interactive 3D scatterplots, and 2D scatterplot matrices. They suggest a tiered guidance approach finding that 2D scatterplots are often ‘good enough.’ If not, try 2D scatterplots on a different dimension reduction technique before going to scatterplot matrix display or concluding a true negative. They find that interactive 3D scatterplots help in very few cases.</p>
<!-- Nonlinear DR quality review -->
<p>Empirical studies scarcely compare tours. However, <span class="citation"><a href="6-ch-conclusion.html#ref-nelson_xgobi_1999" role="doc-biblioref">Nelson, Cook, and Cruz-Neira</a> (<a href="6-ch-conclusion.html#ref-nelson_xgobi_1999" role="doc-biblioref">1998</a>)</span> compare scatterplots of grand tours on 2D monitor with 3D (stereoscopic, not head-mounted) over <span class="math inline">\(n=15\)</span> participants.
Participants perform clusters detection, dimensionality and radial sparseness tasks on six dimensional data. They find that stereoscopic 3D leads to more accuracy in the cluster identification, though interaction time was much increased in the in 3D case. In chapter Chapter <a href="4-ch-userstudy.html#ch-userstudy">4</a>, we extend the evaluation of tours which novelly compares the radial tour as benchmarked against the grand tour and discrete pairs of principal components.</p>
</div>
<div id="nonlinear-models" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> Nonlinear models</h2>
<!-- reason for discussing nonlinear models -->
<p>In Chapter <a href="5-ch-cheem.html#ch-cheem">5</a>, we turn our attention to predictive modeling, the loss of variable-level interpretability of the nonlinear models, and a method to maintain model transparency, local explanations.</p>
<!-- Introduce explanatory vs predictive modeling -->
<p>There are different reasons and emphases to fit a model. <span class="citation"><a href="6-ch-conclusion.html#ref-breiman_statistical_2001" role="doc-biblioref">Breiman</a> (<a href="6-ch-conclusion.html#ref-breiman_statistical_2001" role="doc-biblioref">2001</a>)</span>, reiterated by <span class="citation"><a href="6-ch-conclusion.html#ref-shmueli_explain_2010" role="doc-biblioref">Shmueli</a> (<a href="6-ch-conclusion.html#ref-shmueli_explain_2010" role="doc-biblioref">2010</a>)</span>, taxonomize modeling based on its purpose; <em>explanatory</em> modeling is done for some inferential purpose, while <em>predictive</em> modeling focuses more narrowly on the performance of some objective function. The intended use has important implications for model selection and development. In explanatory modeling, interpretability is vital for drawing inferential conclusions. While the use of black-box models is almost exclusively used in predictive modeling. However, the prevalence of nonlinear models is not without controversy <span class="citation">(<a href="6-ch-conclusion.html#ref-oneil_weapons_2016" role="doc-biblioref">O’Neil 2016</a>; <a href="6-ch-conclusion.html#ref-kodiyan_overview_2019" role="doc-biblioref">Kodiyan 2019</a>)</span>, and the loss of interpretation presents a challenge.</p>
<!-- Interpretability & baises -->
<p>Interpretability is vital for exploring and protecting against potential biases (e.g., sex – <span class="citation"><a href="6-ch-conclusion.html#ref-dastin_amazon_2018" role="doc-biblioref">Dastin</a> (<a href="6-ch-conclusion.html#ref-dastin_amazon_2018" role="doc-biblioref">2018</a>)</span>; <span class="citation"><a href="6-ch-conclusion.html#ref-duffy_apple_2019" role="doc-biblioref">Duffy</a> (<a href="6-ch-conclusion.html#ref-duffy_apple_2019" role="doc-biblioref">2019</a>)</span>, race – <span class="citation"><a href="6-ch-conclusion.html#ref-larson_how_2016" role="doc-biblioref">Larson et al.</a> (<a href="6-ch-conclusion.html#ref-larson_how_2016" role="doc-biblioref">2016</a>)</span>, and age – <span class="citation"><a href="6-ch-conclusion.html#ref-diaz_addressing_2018" role="doc-biblioref">Díaz et al.</a> (<a href="6-ch-conclusion.html#ref-diaz_addressing_2018" role="doc-biblioref">2018</a>)</span>) in any model. For instance, models regularly pick up on biases in the training data that have observed influence on the response (output) feature, which is then built into the model. Variable-level (feature-level) interpretability of models is essential in evaluating such biases.
<!-- Even if the optimization of an objective function is the primary concern, stakeholders, litigators, and opponents would rightly ask for the variable interpretability. --></p>
<!-- Interpretability & data drift -->
<p>Another concern is data drift, where a shift in support or domain of the explanatory variables (features or predictors). Nonlinear models are typically more sensitive and do not extrapolate well outside the support of the training data. Maintaining variable interpretability is similarly essential to address issues arising from data drift.</p>
</div>
<div id="sec:explanations" class="section level2" number="2.7">
<h2><span class="header-section-number">2.7</span> Local explanations</h2>
<!-- Local explanations -->
<p>Explainable Artificial Intelligence (XAI) is an emerging field of research that tries to increase the interpretability of black-box models. A common approach is to use <em>local explanations</em>, which attempt to approximate linear variable importance at the location of each observation (instance). That is a prediction at a specific point in the data domain. Because these are point-specific, the challenge is to visualize them to understand a model comprehensively.</p>
<!-- Reminder of local explanation -->
<p>Consider a highly nonlinear model. It can be hard to determine whether small changes in a variable’s value will make a class prediction change group or identify which variables contribute to an extreme residual. Local explanations shed light on these situations by approximating linear variable’s importance in the vicinity of a single observation. Figure <a href="2-ch-background.html#fig:ch2fig6">2.6</a> motivates local explanations where the analyst wants to know the variable attribution for a particular observation close to the classification boundary in a nonlinear model.</p>
(ref:ch2fig6-cap) Illustration of a nonlinear classification model. An analyst may want to know the variable importance at the vicinity of the highlighted red cross. Knowing this attribution elucidates how the variable influence this point that is precariously close to the classification boundary. Local explanations approximate this linear attribution in the vicinity of one observation. Figure from <span class="citation"><a href="6-ch-conclusion.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro, Singh, and Guestrin</a> (<a href="6-ch-conclusion.html#ref-ribeiro_why_2016" role="doc-biblioref">2016</a>)</span>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch2fig6"></span>
<img src="figures/ch2_fig6_ribeiro16fig.PNG" alt="(ref:ch2fig6-cap)" width="50%"  />
<p class="caption">
Figure 2.6: (ref:ch2fig6-cap)
</p>
</div>
<!-- Taxonomy of local explanations -->
<p>A comprehensive summary of the taxonomy and literature of explanation techniques is provided in Figure 6 of <span class="citation"><a href="6-ch-conclusion.html#ref-arrieta_explainable_2020" role="doc-biblioref">Arrieta et al.</a> (<a href="6-ch-conclusion.html#ref-arrieta_explainable_2020" role="doc-biblioref">2020</a>)</span>. It includes a large number of model-specific explanations such as deepLIFT <span class="citation">(<a href="6-ch-conclusion.html#ref-shrikumar_not_2016" role="doc-biblioref">Shrikumar et al. 2016</a>; <a href="6-ch-conclusion.html#ref-shrikumar_learning_2017" role="doc-biblioref">Shrikumar, Greenside, and Kundaje 2017</a>)</span>, a popular recursive method for estimating importance in neural networks. There are fewer model-agnostic explanations, of which LIME, <span class="citation">(<a href="6-ch-conclusion.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>)</span> SHAP, <span class="citation">(<a href="6-ch-conclusion.html#ref-lundberg_unified_2017" role="doc-biblioref">Lundberg and Lee 2017</a>)</span>, and their variants are popular.</p>
<!-- Uses of local explanations -->
<p>These observation-level explanations are used in various ways depending on the context. In image classification, a saliency map indicate important pixels for the resulting classification <span class="citation">(<a href="6-ch-conclusion.html#ref-simonyan_deep_2014" role="doc-biblioref">Simonyan, Vedaldi, and Zisserman 2014</a>)</span>. For example, snow is regularly highlighted when distinguishing if a picture contains a wolf or husky <span class="citation">(<a href="6-ch-conclusion.html#ref-besse_can_2019" role="doc-biblioref">Besse et al. 2019</a>)</span>. In text analysis, word-level contextual sentiment analysis can be used to highlight the sentiment and magnitude of influential words <span class="citation">(<a href="6-ch-conclusion.html#ref-vanni_textual_2018" role="doc-biblioref">Vanni et al. 2018</a>)</span>. In the case of numeric regression, they are used to explain variable additive contributions from the observed mean to the observation’s prediction <span class="citation">(<a href="6-ch-conclusion.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>)</span>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="1-ch-introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-ch-spinifex.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
