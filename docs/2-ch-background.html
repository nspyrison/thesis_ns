<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Background | Interactive and dynamic visualization of high-dimensional data</title>
  <meta name="description" content="Chapter 2 Background | Interactive and dynamic visualization of high-dimensional data" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Background | Interactive and dynamic visualization of high-dimensional data" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Background | Interactive and dynamic visualization of high-dimensional data" />
  
  
  

<meta name="author" content="Nicholas S Spyrison" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="1-ch-introduction.html"/>
<link rel="next" href="3-ch-spinifex.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="template/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./"> </a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#research-questions"><i class="fa fa-check"></i><b>1.1</b> Research questions</a></li>
<li class="chapter" data-level="1.2" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#methodology"><i class="fa fa-check"></i><b>1.2</b> Methodology</a></li>
<li class="chapter" data-level="1.3" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#contributions"><i class="fa fa-check"></i><b>1.3</b> Contributions</a></li>
<li class="chapter" data-level="1.4" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#thesis-structure"><i class="fa fa-check"></i><b>1.4</b> Thesis structure</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-background.html"><a href="2-ch-background.html"><i class="fa fa-check"></i><b>2</b> Background</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2-ch-background.html"><a href="2-ch-background.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="2-ch-background.html"><a href="2-ch-background.html#multivariate-visualization"><i class="fa fa-check"></i><b>2.2</b> Multivariate visualization</a></li>
<li class="chapter" data-level="2.3" data-path="2-ch-background.html"><a href="2-ch-background.html#linear-projections"><i class="fa fa-check"></i><b>2.3</b> Linear projections</a></li>
<li class="chapter" data-level="2.4" data-path="2-ch-background.html"><a href="2-ch-background.html#evaluating-multivariate-data-visualization"><i class="fa fa-check"></i><b>2.4</b> Evaluating multivariate data visualization</a></li>
<li class="chapter" data-level="2.5" data-path="2-ch-background.html"><a href="2-ch-background.html#nonlinear-models"><i class="fa fa-check"></i><b>2.5</b> Nonlinear models</a></li>
<li class="chapter" data-level="2.6" data-path="2-ch-background.html"><a href="2-ch-background.html#sec:explanations"><i class="fa fa-check"></i><b>2.6</b> Local explanations</a></li>
<li class="chapter" data-level="2.7" data-path="2-ch-background.html"><a href="2-ch-background.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html"><i class="fa fa-check"></i><b>3</b> A User-Controlled Manual Tour for Animated Linear Projections</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:algorithm"><i class="fa fa-check"></i><b>3.1</b> Algorithm</a></li>
<li class="chapter" data-level="3.2" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:oblique"><i class="fa fa-check"></i><b>3.2</b> Oblique cursor movement</a></li>
<li class="chapter" data-level="3.3" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:pkgstructure"><i class="fa fa-check"></i><b>3.3</b> Package structure</a></li>
<li class="chapter" data-level="3.4" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:usecases"><i class="fa fa-check"></i><b>3.4</b> Use cases</a></li>
<li class="chapter" data-level="3.5" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:discussion"><i class="fa fa-check"></i><b>3.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html"><i class="fa fa-check"></i><b>4</b> A Study on the Benefit of a User-Controlled Radial Tour for Variable Attribution of Structure</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:userstudy"><i class="fa fa-check"></i><b>4.1</b> User study</a></li>
<li class="chapter" data-level="4.2" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:results"><i class="fa fa-check"></i><b>4.2</b> Results</a></li>
<li class="chapter" data-level="4.3" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:conclusion"><i class="fa fa-check"></i><b>4.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html"><i class="fa fa-check"></i><b>5</b> Exploring Local Explanations of Nonlinear Models Using the Radial tour</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:shap"><i class="fa fa-check"></i><b>5.1</b> SHAP and tree SHAP local explanations</a></li>
<li class="chapter" data-level="5.2" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:cheemviewer"><i class="fa fa-check"></i><b>5.2</b> The cheem viewer</a></li>
<li class="chapter" data-level="5.3" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:casestudies"><i class="fa fa-check"></i><b>5.3</b> Case studies</a></li>
<li class="chapter" data-level="5.4" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:cheemdiscussion"><i class="fa fa-check"></i><b>5.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html"><i class="fa fa-check"></i><b>6</b> Conclusion</a>
<ul>
<li class="chapter" data-level="6.1" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html#contributions-1"><i class="fa fa-check"></i><b>6.1</b> Contributions</a></li>
<li class="chapter" data-level="6.2" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html#limitations"><i class="fa fa-check"></i><b>6.2</b> Limitations</a></li>
<li class="chapter" data-level="6.3" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html#future-work"><i class="fa fa-check"></i><b>6.3</b> Future work</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-sec-glossary.html"><a href="A-sec-glossary.html"><i class="fa fa-check"></i><b>A</b> Glossary</a></li>
<li class="chapter" data-level="B" data-path="B-supplementary-material.html"><a href="B-supplementary-material.html"><i class="fa fa-check"></i><b>B</b> Supplementary material</a>
<ul>
<li class="chapter" data-level="B.1" data-path="B-supplementary-material.html"><a href="B-supplementary-material.html#thesis"><i class="fa fa-check"></i><b>B.1</b> Thesis</a></li>
<li class="chapter" data-level="B.2" data-path="B-supplementary-material.html"><a href="B-supplementary-material.html#chapter-3-spinifex-links"><i class="fa fa-check"></i><b>B.2</b> Chapter 3, spinifex links</a></li>
<li class="chapter" data-level="B.3" data-path="B-supplementary-material.html"><a href="B-supplementary-material.html#sec:extanalysis"><i class="fa fa-check"></i><b>B.3</b> Chapter 4, user study extended analysis</a></li>
<li class="chapter" data-level="B.4" data-path="B-supplementary-material.html"><a href="B-supplementary-material.html#chapter-5-cheem-links"><i class="fa fa-check"></i><b>B.4</b> Chapter 5, cheem links</a></li>
</ul></li>
<li class="divider"></li>
<li><strong><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></strong></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interactive and dynamic visualization of high-dimensional data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-background" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Background</h1>
<!-- Structure -->
<p>The last chapter discussed the importance of data visualization despite the complexity of viewing high-dimensional data and outlined the research questions to be addressed around the user-control of the radial tour. This chapter first motivates data visualization in general and the importance of interaction. It continues to illustrate common visualizations for quantitative multivariate data before turning to dimension reduction, including further discussion of tours. Then empirical evaluations of multivariate visuals are covered. The chapter concludes with nonlinear models and the extension of their interpretation with local explanations.</p>
<div id="motivation" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Motivation</h2>
<!-- Better than numerical summarization alone -->
<p>Visualization is much more robust than numerical summarization alone <span class="citation">(<a href="bibliography.html#ref-anscombe_graphs_1973" role="doc-biblioref">Anscombe 1973</a>; <a href="bibliography.html#ref-matejka_same_2017" role="doc-biblioref">Matejka and Fitzmaurice 2017</a>)</span>. Figure <a href="2-ch-background.html#fig:ch2fig1">2.1</a> illustrates this. The data sets have a quite different structure revealed in their visualization, while they contain the same summary statistics. All data sets have the same means, standard deviations, and correlation.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch2fig1"></span>
<img src="figures/ch2_fig1_datasaurus.png" alt="The datasaurus dozen is a modern data set illustrating Anscombe’s point that summary statistics cannot always adequately summarize data content. Patterns in the scatterplots are evident despite the data having the same summary statistics. Unless the data is plotted, one would never know that there were such radical differences." width="100%"  />
<p class="caption">
Figure 2.1: The datasaurus dozen is a modern data set illustrating Anscombe’s point that summary statistics cannot always adequately summarize data content. Patterns in the scatterplots are evident despite the data having the same summary statistics. Unless the data is plotted, one would never know that there were such radical differences.
</p>
</div>
<!-- Data interaction -->
<p>Interaction is an essential aspect of modern data visualization <span class="citation">(<a href="bibliography.html#ref-card_psychology_1983" role="doc-biblioref">Card et al. 1983</a>; <a href="bibliography.html#ref-marriott_immersive_2018" role="doc-biblioref">Marriott et al. 2018</a>; <a href="bibliography.html#ref-batch_there_2019" role="doc-biblioref">Batch et al. 2019</a>)</span>. Interaction facilitates accessing increasing amounts of data and amplifies cognition through control and input <span class="citation">(<a href="bibliography.html#ref-dimara_what_2019" role="doc-biblioref">Dimara and Perin 2019</a>)</span>.<!--The term interaction covers a broad range of uses, purposes, and contexts, making it overburdened and potentially ambiguous.--> <span class="citation"><a href="bibliography.html#ref-munzner_visualization_2014" role="doc-biblioref">Munzner</a> (<a href="bibliography.html#ref-munzner_visualization_2014" role="doc-biblioref">2014</a>)</span> posits that responsive and fast computer graphics allow us to move beyond paper and static resources. Interaction is key to navigating within views of sizable data and linking observations and features across these views.</p>
<!-- Multivariate data vis interaction -->
<p>We focus on multivariate data interactions with coordinated views, linked brushing, and tooltip display. Coordinated and multiple views <span class="citation">(<a href="bibliography.html#ref-roberts_state_2007" role="doc-biblioref">Roberts 2007</a>)</span> <span class="citation">(also known as ensemble graphics, <a href="bibliography.html#ref-unwin_ensemble_2018" role="doc-biblioref">Unwin and Valero-Mora 2018</a>)</span> use several types of visuals that give a more comprehensive understanding than any one visual portrays in isolation. The linking of observations between different views and animation frames is facilitated by linked brushing <span class="citation">(<a href="bibliography.html#ref-becker_brushing_1987" role="doc-biblioref">Becker and Cleveland 1987</a>)</span>. In linked brushing, selected observations in one view are colored across all views, allowing these selections to be tracked and correlated across other views and frames. Linked brushing has proved to be helpful in animated tours <span class="citation">(<a href="bibliography.html#ref-arms_benefits_1999" role="doc-biblioref">Arms et al. 1999</a>; <a href="bibliography.html#ref-laa_slice_2020" role="doc-biblioref">Laa et al. 2020</a>; <a href="bibliography.html#ref-lee_casting_2020" role="doc-biblioref">Lee et al. 2020</a>)</span>. Tooltip displays upon the cursor hovering over observation can display identification information and other associated values to aid with point identification and more detailed information to be accessed <span class="citation">(<a href="bibliography.html#ref-sievert_plotly_2018" role="doc-biblioref">Sievert 2018</a>)</span>. The interactive selection of parameters extends the breadth of analysis.</p>
</div>
<div id="multivariate-visualization" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Multivariate visualization</h2>
<!-- Data scope and context -->
<p>In this thesis, we are concerned with the visualization of multivariate data. Specifically, we are interested in quantitative multivariate data. We assume that our data consists of <span class="math inline">\(n\)</span> observations of <span class="math inline">\(p\)</span> variables. Generally <span class="math inline">\(n&gt;p\)</span> with many more observations than variables. While written as though operating on the original variables, the discussion below could also be applied to reduced component spaces (such as PCA approximation in a few components) or feature decomposition of data not fitting this format. <span class="citation"><a href="bibliography.html#ref-kang_visualising_2017" role="doc-biblioref">Kang et al.</a> (<a href="bibliography.html#ref-kang_visualising_2017" role="doc-biblioref">2017</a>)</span> provide an excellent example of decomposing time series data in a quantitative feature matrix.</p>
<!-- Penguins data -->
<p><span class="citation"><a href="bibliography.html#ref-grinstein_high-dimensional_2002" role="doc-biblioref">Grinstein et al.</a> (<a href="bibliography.html#ref-grinstein_high-dimensional_2002" role="doc-biblioref">2002</a>)</span> illustrate many multivariate visualization methods. In particular, this work shows examples of actual visuals. <span class="citation"><a href="bibliography.html#ref-liu_visualizing_2017" role="doc-biblioref">Liu et al.</a> (<a href="bibliography.html#ref-liu_visualizing_2017" role="doc-biblioref">2017</a>)</span> give a good classification and taxonomy of such methods. The content below focuses on the most common and the most relevant visuals. Illustrations are provided with the Palmer penguins data that was used in the Introduction. This data contains 333 observations of three penguin species across four physical measurements: bill length, bill depth, flipper length, and body mass. Observations were collected between 2007 and 2009 near Palmer Station, Antarctica. The penguins data is a good substitute for the over-used iris data.</p>
<div id="scatterplot-matrices" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Scatterplot matrices</h3>
<!-- SPLOM -->
<p>An analyst could look at <span class="math inline">\(p\)</span>-univariate histograms or density curves. Extending this idea, pairs of variables can be exhaustively viewed. Combining these brings us to scatterplot matrices, also known as SPLOM <span class="citation">(<a href="bibliography.html#ref-chambers_graphical_1983" role="doc-biblioref">Chambers et al. 1983</a>)</span>. In a scatterplot matrix, variables are arranged across the columns and rows. The diagonal elements show univariate densities, while off-diagonal positions show scatterplot pairs, as in Figure <a href="2-ch-background.html#fig:penguinsplom">2.2</a>. This is useful for getting a handle on the range of the variables but is not going to scale well when the number of variables <span class="math inline">\(p\)</span> is large. Such visualization will only partially resolve features that could be better shown with contributions from more dimensions.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:penguinsplom"></span>
<img src="figures/ch2_fig2_penguin_splom.png" alt="A scatterplot matrix shows univariate densities and all pairs of bivariate scatterplots. The panels show partial cluster separation, indicating that these variables contain discerning information. This approach is suitable for quickly exploring the range of the data but will not reveal features in more than two dimensions. It is a good exploratory visual but does not scale well with increasing variables." width="100%"  />
<p class="caption">
Figure 2.2: A scatterplot matrix shows univariate densities and all pairs of bivariate scatterplots. The panels show partial cluster separation, indicating that these variables contain discerning information. This approach is suitable for quickly exploring the range of the data but will not reveal features in more than two dimensions. It is a good exploratory visual but does not scale well with increasing variables.
</p>
</div>
<!-- scaling with n -->
<p>As <span class="math inline">\(n\)</span> increases, scatterplot displays also suffer from occlusion as the points overlay each other. This is typically addressed in a few ways. One method is to decrease points’ opacity, allowing more layers to be seen. Another approach is to change the geometric display, such as a 2D density contour or an aggregated heatmap (illustrated in Figure <a href="2-ch-background.html#fig:ch2fig5">2.5</a>). Aggregated displays typically render faster and scale better with increasing observations. Or, if needed, visualization can be performed on a representative subset of the data.</p>
</div>
<div id="parallel-coordinate-plots" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Parallel coordinate plots</h3>
<!-- PCP -->
<p>In scatterplot matrices, each observation is split across all panels. In contrast, observation-linked visuals have a single line or glyph for each observation. In parallel coordinate plots <span class="citation">(<a href="bibliography.html#ref-ocagne_coordonnees_1885" role="doc-biblioref">Ocagne 1885</a>)</span>, variables are arranged horizontally, and lines connect observations after being transformed to a common scale such as quantiles or z-score (standard deviations away from the mean). Figure <a href="2-ch-background.html#fig:penguinpcp">2.3</a> illustrates this method.</p>
<!-- Limitations and scaling -->
<p>Parallel coordinate plots scale much better with dimensions than scatterplot matrices but more poorly with observations. They also suffer from an asymmetry with the variable order. That is, changing the order of the variables may lead to very different conclusions. The <span class="math inline">\(x\)</span>-axis is also used to display variables rather than the values of the observations. This restricts the amount of information that can be interpreted between variables. <span class="citation"><a href="bibliography.html#ref-munzner_visualization_2014" role="doc-biblioref">Munzner</a> (<a href="bibliography.html#ref-munzner_visualization_2014" role="doc-biblioref">2014</a>)</span> asserts that position is the more human-perceptible channel for encoding information; we should prefer to reserve it for distinguishing between values of the observations rather than the arrangement of variables.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:penguinpcp"></span>
<img src="figures/ch2_fig3_penguin_pcp.png" alt="Parallel coordinate plots put variables on a common scale and position them side-by-side with lines connecting observations. Some variation between the clusters can be seen corroborating their importance to explaining cluster separation. This approach scales relatively well with the number of variables but poorly with the number of observations." width="100%"  />
<p class="caption">
Figure 2.3: Parallel coordinate plots put variables on a common scale and position them side-by-side with lines connecting observations. Some variation between the clusters can be seen corroborating their importance to explaining cluster separation. This approach scales relatively well with the number of variables but poorly with the number of observations.
</p>
</div>
<!-- Extension to observation-linked visuals -->
<p>The same issues persist across displays that map observations into <span class="math inline">\(n\)</span> glyphs or pixel heatmaps. Examples of these include star plots <span class="citation">(<a href="bibliography.html#ref-chambers_graphical_1983" role="doc-biblioref">Chambers et al. 1983</a>)</span>, pixel-based visuals <span class="citation">(<a href="bibliography.html#ref-keim_designing_2000" role="doc-biblioref">Keim 2000</a>)</span>, and Chernoff faces <span class="citation">(<a href="bibliography.html#ref-chernoff_use_1973" role="doc-biblioref">Chernoff 1973</a>)</span>. Like parallel coordinate plots, these other visuals scale quite poorly with increasing observations. However, because these visuals scale well with the number of variables, they may be candidate visualizations for low <span class="math inline">\(n\)</span>, high <span class="math inline">\(p\)</span> data.</p>
</div>
<div id="dimension-reduction" class="section level3" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Dimension reduction</h3>
<!-- Dimension reduction, linear and nonlinear -->
<p>The other main approach for visualizing quantitative multivariate data is dimension reduction. This involves a function mapping <span class="math inline">\(p\)</span>-space onto a lower <span class="math inline">\(d\)</span>-dimensional space. Dimension reduction is separated into two categories, linear and nonlinear. The linear case spans all affine mathematical transformations, essentially any function where parallel lines stay parallel. Nonlinear transformations complement the linear case, think transformations containing exponents or interacting terms.</p>
<!-- linear and nonlinear examples -->
<p>Examples in low dimensions are relatable. For instance, shadows are linear projections of a 3-dimensional object down to a 2D shadow. Linear perspective drawings are another instance. An example of a nonlinear transformation is that of 2D maps of the globe. A common example is the Mercator projection, a rectangular display where the area is proportionally distorted with the distance away from the equator <span class="citation">(<a href="bibliography.html#ref-snyder_map_1987" role="doc-biblioref">Snyder 1987</a>)</span>. Other distortions are created when the surface is unwrapped into an elongated ellipse. Yet others create non-continuous gaps on land or oceans to minimize the distortion of targeted areas. Snyder lists over 200 different projections that distort the surface to display as a map, each with unique properties. However, despite familiarity with map projections, users find it difficult to understand the distortions they introduce <span class="citation">(<a href="bibliography.html#ref-hennerdal_beyond_2015" role="doc-biblioref">Hennerdal 2015</a>)</span>.</p>
<!-- Nonlinear hyperparameters -->
<p>As illustrated by map projections, nonlinear projections can be challenging to understand. Various computational quality metrics, such as Trustworthiness, Continuity, Normalized stress, and Average local error, have been introduced to describe the distortion of the space <span class="citation">(<a href="bibliography.html#ref-venna_visualizing_2006" role="doc-biblioref">Venna and Kaski 2006</a>; <a href="bibliography.html#ref-van_der_maaten_dimensionality_2009" role="doc-biblioref">Maaten et al. 2009</a>; <a href="bibliography.html#ref-gracia_new_2016" role="doc-biblioref">Gracia et al. 2016</a>; <a href="bibliography.html#ref-espadoto_toward_2021" role="doc-biblioref">Espadoto et al. 2021</a>)</span>. To quote <span class="citation"><a href="bibliography.html#ref-panagiotelis_manifold_2020" role="doc-biblioref">Panagiotelis</a> (<a href="bibliography.html#ref-panagiotelis_manifold_2020" role="doc-biblioref">2020</a>)</span>, “All nonlinear projections are wrong, but some are useful,” a play on George Box’s quote about models <span class="citation">(<span>“All models are wrong, but some are useful,”</span> <a href="bibliography.html#ref-box_science_1976" role="doc-biblioref">Box 1976</a>)</span>.</p>
<!-- Dealing with hyper parameters & Sticking with linear approaches -->
<p>Furthermore, nonlinear projections have hyperparameters (absent from linear methods) that control how the spaces are distorted to fit into fewer dimensions. These introduce a degree of subjectivity into the resulting projection. Opinions differ on how to best deal with hyperparameters. <span class="citation"><a href="bibliography.html#ref-probst_hyperparameters_2019" role="doc-biblioref">Probst et al.</a> (<a href="bibliography.html#ref-probst_hyperparameters_2019" role="doc-biblioref">2019b</a>)</span> discuss tuning strategies. Others compare implementation default values <span class="citation">(<a href="bibliography.html#ref-gijsbers_meta_2021" role="doc-biblioref">Gijsbers et al. 2021</a>; <a href="bibliography.html#ref-pfisterer_learning_2021" role="doc-biblioref">Pfisterer et al. 2021</a>)</span>. <span class="citation"><a href="bibliography.html#ref-probst_tunability_2019" role="doc-biblioref">Probst et al.</a> (<a href="bibliography.html#ref-probst_tunability_2019" role="doc-biblioref">2019a</a>)</span> look at the sensitivity of hyperparameters to the performance of a model. Automated machine learning takes a programmatic approach to hyperparameter tuning <span class="citation">(<a href="bibliography.html#ref-feurer_efficient_2015" role="doc-biblioref">Feurer et al. 2015</a>; <a href="bibliography.html#ref-hutter_automated_2019" role="doc-biblioref">Hutter et al. 2019</a>; <a href="bibliography.html#ref-yao_taking_2019" role="doc-biblioref">Yao et al. 2019</a>)</span>. Due to the difficulty of interpreting nonlinear mappings and the added subjectivity of hyperparameter selection, this thesis focuses on linear visualization techniques.</p>
</div>
<div id="intrinsic-data-dimensionality" class="section level3" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Intrinsic data dimensionality</h3>
<!-- Intrinsic data dimensionality -->
<p>One way dimension reduction is used is to project multivariate data onto 1-, 2-, or 3D space and visualize the results. However, it can also be used as a preprocessing step for analysis in <span class="math inline">\(d\)</span>-dimensions. The intrinsic dimensionality of data is the number of variables needed to minimally represent the data <span class="citation">(<a href="bibliography.html#ref-grinstein_high-dimensional_2002" role="doc-biblioref">Grinstein et al. 2002</a>)</span>. Intrinsic data dimensionality is an essential consideration of dimension reduction. Consider a Psychology survey consisting of 100 questions about the Big Five personality traits. The data consists of 100 response variables, while the theory would suggest the intrinsic dimensionality is five. It thus makes sense to project onto five dimensions and analyze and visualize this embedded space rather than the original space as this mitigates the exponentially increasing volume of the view space and the computational load with minimal information loss.
<!--Nonlinear techniques regularly perform an implicit PC initialization step that may default to keeping several dozen components to alleviate the run time of data with many variables.--></p>
</div>
</div>
<div id="linear-projections" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Linear projections</h2>
<p>Linear projections map a higher <span class="math inline">\(p\)</span>-dimensional space onto a smaller <span class="math inline">\(d\)</span>-space with an affine mapping (where parallel lines stay parallel). A projection is the resulting space of the data multiplied by a <em>basis</em> <span class="math inline">\(Y_{n \times d} = X_{n \times p} \cdot A_{p \times d}\)</span>. This is an orthonormal matrix that explains the orientation and magnitudes that variables contribute to the resulting space. This basis is often illustrated as a biplot <span class="citation">(<a href="bibliography.html#ref-gabriel_biplot_1971" role="doc-biblioref">Gabriel 1971</a>)</span>, where variable contributions are inscribed in a unit circle showing the direction and the magnitude of contribution as presented in the previous chapter and Figure <a href="2-ch-background.html#fig:ch2fig4">2.4</a>.</p>
<!-- PCA -->
<p>A standard linear projection is principal component analysis <span class="citation">(PCA, <a href="bibliography.html#ref-pearson_liii._1901" role="doc-biblioref">Pearson 1901</a>)</span>, which creates a component space ordered by descending variation. It uses eigenvalue decomposition to identify the basis. These components are typically viewed as discrete orthogonal pairs, commonly approximated as several components. The exact number of components to keep is subjective but typically guided by a screeplot <span class="citation">(<a href="bibliography.html#ref-cattell_scree_1966" role="doc-biblioref">Cattell 1966</a>)</span>. Scree plots illustrate the decreasing variation contained in subsequent components. The analyst then identifies an “elbow” in this plot. PCA is also commonly used in preprocessing to reduce the number of dimensions to embed the data in a space corresponding to the intrinsic dimensionality.</p>
<div id="sec:tour" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Tours, animated linear projections</h3>
<!-- Segue from barstool -->
<p>In a static linear projection, there is only one basis. However, a single projection may not reveal the structure of interest. In contrast, a data visualization tour shows multiple projections by animating between small changes in the basis. In the shadow analogy, structural information of an object is gained by watching its shadow change due to its rotation. An analyst similarly gains structural information by watching a continuous projection over changes to the basis (essentially a rotation of the data). There are various types of tours that are classified by the generation of their basis paths. We enumerate a few related to this work. A more comprehensive discussion and review of tours can be found in the works of <span class="citation"><a href="bibliography.html#ref-cook_grand_2008" role="doc-biblioref">Cook et al.</a> (<a href="bibliography.html#ref-cook_grand_2008" role="doc-biblioref">2008</a>)</span> and <span class="citation"><a href="bibliography.html#ref-lee_state_2021" role="doc-biblioref">Lee et al.</a> (<a href="bibliography.html#ref-lee_state_2021" role="doc-biblioref">2021</a>)</span>.</p>
<!-- Interpolation bases -->
<p>Regardless of the type of tour, they generate a sequence of target bases, and then the tour must interpolate intermediate bases between consecutive bases. This interpolation is performed along a geodesic path between the bases. Geodesic refers to the shortest path (on a <span class="math inline">\(p\)</span>-sphere of possible bases). This ends up being slightly curved in 2D representation for the same reason that flight paths appear curved on maps. The interpolation of bases at small enough angles is foundational for the trackability of observations between frames.</p>
<!-- Grand tour -->
<p>Originally in a <em>grand</em> tour <span class="citation">(<a href="bibliography.html#ref-asimov_grand_1985" role="doc-biblioref">Asimov 1985</a>)</span>, several target bases are randomly selected. Figure <a href="2-ch-background.html#fig:ch2fig4">2.4</a> illustrates six frames of a grand tour. The grand tour is suitable for exploratory data analysis. It will show bases with widely varying contributions but lacks a means of steering the tour and controlling the choice of bases.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch2fig4"></span>
<img src="figures/ch2_fig4_penguin_grandtour.png" alt="Frames from a grand tour. Biplots (grey circles) illustrate the direction and magnitude that variables contribute. In the grand tour, target bases are selected randomly. Tours animate linear projections over small changes in the basis. The observation permanence between frames is an essential distinction in tours. An animation can be viewed at vimeo.com/676723441." width="100%"  />
<p class="caption">
Figure 2.4: Frames from a grand tour. Biplots (grey circles) illustrate the direction and magnitude that variables contribute. In the grand tour, target bases are selected randomly. Tours animate linear projections over small changes in the basis. The observation permanence between frames is an essential distinction in tours. An animation can be viewed at <a href="https://vimeo.com/676723441">vimeo.com/676723441</a>.
</p>
</div>
<!-- Manual tour -->
<p>In contrast, the <em>manual tour</em> <span class="citation">(<a href="bibliography.html#ref-cook_manual_1997" role="doc-biblioref">Cook and Buja 1997</a>)</span> allows the analyst to change the contribution of a selected variable. It does so by initializing a manipulation dimension on a 1- or 2D projection which can then be rotated to alter the contribution of a selected variable. This work focuses on the <em>radial tour</em> sub-variant. The contribution angle is fixed, and the magnitude of contribution varies along the radius, as illustrated in the biplot display. We saw an example of this in Figure <a href="1-ch-introduction.html#fig:ch1fig3">1.2</a>.</p>
<!-- dealing with density -->
<p>Figure <a href="2-ch-background.html#fig:ch2fig5">2.5</a> illustrates the same radial tour shown in the previous chapter across three geometric displays designed to overcome occlusion issues when there are a large number of observations. The use of density contours and aggregated heatmap displays help to mitigate the occlusion of dense observations and is compatible with any scatterplot display.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch2fig5"></span>
<img src="figures/ch2_fig5_penguin_manualtour_geoms.png" alt="Radial tour across three geometric displays. Changing the point opacity or geometric display to density contours or aggregated heatmap are common ways to mitigate occlusion caused by dense observations. The radial tour removes the contribution of bl and, with it, the separation between the orange and green clusters. Heatmap display is not the best choice to show supervised cluster separation but can be helpful to see structure in dense data." width="100%"  />
<p class="caption">
Figure 2.5: Radial tour across three geometric displays. Changing the point opacity or geometric display to density contours or aggregated heatmap are common ways to mitigate occlusion caused by dense observations. The radial tour removes the contribution of <code>bl</code> and, with it, the separation between the orange and green clusters. Heatmap display is not the best choice to show supervised cluster separation but can be helpful to see structure in dense data.
</p>
</div>
</div>
</div>
<div id="evaluating-multivariate-data-visualization" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Evaluating multivariate data visualization</h2>
<!-- quanatative surveys (calculable metrics) -->
<p>Definitions and surveys of quality metrics for the distortion of nonlinear reduction are given in <span class="citation"><a href="bibliography.html#ref-bertini_quality_2011" role="doc-biblioref">Bertini et al.</a> (<a href="bibliography.html#ref-bertini_quality_2011" role="doc-biblioref">2011</a>)</span>. The latest, most comprehensive quantitative surveys are discussed in <span class="citation"><a href="bibliography.html#ref-espadoto_toward_2021" role="doc-biblioref">Espadoto et al.</a> (<a href="bibliography.html#ref-espadoto_toward_2021" role="doc-biblioref">2021</a>)</span> and <span class="citation"><a href="bibliography.html#ref-nonato_multidimensional_2018" role="doc-biblioref">Nonato and Aupetit</a> (<a href="bibliography.html#ref-nonato_multidimensional_2018" role="doc-biblioref">2018</a>)</span>. The former compares 44 dimension reduction techniques across 18 datasets over seven metrics. While the former papers mention tours, they are absent from quantitative surveys.</p>
<!-- Orthogonal variables -->
<p>Some studies compare visualizations across complete contributions of variables.
<span class="citation"><a href="bibliography.html#ref-chang_evaluation_2018" role="doc-biblioref">Chang et al.</a> (<a href="bibliography.html#ref-chang_evaluation_2018" role="doc-biblioref">2018</a>)</span> conducted an <span class="math inline">\(n=51\)</span> participant study comparing parallel coordinate plots and SPLOM either in isolation, sequentially, or as a coordinated view. Accuracy, completion time, and eye focus were measured for six tasks. Three tasks were more accurate with SPLOM and three with parallel coordinates, while the coordinated view was usually marginally more accurate than the max of the separate visuals. <span class="citation"><a href="bibliography.html#ref-cao_z-glyph_2018" role="doc-biblioref">Cao et al.</a> (<a href="bibliography.html#ref-cao_z-glyph_2018" role="doc-biblioref">2018</a>)</span> compare unstandardized line-glyph and star-glyphs with standardized variants (with and without curve fill). Each of the <span class="math inline">\(n=18\)</span> participants performs 72 trials across the six visuals, two levels of dimensions, and two levels of observations. Visuals with variable standardization outperform the unstandardized variants and the radial star-glyph reportedly outperformed line-variant.</p>
<!-- 2D vs D3, mostly PCA reduced -->
<p>Other studies have investigated the relative benefits of projecting to 2- or 3D scatterplots in PCA-reduced spaces. <span class="citation"><a href="bibliography.html#ref-gracia_new_2016" role="doc-biblioref">Gracia et al.</a> (<a href="bibliography.html#ref-gracia_new_2016" role="doc-biblioref">2016</a>)</span> conducted an <span class="math inline">\(n=40\)</span> user study comparing 2- and 3D scatterplots on traditional 2D monitors. Participants perform point classification, distance perception, and outlier identification tasks. The results are mixed and primarily have small differences. There is some evidence to suggest a lower error in distance perception from a 3D scatterplot. <span class="citation"><a href="bibliography.html#ref-wagner_filho_immersive_2018" role="doc-biblioref">Wagner Filho et al.</a> (<a href="bibliography.html#ref-wagner_filho_immersive_2018" role="doc-biblioref">2018</a>)</span> performed an <span class="math inline">\(n=30\)</span> within participants study on PCA reduced space using scatterplot displays between 2D on monitors, 3D on monitors, and 3D display with a head-mounted display. None of the tasks on any dataset lead to a significant difference in accuracy. However, the immersive display reduced effort and navigation, resulting in higher perceived accuracy and engagement.</p>
<!-- Expert & cohort coding -->
<p>Some studies use an expert or cohort coding. <span class="citation"><a href="bibliography.html#ref-sedlmair_empirical_2013" role="doc-biblioref">Sedlmair et al.</a> (<a href="bibliography.html#ref-sedlmair_empirical_2013" role="doc-biblioref">2013</a>)</span> instead use two expert coders to evaluate 75 datasets and four dimension reduction techniques across 2D scatterplots, 2D scatterplot matrices, and interactive 3D scatterplots. They suggest a tiered guidance approach finding that 2D scatterplots are often sufficient to resolve a feature. If not, try an alternative dimension reduction technique before going to scatterplot matrix display or concluding a true negative. They find that interactive 3D scatterplots help in relatively rare cases. <span class="citation"><a href="bibliography.html#ref-lewis_behavioral_2012" role="doc-biblioref">Lewis et al.</a> (<a href="bibliography.html#ref-lewis_behavioral_2012" role="doc-biblioref">2012</a>)</span> compare across three cohorts: experts, uninformed novices, and informed novices (<span class="math inline">\(n=5+15+16=36\)</span>). Participants were asked their opinion of the quality of nine different embedding for each of several data sets. Expert opinion is reportedly more consistent than the novice groups, though this is confounded with the different sample sizes. Interestingly, cohort responses correlated with different quality metrics. Positive ratings from the expert group correlated strongest with the Trustworthiness metric.</p>
<!-- Nonlinear DR quality review -->
<p>Tours are absent from studies calculable quality measures. However, <span class="citation"><a href="bibliography.html#ref-nelson_xgobi_1999" role="doc-biblioref">Nelson et al.</a> (<a href="bibliography.html#ref-nelson_xgobi_1999" role="doc-biblioref">1998</a>)</span> compare scatterplots of grand tours on a 2D monitor with a 3D display (stereoscopic, not head-mounted) over <span class="math inline">\(n=15\)</span> participants. Participants perform clusters detection, dimensionality, and radial sparseness tasks on six-dimensional data. They find that stereoscopic 3D leads to more accuracy for cluster identification, though interaction time greatly increased in the 3D case.</p>
</div>
<div id="nonlinear-models" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Nonlinear models</h2>
<!-- Segue into models -->
<p>So far, the thesis has focused on exploratory data analysis. Another core part of data analysis is fitting <em>regression</em> models of quantitative data <span class="citation">(<a href="bibliography.html#ref-galton_regression_1886" role="doc-biblioref">Galton 1886</a>)</span> and <em>classification</em> models for discrete categories <span class="citation">(<a href="bibliography.html#ref-fisher_logic_1935" role="doc-biblioref">Fisher 1935</a>)</span>. There are different reasons and emphases when considering to fit a model. <span class="citation"><a href="bibliography.html#ref-breiman_statistical_2001" role="doc-biblioref">Breiman</a> (<a href="bibliography.html#ref-breiman_statistical_2001" role="doc-biblioref">2001b</a>)</span>, reiterated by <span class="citation"><a href="bibliography.html#ref-shmueli_explain_2010" role="doc-biblioref">Shmueli</a> (<a href="bibliography.html#ref-shmueli_explain_2010" role="doc-biblioref">2010</a>)</span>, taxonomize models based on their purpose. <em>Explanatory</em> modeling is done for some inferential purpose, while <em>predictive</em> modeling focuses more narrowly on the performance of some objective function. The intended use has important implications for model selection and development. In explanatory modeling, interpretability is vital for drawing inferential conclusions. Nonlinear models range from additive models with at least one polynomial term to more complex machine learning models such as random forest, support-vector machine, or neural network to name a few <span class="citation">(<a href="bibliography.html#ref-boser_training_1992" role="doc-biblioref">Boser et al. 1992</a>; <a href="bibliography.html#ref-anderson_introduction_1995" role="doc-biblioref">Anderson 1995</a>; <a href="bibliography.html#ref-breiman_random_2001" role="doc-biblioref">Breiman 2001a</a>)</span>.</p>
<!-- nonlinear leading to interpretability issues -->
<p>Nonlinear models have many or complex interactive terms, which cause an opaqueness to the interpretation of the variables. This difficulty in interpreting the terms in complex nonlinear models sometimes leads to them being referred to as black box models. Despite the potentially better performance of nonlinear models, their use is not without controversy <span class="citation">(<a href="bibliography.html#ref-oneil_weapons_2016" role="doc-biblioref">O’Neil 2016</a>; <a href="bibliography.html#ref-kodiyan_overview_2019" role="doc-biblioref">Kodiyan 2019</a>)</span>. And the loss of interpretation presents a challenge.</p>
<!-- Interpretability & baises -->
<p>Interpretability is vital for exploring and protecting against potential biases in any model (e.g. sex – <span class="citation"><a href="bibliography.html#ref-dastin_amazon_2018" role="doc-biblioref">Dastin</a> (<a href="bibliography.html#ref-dastin_amazon_2018" role="doc-biblioref">2018</a>)</span>; <span class="citation"><a href="bibliography.html#ref-duffy_apple_2019" role="doc-biblioref">Duffy</a> (<a href="bibliography.html#ref-duffy_apple_2019" role="doc-biblioref">2019</a>)</span>, race – <span class="citation"><a href="bibliography.html#ref-larson_how_2016" role="doc-biblioref">Larson et al.</a> (<a href="bibliography.html#ref-larson_how_2016" role="doc-biblioref">2016</a>)</span>, and age – <span class="citation"><a href="bibliography.html#ref-diaz_addressing_2018" role="doc-biblioref">Díaz et al.</a> (<a href="bibliography.html#ref-diaz_addressing_2018" role="doc-biblioref">2018</a>)</span>). For instance, models regularly pick up on biases in the training data where such classes correlate with changes in the response variable. This bias is then built into the model. Variable-level (feature-level) interpretability of models is essential in evaluating and addressing such biases.</p>
<!-- Interpretability & data drift -->
<p>Another concern is data drift, where a shift in the range of the explanatory variables (features or predictors). Some nonlinear models are sensitive to this and do not extrapolate well outside the support of the training data <span class="citation">(<a href="bibliography.html#ref-espadoto_toward_2021" role="doc-biblioref">Espadoto et al. 2021</a>)</span>. Maintaining variable interpretability is also essential to address issues arising from data drift.</p>
</div>
<div id="sec:explanations" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> Local explanations</h2>
<!-- Local explanations -->
<p>Explainable Artificial Intelligence (XAI) is an emerging field of research that aims to increase the interpretability of nonlinear models <span class="citation">(<a href="bibliography.html#ref-adadi_peeking_2018" role="doc-biblioref">Adadi and Berrada 2018</a>; <a href="bibliography.html#ref-arrieta_explainable_2020" role="doc-biblioref">Arrieta et al. 2020</a>)</span>. A common approach is <em>local explanations</em>, which attempt to approximate linear variable importance in the vicinity of one observation (instance). This is a linear measure indicating which variables are essential for distinguishing between the mean of the data and the prediction near one observation. Because these are point-specific, the challenge is to comprehensively visualize them to better understand a model.</p>
<!-- Reminder of local explanation -->
<p>Consider a highly nonlinear model. It can be hard to determine which variables in the data are sensitive to changes in the classification or sizable changes in a residual. Local explanations shed light on these situations by approximating linear variable importance in the vicinity of a single observation. Figure <a href="2-ch-background.html#fig:ch2fig6">2.6</a> motivates local explanations where the analyst wants to know the variable attribution for a particular observation close to the classification boundary in a nonlinear model.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch2fig6"></span>
<img src="figures/ch2_fig6_ribeiro16fig.PNG" alt="Illustration of a nonlinear classification model. An analyst may want to know the variable importance in the vicinity of the highlighted red cross. Understanding this attribution elucidates how the variables influence this point that is precariously close to the classification boundary. Local explanations approximate this linear attribution in the vicinity of one observation. Figure from Ribeiro et al. (2016)." width="50%"  />
<p class="caption">
Figure 2.6: Illustration of a nonlinear classification model. An analyst may want to know the variable importance in the vicinity of the highlighted red cross. Understanding this attribution elucidates how the variables influence this point that is precariously close to the classification boundary. Local explanations approximate this linear attribution in the vicinity of one observation. Figure from <span class="citation"><a href="bibliography.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro et al.</a> (<a href="bibliography.html#ref-ribeiro_why_2016" role="doc-biblioref">2016</a>)</span>.
</p>
</div>
<!-- Taxonomy of local explanations -->
<p>A comprehensive summary of the taxonomy and literature of explanation techniques is provided in Figure 6 of <span class="citation"><a href="bibliography.html#ref-arrieta_explainable_2020" role="doc-biblioref">Arrieta et al.</a> (<a href="bibliography.html#ref-arrieta_explainable_2020" role="doc-biblioref">2020</a>)</span>. It includes a large number of model-specific explanations such as deepLIFT <span class="citation">(<a href="bibliography.html#ref-shrikumar_not_2016" role="doc-biblioref">Shrikumar et al. 2016</a>, <a href="bibliography.html#ref-shrikumar_learning_2017" role="doc-biblioref">2017</a>)</span>, a popular recursive method for estimating importance in neural networks. There are fewer model-agnostic explanations, of which LIME, <span class="citation">(<a href="bibliography.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro et al. 2016</a>)</span> SHAP, <span class="citation">(<a href="bibliography.html#ref-lundberg_unified_2017" role="doc-biblioref">Lundberg and Lee 2017</a>)</span>, and their variants are popular.</p>
<!-- Uses of local explanations -->
<p>These observation-level explanations are used in various ways depending on the context. In image classification, a saliency map indicate necessary pixels for the resulting classification <span class="citation">(<a href="bibliography.html#ref-simonyan_deep_2014" role="doc-biblioref">Simonyan et al. 2014</a>)</span>. For example, snow may be highlighted when distinguishing if a picture contains a wolf or husky <span class="citation">(<a href="bibliography.html#ref-besse_can_2019" role="doc-biblioref">Besse et al. 2019</a>)</span>. In text analysis, word-level contextual sentiment analysis can be used to highlight the sentiment and magnitude of influential words <span class="citation">(<a href="bibliography.html#ref-vanni_textual_2018" role="doc-biblioref">Vanni et al. 2018</a>)</span>. In the case of numeric regression, they are used to explain variable additive contributions from the observed mean to the observation’s prediction <span class="citation">(<a href="bibliography.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro et al. 2016</a>)</span>.</p>
</div>
<div id="conclusion" class="section level2" number="2.7">
<h2><span class="header-section-number">2.7</span> Conclusion</h2>
<!-- Recap 1; motivation, orthogonal views and dimension reduction-->
<p>This chapter discussed the motivation for data visualization, which conveys more information than statistic summarization alone. User interaction is important for linking observation and auxiliary information between coordinated views and animation frames. Several visuals were discussed before turning to dimension reduction. Because of the opaqueness of the distortions and the added subjectivity from nonlinear dimension reduction, this thesis focused more on viewing many linear projections. In particular, over the changing basis in tours, and the user-control steering of the basis radial tour is of particular interest.</p>
<!-- Recap 2; evaluations and nonlinear models -->
<p>Metric and empirical evaluations of multivariate data were discussed. Nonlinear models, and XAI’s use of local explanations to extend the interpretability of models were also covered. There is an absence of studies comparing animated tours with alternative visualizations.</p>
<!-- Upcoming -->
<p>The following chapters respectively address the three research questions covered in the Introduction. Chapter <a href="3-ch-spinifex.html#ch-spinifex">3</a> discusses the implementation of a package that facilitates the creation of radial tours and extends the display and exporting of tours in general. Chapter <a href="4-ch-userstudy.html#ch-userstudy">4</a> covers the first user study evaluating the radial tour compared with two common alternatives. Chapter <a href="5-ch-cheem.html#ch-cheem">5</a> introduces a novel analysis that explores local explanations of nonlinear models with the radial tour.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="1-ch-introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-ch-spinifex.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
