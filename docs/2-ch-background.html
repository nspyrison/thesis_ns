<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Background | Interactive and dynamic visualization of high-dimensional data via animated linear projections</title>
  <meta name="description" content="Chapter 2 Background | Interactive and dynamic visualization of high-dimensional data via animated linear projections" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Background | Interactive and dynamic visualization of high-dimensional data via animated linear projections" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Background | Interactive and dynamic visualization of high-dimensional data via animated linear projections" />
  
  
  

<meta name="author" content="Nicholas S Spyrison" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="1-ch-introduction.html"/>
<link rel="next" href="3-ch-spinifex.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="template/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./"> </a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#research-questions"><i class="fa fa-check"></i><b>1.1</b> Research questions</a></li>
<li class="chapter" data-level="1.2" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#methodology"><i class="fa fa-check"></i><b>1.2</b> Methodology</a></li>
<li class="chapter" data-level="1.3" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#contributions"><i class="fa fa-check"></i><b>1.3</b> Contributions</a></li>
<li class="chapter" data-level="1.4" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#thesis-structure"><i class="fa fa-check"></i><b>1.4</b> Thesis structure</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-background.html"><a href="2-ch-background.html"><i class="fa fa-check"></i><b>2</b> Background</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2-ch-background.html"><a href="2-ch-background.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="2-ch-background.html"><a href="2-ch-background.html#multivariate-visualization"><i class="fa fa-check"></i><b>2.2</b> Multivariate visualization</a></li>
<li class="chapter" data-level="2.3" data-path="2-ch-background.html"><a href="2-ch-background.html#linear-projections"><i class="fa fa-check"></i><b>2.3</b> Linear projections</a></li>
<li class="chapter" data-level="2.4" data-path="2-ch-background.html"><a href="2-ch-background.html#evaluating-multivariate-data-visualization"><i class="fa fa-check"></i><b>2.4</b> Evaluating multivariate data visualization</a></li>
<li class="chapter" data-level="2.5" data-path="2-ch-background.html"><a href="2-ch-background.html#nonlinear-models"><i class="fa fa-check"></i><b>2.5</b> Nonlinear models</a></li>
<li class="chapter" data-level="2.6" data-path="2-ch-background.html"><a href="2-ch-background.html#sec:explanations"><i class="fa fa-check"></i><b>2.6</b> Local explanations</a></li>
<li class="chapter" data-level="2.7" data-path="2-ch-background.html"><a href="2-ch-background.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html"><i class="fa fa-check"></i><b>3</b> A User-Controlled Radial Tour for Animated Linear Projections</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:algorithm"><i class="fa fa-check"></i><b>3.1</b> Algorithm</a></li>
<li class="chapter" data-level="3.2" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#oblique-cursor-movement"><i class="fa fa-check"></i><b>3.2</b> Oblique cursor movement</a></li>
<li class="chapter" data-level="3.3" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:pkgstructure"><i class="fa fa-check"></i><b>3.3</b> Package structure</a></li>
<li class="chapter" data-level="3.4" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:usecases"><i class="fa fa-check"></i><b>3.4</b> Use cases</a></li>
<li class="chapter" data-level="3.5" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:discussion"><i class="fa fa-check"></i><b>3.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html"><i class="fa fa-check"></i><b>4</b> A Study on the Benefit of a User-Controlled Radial Tour for Variable Importance for Structure in High-Dimensional Data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:userstudy"><i class="fa fa-check"></i><b>4.1</b> User study</a></li>
<li class="chapter" data-level="4.2" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:results"><i class="fa fa-check"></i><b>4.2</b> Results</a></li>
<li class="chapter" data-level="4.3" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:conclusion"><i class="fa fa-check"></i><b>4.3</b> Conclusion</a></li>
<li class="chapter" data-level="4.4" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:spinifex"><i class="fa fa-check"></i><b>4.4</b> Accompanying tool: radial tour application</a></li>
<li class="chapter" data-level="4.5" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:appendix"><i class="fa fa-check"></i><b>4.5</b> Extended analysis</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html"><i class="fa fa-check"></i><b>5</b> Exploring Local Explanations of Nonlinear Models Using the Radial tour</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:shap"><i class="fa fa-check"></i><b>5.1</b> SHAP and tree SHAP local explanations</a></li>
<li class="chapter" data-level="5.2" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:cheemviwer"><i class="fa fa-check"></i><b>5.2</b> The cheem viewer</a></li>
<li class="chapter" data-level="5.3" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:casestudies"><i class="fa fa-check"></i><b>5.3</b> Case studies</a></li>
<li class="chapter" data-level="5.4" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:cheemdiscussion"><i class="fa fa-check"></i><b>5.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html"><i class="fa fa-check"></i><b>6</b> Conclusion</a>
<ul>
<li class="chapter" data-level="6.1" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html#contributions-1"><i class="fa fa-check"></i><b>6.1</b> Contributions</a></li>
<li class="chapter" data-level="6.2" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html#limitations"><i class="fa fa-check"></i><b>6.2</b> Limitations</a></li>
<li class="chapter" data-level="6.3" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html#future-work"><i class="fa fa-check"></i><b>6.3</b> Future work</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-ch-appendix.html"><a href="7-ch-appendix.html"><i class="fa fa-check"></i><b>7</b> Appendix</a>
<ul>
<li class="chapter" data-level="7.1" data-path="7-ch-appendix.html"><a href="7-ch-appendix.html#sec-glossary"><i class="fa fa-check"></i><b>7.1</b> Glossary</a></li>
<li class="chapter" data-level="7.2" data-path="7-ch-appendix.html"><a href="7-ch-appendix.html#animated-tour-links"><i class="fa fa-check"></i><b>7.2</b> Animated tour links</a></li>
<li class="chapter" data-level="7.3" data-path="7-ch-appendix.html"><a href="7-ch-appendix.html#supplementary-material"><i class="fa fa-check"></i><b>7.3</b> Supplementary material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><strong><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></strong></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interactive and dynamic visualization of high-dimensional data via animated linear projections</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-background" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Background</h1>
<!-- Structure -->
<p>In the last chapter, we discussed the importance of data visualization despite the complexity of viewing high-dimensional data and outlined the reseach questions we wish to address with the user-control of the radial tour. This chapter first motivates the use of data visualization in general and the importance of interaction. Then we cover common visualizations for multivariate data before turning to dimension reduction. We cover linear dimension reduction, including further discussion of tours. Then we discuss empirical evaluations of multivariate visuals. Lastly, this chapter concludes with nonlinear models and extending their interpretation with local explanations.</p>
<div id="motivation" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Motivation</h2>
<!-- Better than numerical summarization alone -->
<p>Visualization is much more robust than numerical summarization alone <span class="citation">(<a href="bibliography.html#ref-anscombe_graphs_1973" role="doc-biblioref">Anscombe 1973</a>; <a href="bibliography.html#ref-matejka_same_2017" role="doc-biblioref">Matejka and Fitzmaurice 2017</a>)</span>. Figure <a href="2-ch-background.html#fig:ch2fig1">2.1</a> illustrates this. The data sets have quite different structure that is revealed in the visualization but not in the summary statistics, as all data sets have the same means, standard deviations, and correlation.</p>
(ref:ch2fig1-cap) The datasaurus dozen is a modern data set illustrating Anscombe’s point that summary statistics cannot always adequately summarize the content of data. The data patterns shown in the scatterplots have the same summary statistics. Unless the data is plotted, one would never know that there were such radical differences.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch2fig1"></span>
<img src="figures/ch2_fig1_datasaurus.png" alt="(ref:ch2fig1-cap)" width="100%"  />
<p class="caption">
Figure 2.1: (ref:ch2fig1-cap)
</p>
</div>
<!-- Data interaction -->
<p>Interaction is known to be an essential aspect of modern data visualization <span class="citation">(<a href="bibliography.html#ref-batch_there_2019" role="doc-biblioref">Batch et al. 2019</a>; <a href="bibliography.html#ref-card_psychology_1983" role="doc-biblioref">Card, Moran, and Newell 1983</a>; <a href="bibliography.html#ref-marriott_immersive_2018" role="doc-biblioref">Marriott et al. 2018</a>)</span>. <span class="citation"><a href="bibliography.html#ref-munzner_visualization_2014" role="doc-biblioref">Munzner</a> (<a href="bibliography.html#ref-munzner_visualization_2014" role="doc-biblioref">2014</a>)</span> posits that responsive and fast computer graphics will allow us to move beyond paper and static resources. Interaction is key to navigate exhaustive views of sizable data and also link observations and features across. <!--The term interaction covers a broad range of uses, purposes, and contexts, making it overburdened and potentially ambiguous.--> Interaction facilitates accessing increasing amounts of data and amplifies cognition through control and input <span class="citation">(<a href="bibliography.html#ref-dimara_what_2019" role="doc-biblioref">Dimara and Perin 2019</a>)</span>.</p>
<!-- Multivariate data vis interaction -->
<p>Here we focus on the quantitative multivariate data interactions with coordinated view, linked brushing, and tooltip display. Coordinated and multiple views <span class="citation">(<a href="bibliography.html#ref-roberts_state_2007" role="doc-biblioref">Roberts 2007</a>)</span> <span class="citation">(also known as ensemble graphics, <a href="bibliography.html#ref-unwin_ensemble_2018" role="doc-biblioref">Unwin and Valero-Mora 2018</a>)</span> use several types of visuals which give a more comprehensive understanding than any one visual portrays in isolation. Linking observations between different views and animation frames is facilitated by linked brushing <span class="citation">(<a href="bibliography.html#ref-becker_brushing_1987" role="doc-biblioref">Becker and Cleveland 1987</a>)</span>.
Linked brushing colors selected observations in one view allowing these selections to be tracked and correlated across other views and frames. Linked brushing has proved to be helpful in animated tours <span class="citation">(<a href="bibliography.html#ref-arms_benefits_1999" role="doc-biblioref">Arms, Cook, and Cruz-Neira 1999</a>; <a href="bibliography.html#ref-laa_slice_2020" role="doc-biblioref">Laa, Cook, and Valencia 2020</a>; <a href="bibliography.html#ref-lee_casting_2020" role="doc-biblioref">Lee, Laa, and Cook 2020</a>)</span>. Tooltips display observation ID upon the cursor hovering over observations can also aid identification and the display associated values. Interactive selection of parameters extend the breath of an analysis and animation interactions such as play, pause, and frame selection are regularly used.</p>
</div>
<div id="multivariate-visualization" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Multivariate visualization</h2>
<!-- Data scope and context -->
<p>In this thesis we are concerned with visualization of multivariate data. Specifically we are interested in quantitative multivariate data. We assume that our data consists of <span class="math inline">\(n\)</span> observations of <span class="math inline">\(p\)</span> variables. Generally <span class="math inline">\(n&gt;p\)</span> with many more observations than variables. While written as though always operating on the original variables, the visualizations discussed below could also be applied to reduced component spaces (such as PCA approximation in few components) or feature decomposition of data not fitting this format. <span class="citation"><a href="bibliography.html#ref-kang_visualising_2017" role="doc-biblioref">Kang, Hyndman, and Smith-Miles</a> (<a href="bibliography.html#ref-kang_visualising_2017" role="doc-biblioref">2017</a>)</span> is a good example of decomposing time series data in a quantitative feature matrix.</p>
<!-- Penguins data -->
<p><span class="citation"><a href="bibliography.html#ref-grinstein_high-dimensional_2002" role="doc-biblioref">Grinstein, Trutschl, and Cvek</a> (<a href="bibliography.html#ref-grinstein_high-dimensional_2002" role="doc-biblioref">2002</a>)</span> illustrates many multivariate visualization methods. In particular it shows examples of the actual visualizations. <span class="citation"><a href="bibliography.html#ref-liu_visualizing_2017" role="doc-biblioref">Liu et al.</a> (<a href="bibliography.html#ref-liu_visualizing_2017" role="doc-biblioref">2017</a>)</span> give a good classification and taxonomy. Here we focus on the most common and the most relevant visuals. We illustrate these with the Palmer penguins data which we meet in the Introduction. This data contains 333 observations of three penguin species across four physical measurements: bill length, bill depth, flipper length, and body mass. Observations were collected between 2007 and 2009 near Palmer Station, Antarctica. This is a good substitute for the over-used iris data.</p>
<div id="scatterplot-matrices" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Scatterplot matrices</h3>
<!-- SPLOM -->
<p>An analyst could look at <span class="math inline">\(p\)</span>-univariate histograms or density curves. Extending this idea, pairs of variable can be exhaustively viewed. Combining these brings us to scatterplot matrices, also known as SPLOM <span class="citation">(<a href="bibliography.html#ref-chambers_graphical_1983" role="doc-biblioref">Chambers et al. 1983</a>)</span>. In a scatterplot matrix, variables are arranged across the columns and rows. The diagonal elements show univariate densities, while off-diagonal positions show scatterplot pairs, as in Figure <a href="2-ch-background.html#fig:penguinsplom">2.2</a>. This is useful for getting a handle on the range of the variables but is not going to scale well when the number of variables <span class="math inline">\(p\)</span> is large. Such visualizaiton will only partially resolve features that could be better show information from more dimensions.</p>
(ref:penguinsplom-cap) Scatterplot matrix views univariate densities and all pairs of bivariate scatterplots. The panels show partial cluster separation indicating these variables contain some cluster discerning information. This approach is good for quickly exploring the range of the data but will not reveal features in more than two dimensions. It is a good exploratory visual but does not scale well with the number of variables.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:penguinsplom"></span>
<img src="figures/ch2_fig2_penguin_splom.png" alt="(ref:penguinsplom-cap)" width="100%"  />
<p class="caption">
Figure 2.2: (ref:penguinsplom-cap)
</p>
</div>
<!-- scaling with n -->
<p>As <span class="math inline">\(n\)</span> increases, scatterplot displays also suffer from occlusion as the points overlay each other. This is typically addressed in a few ways. One method is to decrease points’ opacity, allowing more layers to be seen. Another approach is to change the geometric display to 2D density contours or an aggregated heatmap (illustrated in Figure <a href="2-ch-background.html#fig:ch2fig5">2.5</a>). These aggregations typically render faster and scale better with increasing observations. Or, if needed, visualization can be performed on a representative subset of the data.</p>
</div>
<div id="parallel-coordinate-plots" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Parallel coordinate plots</h3>
<!-- PCP -->
<p>In scatterplot matrices, each observation is spread across all panels. In contrast observation-linked visuals have a single line or glyph for each observation. In parallel coordinate plots <span class="citation">(<a href="bibliography.html#ref-ocagne_coordonnees_1885" role="doc-biblioref">Ocagne 1885</a>)</span>, variables are arranged horizontally, and lines connect observations after being transformed to a common scale such as quantiles or z-value (standard deviations away from the mean). Figure <a href="2-ch-background.html#fig:penguinpcp">2.3</a> illustrates this method. This scales much better with dimensions but poorly with observations. It also suffers from an asymmetry with the variable order. That is, changing the order of the variables will lead to very different conclusions. The x-axis is also used to display variables rather than the values of the observations. This restricts the amount of information that can be interpreted between variables. <span class="citation"><a href="bibliography.html#ref-munzner_visualization_2014" role="doc-biblioref">Munzner</a> (<a href="bibliography.html#ref-munzner_visualization_2014" role="doc-biblioref">2014</a>)</span> asserts that position is the more human-perceptible channel for encoding information; we should prefer to reserve it for the values of the observations rather than distinguishing variables.</p>
(ref:penguinpcp-cap) Parallel coordinate plots put variables on a common scale and position them side-by-side with lines connecting observations. Some variation between the clusters can be seen corroborating their importance to explaining cluster separation. This approach scales relatively well with the number of variables but poorly with the number of observations. It is more difficult to perceive summary statistics (correlation, for example) and suffers from asymmetry with the order of variables. Changing the order of the variables will likely result in different interpretations. Further, the perception channel of position is used for distinguishing between variables rather than values within the variable.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:penguinpcp"></span>
<img src="figures/ch2_fig3_penguin_pcp.png" alt="(ref:penguinpcp-cap)" width="100%"  />
<p class="caption">
Figure 2.3: (ref:penguinpcp-cap)
</p>
</div>
<!-- Observation based visuals and scaling -->
<p>The same issues persist across other displays that link observations into <span class="math inline">\(n\)</span> glyph or heatmaps. Examples of these include star plots <span class="citation">(<a href="bibliography.html#ref-chambers_graphical_1983" role="doc-biblioref">Chambers et al. 1983</a>)</span>, pixel-based visuals <span class="citation">(<a href="bibliography.html#ref-keim_designing_2000" role="doc-biblioref">Keim 2000</a>)</span>, and Chernoff faces <span class="citation">(<a href="bibliography.html#ref-chernoff_use_1973" role="doc-biblioref">Chernoff 1973</a>)</span>. Parallel coordinate plots and these other visuals scale quite poorly observations. Because this visuals scale well with variables they maybe candidate visualizations for low <span class="math inline">\(n\)</span>, high <span class="math inline">\(p\)</span> data.</p>
</div>
<div id="dimension-reduction" class="section level3" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Dimension reduction</h3>
<!-- Dimension reduction, linear and nonlinear -->
<p>Ultimately, we turn to dimension reduction to visualize multivariate spaces. This involves a function to map <span class="math inline">\(p\)</span>-spaces onto a lower <span class="math inline">\(d\)</span>-dimensional space. Dimension reduction is separated into two categories, linear and nonlinear. The linear case spans all affine mathematical transformations, essentially any function where parallel lines stay parallel. Nonlinear transformations complement the linear case, think transformations containing exponents or interacting terms. Examples in low dimensions are relatable. For instance, shadows are linear projections of a 3-dimensional object down to the 2D shadow. Linear perspective drawings are another instance. An example of a nonlinear transformation is that of 2D maps of the globe. The most common maybe the Mercator projection, a rectangular display where the area is proportionally distorted with the distance away from the equator <span class="citation">(<a href="bibliography.html#ref-snyder_map_1987" role="doc-biblioref">Snyder 1987</a>)</span>. Other distortions are created when the surface is unwrapped into an elongated ellipse. Yet others create non-continuous gaps on land or oceans to minimize the distortion of targeted areas. <span class="citation"><a href="bibliography.html#ref-wikipedia_list_2021" role="doc-biblioref">wikipedia</a> (<a href="bibliography.html#ref-wikipedia_list_2021" role="doc-biblioref">2021</a>)</span> lists over 75 different projections to distort the surface to display as a map, each with unique properties distorting the 3D surface.</p>
<!-- Nonlinear hyperparameters -->
<p>Nonlinear techniques often have hyperparameters (that are absent from linear methods). Hyperparameters are inputs that affect how the spaces are distorted to fit into fewer dimensions. Various computational quality metrics, such as Trustworthiness, Continuity, Normalized stress, and Average local error, describe the distortion of the space <span class="citation">(<a href="bibliography.html#ref-espadoto_toward_2021" role="doc-biblioref">Espadoto et al. 2021</a>; <a href="bibliography.html#ref-gracia_new_2016" role="doc-biblioref">Gracia et al. 2016</a>; <a href="bibliography.html#ref-van_der_maaten_dimensionality_2009" role="doc-biblioref">Maaten, Postma, and Van den Herik 2009</a>; <a href="bibliography.html#ref-venna_visualizing_2006" role="doc-biblioref">Venna and Kaski 2006</a>)</span>. To quote <span class="citation"><a href="bibliography.html#ref-panagiotelis_manifold_2020" role="doc-biblioref">Panagiotelis</a> (<a href="bibliography.html#ref-panagiotelis_manifold_2020" role="doc-biblioref">2020</a>)</span>, “All nonlinear projections are wrong, but some are useful,” a play on George Box’s quote about models <span class="citation">(<span>“All <em>models</em> are wrong, but some are useful,”</span> <a href="bibliography.html#ref-box_science_1976" role="doc-biblioref">Box 1976</a>)</span>. The distortions are hard to interpret and, thus, hard to communicate.</p>
<!-- Dealing with hyper parameters & Sticking with linear approaches -->
<p>Opinions differ on how to best deal with hyperparameters. <span class="citation"><a href="bibliography.html#ref-probst_hyperparameters_2019" role="doc-biblioref">Probst, Wright, and Boulesteix</a> (<a href="bibliography.html#ref-probst_hyperparameters_2019" role="doc-biblioref">2019</a>)</span> discusses tuning strategies. Others compare implementation defaults <span class="citation">(<a href="bibliography.html#ref-gijsbers_meta_2021" role="doc-biblioref">Gijsbers et al. 2021</a>; <a href="bibliography.html#ref-pfisterer_learning_2021" role="doc-biblioref">Pfisterer et al. 2021</a>)</span>. <span class="citation"><a href="bibliography.html#ref-probst_tunability_2019" role="doc-biblioref">Probst, Boulesteix, and Bischl</a> (<a href="bibliography.html#ref-probst_tunability_2019" role="doc-biblioref">2019</a>)</span> look at the sensitivity of hyperparmeters to the performance of a model. Automated machine learning takes a programmatic approach to hyperparameter tuning <span class="citation">(<a href="bibliography.html#ref-feurer_efficient_2015" role="doc-biblioref">Feurer et al. 2015</a>; <a href="bibliography.html#ref-hutter_automated_2019" role="doc-biblioref">Hutter, Kotthoff, and Vanschoren 2019</a>; <a href="bibliography.html#ref-yao_taking_2019" role="doc-biblioref">Yao et al. 2019</a>)</span>. Due to the difficulty of interpreting and communicating nonlinear mappings and the added subjectivity of hyperparameter selection, this thesis focuses on linear visualization techniques.
<!-- Moreover, nonlinear dimension reduction can introduce features not in the data depending on the selection of hyperparameters. Then plausibly, the presence of structure in a nonlinear model is necessary but not sufficient to conclude the existence of a structure in the data. --></p>
<!-- >> _XXX: Is this the best spot for free lunch comment? IDD builds off of this but feels like it belongs at the end of this supersection._ -->
<!-- No free lunch trade off -->
<p>Unfortunately, there is no free lunch here. An increase in the original data dimensions will lead to exponentially larger viewing space in the linear case or an exponentially perturbed and distorted space in nonlinear techniques. Neither is a panacea for visualizing multivariate spaces.</p>
</div>
<div id="intrinsic-data-dimensionality" class="section level3" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Intrinsic data dimensionality</h3>
<!-- Intrinsic data dimensionality -->
<p>The intrinsic dimensionality of data is the number of variables needed to minimally represent the data <span class="citation">(<a href="bibliography.html#ref-grinstein_high-dimensional_2002" role="doc-biblioref">Grinstein, Trutschl, and Cvek 2002</a>)</span>. Intrinsic data dimensionality is an essential consideration of dimension reduction to mitigate runtime of models and viewing time of visualization. Consider a Psychology survey consisting of 100 questions about the Big Five personality traits. The data consists of 100 response variables, while the theory would suggest the intrinsic dimensionality is five. Reducing the disparity of these volumes would be necessary to gate the exponentially increasing space to view. Nonlinear techniques regularly perform and implicit PCA initialization steps that can default to keeping several dozen components to mitigate run time of data with hundreds or thousands of variables.</p>
</div>
</div>
<div id="linear-projections" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Linear projections</h2>
<p>Linear projections map a higher <span class="math inline">\(p\)</span>-dimensional space onto a smaller <span class="math inline">\(d\)</span>-space with an affine mapping (where parallel lines stay parallel). A projection is the resulting space of the data multiplied by a basis <span class="math inline">\(Y_{n \times d} = X_{n \times p} \cdot A_{p \times d}\)</span>. This basis is an orthonormal matrix that explains the orientation and magnitudes that variable contribute to the resulting space. This basis is often illustrated as a biplot <span class="citation">(<a href="bibliography.html#ref-gabriel_biplot_1971" role="doc-biblioref">Gabriel 1971</a>)</span>, where variable contributions are inscribed in a unit circle showing the direction and the magnitude of contribution as illustrated in the previous chapter.</p>
<!-- PCA -->
<p>A common linear projection is principal component analysis <span class="citation">(PCA, <a href="bibliography.html#ref-pearson_liii._1901" role="doc-biblioref">Pearson 1901</a>)</span>, which creates a component space ordered by descending variation. It uses eigenvalue decomposition to identify the basis. These components are typically viewed as discrete orthogonal pairs, commonly approximated as a several components. The exact number of components to keep is subjective but typically guided by the use of a screeplot <span class="citation">(<a href="bibliography.html#ref-cattell_scree_1966" role="doc-biblioref">Cattell 1966</a>)</span>. Scree plots illustrate the decreasing variation contained in subsequent components. The analyst then identifies an elbow in this plot. PCA is also commonly used in preprocessing and model initialization when there are many more variables than the intrinsic data dimensionality, such as the Big Five personality example. These reduced component spaces can be used in visualization, albeit with the added abstraction of another linear mapping to get back to the original variables.</p>
<div id="sec:tour" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Tours, animated linear projections</h3>
<!-- Segue from barstool -->
<p>In one static linear projection, one basis uniquely maps the projection. In contrast to a single projection, a data visualization tour animates many such projections over small changes in the basis. In the shadow analogy, structural features of an object is gained by watching its shadow change due to its rotation. An analyst similarly gains information about the data object by watching continuous changes to the basis (the orientation of the data). There are various types of tours that are classified by the generation of their basis paths. We enumerate a few related to this work. A more comprehensive discussion and review of tours can be found in the works of <span class="citation"><a href="bibliography.html#ref-cook_grand_2008" role="doc-biblioref">Cook et al.</a> (<a href="bibliography.html#ref-cook_grand_2008" role="doc-biblioref">2008</a>)</span> and <span class="citation"><a href="bibliography.html#ref-lee_state_2021" role="doc-biblioref">Lee et al.</a> (<a href="bibliography.html#ref-lee_state_2021" role="doc-biblioref">2021</a>)</span>.</p>
<!-- Interpolation frames -->
<p>Regardless of the type of tour target basis identified, tours must interpolate frames between distant generated target bases. This interpolation is performed along a geodesic path between the bases. Geodesic refers to the shortest path (on a <span class="math inline">\(p\)</span>-sphere of possible bases). This ends up being slightly curved in 2D representation in the same way that flight paths appear curved on 2D representations of the globe. The interpolated frames at small enough angles is essential for the trackability of observations between frames.</p>
<!-- Grand tour -->
<p>Originally in a <em>grand</em> tour <span class="citation">(<a href="bibliography.html#ref-asimov_grand_1985" role="doc-biblioref">Asimov 1985</a>)</span>, several target frames are randomly selected. Figure <a href="2-ch-background.html#fig:ch2fig4">2.4</a> illustrates six frames from a grand tour. The grand tour is good for EDA in that it will show frames with widely varying contributions but lacks a destination, objective function, or means of steering.</p>
(ref:ch2fig4-cap) Frames from a grand tour. Biplots (grey circles) illustrate the direction and magnitude that variables contribute. In the grand tour, target bases are selected randomly. Tours animate linear projections over small changes in the basis. The observation permanence between frames is an essential distinction in tours. An animation can be viewed at <a href="https://vimeo.com/676723441">vimeo.com/676723441</a>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch2fig4"></span>
<img src="figures/ch2_fig4_penguin_grandtour.png" alt="(ref:ch2fig4-cap)" width="100%"  />
<p class="caption">
Figure 2.4: (ref:ch2fig4-cap)
</p>
</div>
<!-- manual tour -->
<p>The grand tour has no input over the bases selected. In contrast, the <em>manual tour</em> <span class="citation">(<a href="bibliography.html#ref-cook_manual_1997" role="doc-biblioref">Cook and Buja 1997</a>)</span> allows the analyst to change the contribution of a selected variable. It does so by initializing a manipulation dimension on a 1- or 2D projection which can then be rotated to alter the contribution of a selected variable. This work focuses on the <em>radial tour</em> sub-variant, where the angle of contribution is fixed and the magnitude of contribution along the radius is controlled. Figure <a href="2-ch-background.html#fig:ch2fig5">2.5</a> illustrates the same radial tour shown in the previous chapter across three geometric displays. The use of density contours and aggregated heatmap displays is useful to mitigate the occlusion of dense observations and is compatible with any scatterplot display.</p>
(ref:ch2fig5-cap) Radial tour across three geometric displays. The contribution of <code>bl</code> is removed from the frame and with it the separation between the orange and green clusters is also removed. Changing to density contours or hexagonal heatmap display are common ways to mitigate occlusion caused by dense observations. Heatmap display may not be the best choice to show supervised cluster separation, but can be useful to see structure in dense data.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch2fig5"></span>
<img src="figures/ch2_fig5_penguin_manualtour_geoms.png" alt="(ref:ch2fig5-cap)" width="100%"  />
<p class="caption">
Figure 2.5: (ref:ch2fig5-cap)
</p>
</div>
</div>
</div>
<div id="evaluating-multivariate-data-visualization" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Evaluating multivariate data visualization</h2>
<!-- Examples, surveys, and evaluations -->
<p><span class="citation"><a href="bibliography.html#ref-grinstein_high-dimensional_2002" role="doc-biblioref">Grinstein, Trutschl, and Cvek</a> (<a href="bibliography.html#ref-grinstein_high-dimensional_2002" role="doc-biblioref">2002</a>)</span> provide collected illustrations from many visualization methods. While <span class="citation"><a href="bibliography.html#ref-liu_visualizing_2017" role="doc-biblioref">Liu et al.</a> (<a href="bibliography.html#ref-liu_visualizing_2017" role="doc-biblioref">2017</a>)</span> provide a good classification of visualization techniques. Definitions and survey of quality metrics are given in <span class="citation"><a href="bibliography.html#ref-bertini_quality_2011" role="doc-biblioref">Bertini, Tatu, and Keim</a> (<a href="bibliography.html#ref-bertini_quality_2011" role="doc-biblioref">2011</a>)</span>. The latest, most comprehensive empirical evaluations are dicussed in <span class="citation"><a href="bibliography.html#ref-espadoto_toward_2021" role="doc-biblioref">Espadoto et al.</a> (<a href="bibliography.html#ref-espadoto_toward_2021" role="doc-biblioref">2021</a>)</span> and <span class="citation"><a href="bibliography.html#ref-nonato_multidimensional_2018" role="doc-biblioref">Nonato and Aupetit</a> (<a href="bibliography.html#ref-nonato_multidimensional_2018" role="doc-biblioref">2018</a>)</span>. While the former paper include discussion of tours, they are absent from empirical evaluations.</p>
<!-- 2D vs D3, mostly PCA reduced -->
<p><span class="citation"><a href="bibliography.html#ref-gracia_new_2016" role="doc-biblioref">Gracia et al.</a> (<a href="bibliography.html#ref-gracia_new_2016" role="doc-biblioref">2016</a>)</span> conducted an <span class="math inline">\(n=40\)</span> user study comparing between 2D and 3D scatterplots on traditional 2D monitors. Participants perform point classification, distance perception, and outlier identification tasks. The results are mixed and primarily have small differences. There is some evidence to suggest a lower error in distance perception from 3D scatterplot. <span class="citation"><a href="bibliography.html#ref-wagner_filho_immersive_2018" role="doc-biblioref">Wagner Filho et al.</a> (<a href="bibliography.html#ref-wagner_filho_immersive_2018" role="doc-biblioref">2018</a>)</span> performed an <span class="math inline">\(n=30\)</span> within participants using scatterplot display between 2D, 3D displays on monitors and 3D display with a head-mounted display on PCA reduced spaces. None of the tasks on any dataset lead to a significant difference in accuracy. However, the immersive display reduced effort and ammount of navigation while resulting in higher perceived accuracy and engagement. <span class="citation"><a href="bibliography.html#ref-sedlmair_empirical_2013" role="doc-biblioref">Sedlmair, Munzner, and Tory</a> (<a href="bibliography.html#ref-sedlmair_empirical_2013" role="doc-biblioref">2013</a>)</span> instead uses two expert coders to evaluate 75 datasets and four dimension reduction techniques for 2D scatterplots, interactive 3D scatterplots, and 2D scatterplot matrices. They suggest a tiered guidance approach finding that 2D scatterplots are often “good enough” to resolve a feature. If not, try with a alternative dimension reduction technique before going to scatterplot matrix display or concluding a true negative. They find that interactive 3D scatterplots help in relatively rare cases.</p>
<!-- Nonlinear DR quality review -->
<p>Tours are absent from emipical studies. However, <span class="citation"><a href="bibliography.html#ref-nelson_xgobi_1999" role="doc-biblioref">Nelson, Cook, and Cruz-Neira</a> (<a href="bibliography.html#ref-nelson_xgobi_1999" role="doc-biblioref">1998</a>)</span> compare scatterplots of grand tours on a 2D monitor with 3D (stereoscopic, not head-mounted) over <span class="math inline">\(n=15\)</span> participants. Participants perform clusters detection, dimensionality, and radial sparseness tasks on six-dimensional data. They find that stereoscopic 3D leads to more accuracy for the cluster identification, though interaction time much increased in the 3D case. In Chapter <a href="4-ch-userstudy.html#ch-userstudy">4</a>, we extend the evaluation of tours which novelly compares the radial tour as benchmarked against the grand tour and discrete pairs of principal components.</p>
</div>
<div id="nonlinear-models" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Nonlinear models</h2>
<p>Nonlinear models have many or complex interactive terms which causes an opacity to the interpretation of the variables. This loss of the interpretability of variables in nonlinear models sometimes lead to them being refereed to as black-box models.</p>
<!-- Introduce explanatory vs predictive modeling -->
<p>There are different reasons and emphases when considering to fit a model. <span class="citation"><a href="bibliography.html#ref-breiman_statistical_2001" role="doc-biblioref">Breiman</a> (<a href="bibliography.html#ref-breiman_statistical_2001" role="doc-biblioref">2001</a>)</span>, reiterated by <span class="citation"><a href="bibliography.html#ref-shmueli_explain_2010" role="doc-biblioref">Shmueli</a> (<a href="bibliography.html#ref-shmueli_explain_2010" role="doc-biblioref">2010</a>)</span>, taxonomize models based on their purpose; <em>explanatory</em> modeling is done for some inferential purpose, while <em>predictive</em> modeling focuses more narrowly on the performance of some objective function. The intended use has important implications for model selection and development. In explanatory modeling, interpretability is vital for drawing inferential conclusions. While black-box models are prefered in predictive modeling. However, the use and prevalence of nonlinear models is not without controversy <span class="citation">(<a href="bibliography.html#ref-oneil_weapons_2016" role="doc-biblioref">O’Neil 2016</a>; <a href="bibliography.html#ref-kodiyan_overview_2019" role="doc-biblioref">Kodiyan 2019</a>)</span>. And the loss of interpretation presents a challenge.</p>
<!-- Interpretability & baises -->
<p>Interpretability is vital for exploring and protecting against potential biases (e.g., sex – <span class="citation"><a href="bibliography.html#ref-dastin_amazon_2018" role="doc-biblioref">Dastin</a> (<a href="bibliography.html#ref-dastin_amazon_2018" role="doc-biblioref">2018</a>)</span>; <span class="citation"><a href="bibliography.html#ref-duffy_apple_2019" role="doc-biblioref">Duffy</a> (<a href="bibliography.html#ref-duffy_apple_2019" role="doc-biblioref">2019</a>)</span>, race – <span class="citation"><a href="bibliography.html#ref-larson_how_2016" role="doc-biblioref">Larson et al.</a> (<a href="bibliography.html#ref-larson_how_2016" role="doc-biblioref">2016</a>)</span>, and age – <span class="citation"><a href="bibliography.html#ref-diaz_addressing_2018" role="doc-biblioref">Díaz et al.</a> (<a href="bibliography.html#ref-diaz_addressing_2018" role="doc-biblioref">2018</a>)</span>) in any model. For instance, models regularly pick up on biases in the training data where protected classes correlate with the response feature. This bias is then built into the model. Variable-level (feature-level) interpretability of models is essential in evaluating such biases.<!-- Even if the optimization of an objective function is the primary concern, stakeholders, litigators, and opponents would rightly ask for the variable interpretability. --></p>
<!-- Interpretability & data drift -->
<p>Another concern is data drift, where a shift in the range of the explanatory variables (features or predictors). Some nonlinear models are sensitive to this and do not extrapolate well outside the support of the training data. Maintaining variable interpretability is also essential to address issues arising from data drift.</p>
</div>
<div id="sec:explanations" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> Local explanations</h2>
<!-- Local explanations -->
<p>Explainable Artificial Intelligence (XAI) is an emerging field of research that tries to increase the interpretability of black-box models. A common approach is <em>local explanations</em>, which attempt to approximate linear variable importance in the vicinity of one observation (instance). This a linear measure indicating which variables are important for distinguishing between the mean of the data and the prediction near one observation. Because these are point-specific, the challenge is to visualize them to understand a model comprehensively.</p>
<!-- Reminder of local explanation -->
<p>Consider a highly nonlinear model. It can be hard to determine which variables in the data will lead to a different classification or changes to in its residual. Local explanations shed light on these situations by approximating linear variable’s importance in the vicinity of a single observation. Figure <a href="2-ch-background.html#fig:ch2fig6">2.6</a> motivates local explanations where the analyst wants to know the variable attribution for a particular observation close to the classification boundary in a nonlinear model.</p>
(ref:ch2fig6-cap) Illustration of a nonlinear classification model. An analyst may want to know the variable importance at the vicinity of the highlighted red cross. Understanding this attribution elucidates how the variable influence this point that is precariously close to the classification boundary. Local explanations approximate this linear attribution in the vicinity of one observation. Figure from <span class="citation"><a href="bibliography.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro, Singh, and Guestrin</a> (<a href="bibliography.html#ref-ribeiro_why_2016" role="doc-biblioref">2016</a>)</span>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch2fig6"></span>
<img src="figures/ch2_fig6_ribeiro16fig.PNG" alt="(ref:ch2fig6-cap)" width="50%"  />
<p class="caption">
Figure 2.6: (ref:ch2fig6-cap)
</p>
</div>
<!-- Taxonomy of local explanations -->
<p>A comprehensive summary of the taxonomy and literature of explanation techniques is provided in Figure 6 of <span class="citation"><a href="bibliography.html#ref-arrieta_explainable_2020" role="doc-biblioref">Arrieta et al.</a> (<a href="bibliography.html#ref-arrieta_explainable_2020" role="doc-biblioref">2020</a>)</span>. It includes a large number of model-specific explanations such as deepLIFT <span class="citation">(<a href="bibliography.html#ref-shrikumar_not_2016" role="doc-biblioref">Shrikumar et al. 2016</a>; <a href="bibliography.html#ref-shrikumar_learning_2017" role="doc-biblioref">Shrikumar, Greenside, and Kundaje 2017</a>)</span>, a popular recursive method for estimating importance in neural networks. There are fewer model-agnostic explanations, of which LIME, <span class="citation">(<a href="bibliography.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>)</span> SHAP, <span class="citation">(<a href="bibliography.html#ref-lundberg_unified_2017" role="doc-biblioref">Lundberg and Lee 2017</a>)</span>, and their variants are popular.</p>
<!-- Uses of local explanations -->
<p>These observation-level explanations are used in various ways depending on the context. In image classification, a saliency map indicate important pixels for the resulting classification <span class="citation">(<a href="bibliography.html#ref-simonyan_deep_2014" role="doc-biblioref">Simonyan, Vedaldi, and Zisserman 2014</a>)</span>. For example, snow is regularly highlighted when distinguishing if a picture contains a wolf or husky <span class="citation">(<a href="bibliography.html#ref-besse_can_2019" role="doc-biblioref">Besse et al. 2019</a>)</span>. In text analysis, word-level contextual sentiment analysis can be used to highlight the sentiment and magnitude of influential words <span class="citation">(<a href="bibliography.html#ref-vanni_textual_2018" role="doc-biblioref">Vanni et al. 2018</a>)</span>. In the case of numeric regression, they are used to explain variable additive contributions from the observed mean to the observation’s prediction <span class="citation">(<a href="bibliography.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>)</span>.</p>
</div>
<div id="conclusion" class="section level2" number="2.7">
<h2><span class="header-section-number">2.7</span> Conclusion</h2>
<!-- Circle back around to higher level -->
<!-- Research gap; absence of tours, linear projection comparisons,  -->
<!-- Reference to further chapters -->
<!-- Motivation -->
<p>We have discussed the motivation fordata visualization and the importance of user interaction. We discussed several visuals before turning to linear dimension reduction. Empirical evaluations, black-box models, and their local explanations were discussed. There is an absence of studies comparing animated tours with alternative visualization. XAI and extending the interpretability of black-box models are sought-after.</p>
<p>The following chapters respectively, address the three research questions covered in the Introductiopn. Chapter <a href="3-ch-spinifex.html#ch-spinifex">3</a> discusses the implementation of a package that facilitates the creation radial tours and extends the display and exporting of tours in general. Chapter <a href="4-ch-userstudy.html#ch-userstudy">4</a> covers the first user-study evaluating the radial tour compared with two common alternatives. Chapter <a href="5-ch-cheem.html#ch-cheem">5</a>, introduces a novel analysis that explores local explanations of nonlinear models with the radial tour.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="1-ch-introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-ch-spinifex.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
