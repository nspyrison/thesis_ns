<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Background | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models</title>
  <meta name="description" content="Chapter 2 Background | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Background | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Background | Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models" />
  
  
  

<meta name="author" content="Nicholas S Spyrison" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="1-ch-introduction.html"/>
<link rel="next" href="3-ch-spinifex.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="template/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./"> </a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i>Abstract</a>
<ul>
<li class="chapter" data-level="" data-path="abstract.html"><a href="abstract.html#publications-and-papers-during-candidature-not-part-of-this-thesis"><i class="fa fa-check"></i>Publications and papers during candidature not part of this thesis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#research-questions"><i class="fa fa-check"></i><b>1.1</b> Research questions</a></li>
<li class="chapter" data-level="1.2" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#methodology"><i class="fa fa-check"></i><b>1.2</b> Methodology</a></li>
<li class="chapter" data-level="1.3" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#contributions"><i class="fa fa-check"></i><b>1.3</b> Contributions</a></li>
<li class="chapter" data-level="1.4" data-path="1-ch-introduction.html"><a href="1-ch-introduction.html#thesis-structure"><i class="fa fa-check"></i><b>1.4</b> Thesis structure</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-background.html"><a href="2-ch-background.html"><i class="fa fa-check"></i><b>2</b> Background</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2-ch-background.html"><a href="2-ch-background.html#scatterplot-matrices"><i class="fa fa-check"></i><b>2.1</b> Scatterplot matrices</a></li>
<li class="chapter" data-level="2.2" data-path="2-ch-background.html"><a href="2-ch-background.html#parallel-coordinate-plots"><i class="fa fa-check"></i><b>2.2</b> Parallel coordinate plots</a></li>
<li class="chapter" data-level="2.3" data-path="2-ch-background.html"><a href="2-ch-background.html#dimension-reduction"><i class="fa fa-check"></i><b>2.3</b> Dimension reduction</a></li>
<li class="chapter" data-level="2.4" data-path="2-ch-background.html"><a href="2-ch-background.html#tours-animated-linear-projections"><i class="fa fa-check"></i><b>2.4</b> Tours, animated linear projections</a></li>
<li class="chapter" data-level="2.5" data-path="2-ch-background.html"><a href="2-ch-background.html#evaluating-multivariate-data-visualization"><i class="fa fa-check"></i><b>2.5</b> Evaluating multivariate data visualization</a></li>
<li class="chapter" data-level="2.6" data-path="2-ch-background.html"><a href="2-ch-background.html#non-linear-models"><i class="fa fa-check"></i><b>2.6</b> Non-linear models</a></li>
<li class="chapter" data-level="2.7" data-path="2-ch-background.html"><a href="2-ch-background.html#sec:explanations"><i class="fa fa-check"></i><b>2.7</b> Local explanations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html"><i class="fa fa-check"></i><b>3</b> spinifex: an R package for creating user-controlled animated linear projections</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:algorithm"><i class="fa fa-check"></i><b>3.1</b> Algorithm</a></li>
<li class="chapter" data-level="3.2" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#oblique-cursor-movement"><i class="fa fa-check"></i><b>3.2</b> Oblique cursor movement</a></li>
<li class="chapter" data-level="3.3" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:pkgstructure"><i class="fa fa-check"></i><b>3.3</b> Package structure</a></li>
<li class="chapter" data-level="3.4" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:usecases"><i class="fa fa-check"></i><b>3.4</b> Use cases</a></li>
<li class="chapter" data-level="3.5" data-path="3-ch-spinifex.html"><a href="3-ch-spinifex.html#sec:discussion"><i class="fa fa-check"></i><b>3.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html"><i class="fa fa-check"></i><b>4</b> A Study on the Benefit of a User-Controlled Radial Tour for Variable Importance for Structure in High-Dimensional Data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:userstudy"><i class="fa fa-check"></i><b>4.1</b> User study</a></li>
<li class="chapter" data-level="4.2" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:results"><i class="fa fa-check"></i><b>4.2</b> Results</a></li>
<li class="chapter" data-level="4.3" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:conclusion"><i class="fa fa-check"></i><b>4.3</b> Conclusion</a></li>
<li class="chapter" data-level="4.4" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:spinifex"><i class="fa fa-check"></i><b>4.4</b> Accompanying tool: radial tour application</a></li>
<li class="chapter" data-level="4.5" data-path="4-ch-userstudy.html"><a href="4-ch-userstudy.html#sec:appendix"><i class="fa fa-check"></i><b>4.5</b> Extended analysis</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html"><i class="fa fa-check"></i><b>5</b> Exploring Local Explanations of Non-linear Models Using Animated Linear Projections</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:shap"><i class="fa fa-check"></i><b>5.1</b> SHAP and tree SHAP local explanations</a></li>
<li class="chapter" data-level="5.2" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:cheemviwer"><i class="fa fa-check"></i><b>5.2</b> The cheem viewer</a></li>
<li class="chapter" data-level="5.3" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:casestudies"><i class="fa fa-check"></i><b>5.3</b> Case studies</a></li>
<li class="chapter" data-level="5.4" data-path="5-ch-cheem.html"><a href="5-ch-cheem.html#sec:cheemdiscussion"><i class="fa fa-check"></i><b>5.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html"><i class="fa fa-check"></i><b>6</b> Conclusion</a>
<ul>
<li class="chapter" data-level="6.1" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html#contributions-1"><i class="fa fa-check"></i><b>6.1</b> Contributions</a></li>
<li class="chapter" data-level="6.2" data-path="6-ch-conclusion.html"><a href="6-ch-conclusion.html#limitations-and-future-work"><i class="fa fa-check"></i><b>6.2</b> Limitations and future work</a></li>
</ul></li>
<li class="divider"></li>
<li><strong><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></strong></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interactive and dynamic visualization of high-dimensional data via animated linear projections, their efficacy, and their application to local explanations of non-linear models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-background" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Background</h1>
<p>This section starts with setting the scope for the type of data we are focusing on, gives a brief motivation then works through previous multivariate visualizations and their scalability. By the end, the focus narrows to linear projections, and in particular, the class of animated linear projections known as the tour.</p>
<!-- Data scope and context -->
<p>For our purposes, I will be focusing on the case where data <span class="math inline">\(X_{nxp}\)</span> contains <span class="math inline">\(n\)</span> observations of <span class="math inline">\(p\)</span> variables is complete with no missing values, variables are numeric (ideally not ordinal levels), and <span class="math inline">\(n&gt;p\)</span> typically many more observations than variables. While I write as though always operating on the original variable space, these methods could similarly be applied to component spaces or feature decompositions of data not fitting this format. In the case of the linear projection let, <span class="math inline">\(Y_{nxd} = X_{nxp} \cdot A_{pxd}\)</span> be the embedding of the data mapped by the basis <span class="math inline">\(A\)</span>, where <span class="math inline">\(d&lt;p\)</span>. When <span class="math inline">\(p\)</span> is large, say over 10 or 20 variables, the viewing space is quite large. A PCA initialization step is commonly used in these cases where the variables are approximated as fewer principal components. This reduced space can be viewed, albeit with the disadvantage of having another linear mapping back to the original space. <span class="citation"><a href="6-ch-conclusion.html#ref-wickham_visualizing_2015" role="doc-biblioref">Hadley Wickham, Cook, and Hofmann</a> (<a href="6-ch-conclusion.html#ref-wickham_visualizing_2015" role="doc-biblioref">2015</a>)</span> also view model spaces and features while urging to prefer visualizing data-space directly.</p>
<!-- Better than numerical summarization alone -->
<p>Visualization is much more robust than numerical summarization alone <span class="citation">(<a href="6-ch-conclusion.html#ref-anscombe_graphs_1973" role="doc-biblioref">Anscombe 1973</a>; <a href="6-ch-conclusion.html#ref-matejka_same_2017" role="doc-biblioref">Matejka and Fitzmaurice 2017</a>)</span>. In these studies, data sets have the same summary statistics yet contain obvious visual trends and shapes that could go completely unheeded if plotting is foregone. Data visualization is fundamental to EDA, quickly evaluating the support of the data, and ensuring that models are suitable.</p>
(ref:ch2fig1-cap) Starting from the profile of a dinosaur, observations are allowed to drift (by iterated simulated annealing) toward 12 patterns provided that they stay close to the original statistics <span class="citation">(<a href="6-ch-conclusion.html#ref-matejka_same_2017" role="doc-biblioref">Matejka and Fitzmaurice 2017</a>)</span>. Visualization of data yields stark designs that are easy to miss in numerical summarization.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch2fig1"></span>
<img src="figures/ch2_fig1_matejka17fig.PNG" alt="(ref:ch2fig1-cap)" width="100%"  />
<p class="caption">
Figure 2.1: (ref:ch2fig1-cap)
</p>
</div>
<!-- Penguins data -->
<p>The work in <span class="citation"><a href="6-ch-conclusion.html#ref-grinstein_high-dimensional_2002" role="doc-biblioref">Grinstein, Trutschl, and Cvek</a> (<a href="6-ch-conclusion.html#ref-grinstein_high-dimensional_2002" role="doc-biblioref">2002</a>)</span> gives a good taxonomy of high-dimensional visualization. Below we will consider views of othortogonal views, observation based visuals and discuss with tour methods in more detail. We are concerned with the question “How can an analyst visualize arbitrary <span class="math inline">\(p-\)</span>dimensions?” We continue to use the Palmer penguins data in illustrations. there are 333 observation of four physical measurements of bill length, bill depth, flipper length, and body mass. It measured three species of penguins between 2007 and 2009 near Palmer Station, Antarctica.</p>
<div id="scatterplot-matrices" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Scatterplot matrices</h2>
<!-- SPLOM -->
<p>Viewing as many univariate histograms or density curves is one method. Similarly, one could look at all variable pairing as scatter plots. This forms the crux of the scatterplot matrices, also known as SPLOM <span class="citation">(<a href="6-ch-conclusion.html#ref-chambers_graphical_1983" role="doc-biblioref">Chambers et al. 1983</a>)</span>. In a scatterplot matrix, variables are displayed across the columns and rows. The diagonal elements show the univariate densities, while off-diagonal positions show scatterplot pairs. This is useful for getting a handle on the support of the variables but is not going to scale well with dimension and is not a suitable audience-ready display. It is hectic and doesn’t draw attention to any one spot. <span class="citation"><a href="6-ch-conclusion.html#ref-munzner_visualization_2014" role="doc-biblioref">Munzner</a> (<a href="6-ch-conclusion.html#ref-munzner_visualization_2014" role="doc-biblioref">2014</a>)</span> reminds us to abstract all of the cognitive work out of the visual, allowing the audience to focus on seeing the evidence supporting the claim.</p>
(ref:penguinsplom-cap) Scatterplot matrix of penguins data. This is a good exploratory step, but does not direct the audience attention and will not scale well at <span class="math inline">\(p\)</span> increases.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:penguinsplom"></span>
<img src="figures/ch2_fig2_penguin_splom.png" alt="(ref:penguinsplom-cap)" width="100%"  />
<p class="caption">
Figure 2.2: (ref:penguinsplom-cap)
</p>
</div>
<p>XXX: TODO move PCA and Grand tour to background (out of intro and RQ)
<!-- PCA -->
<!-- One example of linear projections is that of Principal Component Analysis [PCA, @pearson_liii._1901], which creates a component space ordered by descending variation. It uses eigenvalue decomposition to identify the basis reorientation. These components are typically viewed as discrete orthogonal pairs, commonly approximated in fewer components. --></p>
</div>
<div id="parallel-coordinate-plots" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Parallel coordinate plots</h2>
<!-- PCP & observation based visuals -->
<p>Alternatively, we could consider a class of observation-based visuals. In parallel coordinate plots <span class="citation">(<a href="6-ch-conclusion.html#ref-ocagne_coordonnees_1885" role="doc-biblioref">Ocagne 1885</a>)</span>, variables are arranged horizontally, and lines connect observations with the height mapping to the quantile or z-value for each variable. This scales much better with dimensions but poorly with observations. It also suffers from an asymmetry with the variable order. That is, changing the order of the variable will lead people to very different conclusions. The x-axis is also used to display variables rather than the values of the observations. This restricts the amount of information that can be interpreted between variables. Munzner asserts that position is the more human-perceptible channel for encoding information; we should like to reserve it for the values of the observations. The same issues persist across other observation-based displays such as radial variants, pixel-oriented visuals, and Chernoff faces <span class="citation">(<a href="6-ch-conclusion.html#ref-keim_designing_2000" role="doc-biblioref">Keim 2000</a>; <a href="6-ch-conclusion.html#ref-chernoff_use_1973" role="doc-biblioref">Chernoff 1973</a>)</span>. These visuals are better suited for the <span class="math inline">\(n&lt;p\)</span> case with more variables than observations.</p>
(ref:penguinpcp-cap) Parallel coordinate plots of penguins data. This does not scale well with observations, suffers from asymmetry with the variable ordering, and horizontal position is used for variables rather than observation levels.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:penguinpcp"></span>
<img src="figures/ch2_fig3_penguin_pcp.png" alt="(ref:penguinpcp-cap)" width="100%"  />
<p class="caption">
Figure 2.3: (ref:penguinpcp-cap)
</p>
</div>
</div>
<div id="dimension-reduction" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Dimension reduction</h2>
<!-- Dimension reduction -->
<p>Ultimately, we will need to turn to dimension reduction to create a compelling visual allowing audiences to focus on features with contributions from multiple variables. Dimension reduction is separated into two categories, linear and non-linear. The linear case spans all affine mathematical transformations, essentially any mapping where parallel lines stay parallel. Non-linear transformations are the complement of the linear case, think transformations containing exponents or interacting terms. Examples in low dimensions are relatable. For instance, shadows are examples of linear projections where a 3-dimensional object casts a 2D projection, the shadow. Our vision at any one instance of time, or a picture, is also a 2D projection. An example of a non-linear transformation is that of 2D representation of the globe. There are many different ways (and features to optimize) to distort the surface to display as a map. The most common may be rectangular displays where the area is distorted proportional with the distance away from the equator. Other distortions are created when the surface is unwrapped as a long ellipse. Yet others create non-continuous gaps in oceans to minimize the distortion of countries.</p>
<!-- Non-linear -->
<p>Non-linear techniques often have hyperparameters that affect how the spaces are distorted to fit into fewer dimensions. To quote Anastasios Panagiotelis, “All non-linear projections are wrong, but some are useful,” a play on George Box’s quote about models. Non-linear techniques distort the space in unclear ways, and what is more, they can introduce features not in the data depending on the selection of hyperparameters. The presence of structure in a non-linear model is necessary but not sufficient to conclude the existence of a structure in the data.</p>
<!-- No free lunch trade off -->
<p>Unfortunately, there is no free lunch here. An increase in the original data dimensions will lead to a <span class="math inline">\(p-d\)</span>-dimensional viewing space in the linear case and an increasingly perturbed and distorted space in non-linear techniques. The intrinsic dimensionality of data is the number of variables needed to minimally represent the data <span class="citation">(<a href="6-ch-conclusion.html#ref-grinstein_high-dimensional_2002" role="doc-biblioref">Grinstein, Trutschl, and Cvek 2002</a>)</span>. Intrinsic data dimensionality is an important aspect of dimension reduction that does not have to end in visual but is also a standard part of factor analysis and preprocessing data. Consider a Psychology survey consisting of 100 questions about the Big Five personality traits. The data consists of 100 variables, while the theory would suggest the intrinsic dimensionality is five. The data likely picks up on other aspects and may be better summarized with eight or ten dimensions. If this were the case, reducing the data to this space would be necessary to gate the exponentially increasing view time.</p>
<p>XXX: TODO Work in PCA and Grand tour to background (out of intro and RQ)
<!-- PCA -->
One example of linear projections is that of Principal Component Analysis <span class="citation">(PCA, <a href="6-ch-conclusion.html#ref-pearson_liii._1901" role="doc-biblioref">Pearson 1901</a>)</span>, which creates a component space ordered by descending variation. It uses eigenvalue decomposition to identify the basis reorientation. These components are typically viewed as discrete orthogonal pairs, commonly approximated in fewer components.</p>
</div>
<div id="tours-animated-linear-projections" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Tours, animated linear projections</h2>
<!-- Segue from barstool -->
<p>A single linear projection is a resulting space from the data multiplied by the basis. In contrast to one such projection, a data visualization tour animates many such projections through small changes in the basis. In the barstool shadow analogy, structural information about a hidden object was gained by watching its shadow change from its rotation. An analyst similarly gains information about the data object by watching continuous changes to the basis (the orientation of the data). There are various types of tours that are classified by the generation of their basis paths. We enumerate a few related to this work. A more comprehensive discussion and review of tours can be found in the works of <span class="citation"><a href="6-ch-conclusion.html#ref-cook_grand_2008" role="doc-biblioref">Cook et al.</a> (<a href="6-ch-conclusion.html#ref-cook_grand_2008" role="doc-biblioref">2008</a>)</span> and <span class="citation"><a href="6-ch-conclusion.html#ref-lee_state_2021" role="doc-biblioref">Lee et al.</a> (<a href="6-ch-conclusion.html#ref-lee_state_2021" role="doc-biblioref">2021</a>)</span>.</p>
<!-- Grand tour -->
<p>Originally in a <em>grand</em> tour <span class="citation">(<a href="6-ch-conclusion.html#ref-asimov_grand_1985" role="doc-biblioref">Asimov 1985</a>)</span> several target frames are randomly selected.
Figure <a href="2-ch-background.html#fig:ch2fig4">2.4</a> shows six frames of a grand tour. The grand tour is good for EDA in that it will show frames with widely varying contributions, but without either an destination nor a means steer.</p>
(ref:ch2fig4-cap) Frames of a grand tour of Palmer Penguin data. Tours animate linear projections over small changes in the basis. The observation permanence between frames is an essential distinction in tours. Target frames are selected randomly in the grand tour. An animation can be viewed at <a href="https://vimeo.com/670936494">vimeo.com/670936494</a>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch2fig4"></span>
<img src="figures/ch2_fig4_penguin_grandtour.png" alt="(ref:ch2fig4-cap)" width="100%"  />
<p class="caption">
Figure 2.4: (ref:ch2fig4-cap)
</p>
</div>
<!-- Guided tour -->
<!--  A _guided_ tour uses simulated annealing to move progressively closer to an objective function in the resulting projection [@hurley_analyzing_1990]. Starting from a supplied basis, the guided tour searchs for nearby bases that locally optimize an objective function. -->
(ref:ch2fig5-cap) Illustration of geodesic interpolation between a tours target frames (grey) and the resulting intermediate frame (white). Figure from <span class="citation"><a href="6-ch-conclusion.html#ref-buja_computational_2005" role="doc-biblioref">Buja et al.</a> (<a href="6-ch-conclusion.html#ref-buja_computational_2005" role="doc-biblioref">2005</a>)</span>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch2fig5"></span>
<img src="figures/ch2_fig5_buja05fig.PNG" alt="(ref:ch2fig5-cap)" width="100%"  />
<p class="caption">
Figure 2.5: (ref:ch2fig5-cap)
</p>
</div>
<p>Regardless the type of tour target basis identified, tours must interpolate frames between distant target bases. Figure <a href="2-ch-background.html#fig:ch2fig5">2.5</a> illustrates the geodesic interpolation between distant target frames. Animating though small enough changes between these interpolated frame is important for the trackability of observations as the basis changes, a improvement over static linear projections.</p>
</div>
<div id="evaluating-multivariate-data-visualization" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Evaluating multivariate data visualization</h2>
<p>We have discussed a number of different multivariate visualizations. The analyst is left to determine which visualization to use. In Chapter <a href="4-ch-userstudy.html#ch-userstudy">4</a>, we conduct a user study comparing competing visualizations.</p>
<!-- Related studies & research gap -->
<p>There have been several user studies comparing scatterplots of dimension-reduced spaces across 2D and 3D scatterplots on traditional 2D monitors <span class="citation">(<a href="6-ch-conclusion.html#ref-gracia_new_2016" role="doc-biblioref">Gracia et al. 2016</a>; <a href="6-ch-conclusion.html#ref-wagner_filho_immersive_2018" role="doc-biblioref">Wagner Filho et al. 2018</a>)</span>. There are also empirical statistics used to describe non-linear reduction and how well and faithfully they embed the data in a lower space <span class="citation">(<a href="6-ch-conclusion.html#ref-bertini_quality_2011" role="doc-biblioref">Bertini, Tatu, and Keim 2011</a>; <a href="6-ch-conclusion.html#ref-liu_visualizing_2017" role="doc-biblioref">Liu et al. 2017</a>; <a href="6-ch-conclusion.html#ref-sedlmair_empirical_2013" role="doc-biblioref">Sedlmair, Munzner, and Tory 2013</a>; <a href="6-ch-conclusion.html#ref-van_der_maaten_visualizing_2008" role="doc-biblioref">Maaten and Hinton 2008</a>)</span>. <span class="citation"><a href="6-ch-conclusion.html#ref-nelson_xgobi_1999" role="doc-biblioref">Nelson, Cook, and Cruz-Neira</a> (<a href="6-ch-conclusion.html#ref-nelson_xgobi_1999" role="doc-biblioref">1998</a>)</span> even compared 2D with 3D scatterplot tours to detect clusters and dependence in multivariate data. However, studies are absent comparing competing visual techniques.</p>
</div>
<div id="non-linear-models" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> Non-linear models</h2>
<p>In Chapter <a href="5-ch-cheem.html#ch-cheem">5</a>, we turn our attention to predictive modeling, the loss of variable-level interpretablility of the non-linear models, and an attempt to maintain model transparency with local explanation.</p>
<!-- Introduce explanatory vs predictive modeling -->
<p>There are different reasons and emphases to fit a model. <span class="citation"><a href="6-ch-conclusion.html#ref-breiman_statistical_2001" role="doc-biblioref">Breiman</a> (<a href="6-ch-conclusion.html#ref-breiman_statistical_2001" role="doc-biblioref">2001</a>)</span>, reiterated by <span class="citation"><a href="6-ch-conclusion.html#ref-shmueli_explain_2010" role="doc-biblioref">Shmueli</a> (<a href="6-ch-conclusion.html#ref-shmueli_explain_2010" role="doc-biblioref">2010</a>)</span>, taxonomize modeling based on its purpose; <em>explanatory</em> modeling is done for some inferential purpose, while <em>predictive</em> modeling focuses more narrowly on the performance of the objective function. The intended use has important implications for model selection and development. In explanatory modeling, interpretability is vital for drawing inferential conclusions. The use of black-box models are almost exclusively used in predictive modeling, but not without their share of controversy <span class="citation">(<a href="6-ch-conclusion.html#ref-oneil_weapons_2016" role="doc-biblioref">O’Neil 2016</a>; <a href="6-ch-conclusion.html#ref-kodiyan_overview_2019" role="doc-biblioref">Kodiyan 2019</a>)</span>. However, the loss of interpretation presents a challenge.</p>
<!-- Interpretability & baises -->
<p>Interpretability is vital for exploring and protecting against potential biases (e.g., sex <span class="citation">(<a href="6-ch-conclusion.html#ref-dastin_amazon_2018" role="doc-biblioref">Dastin 2018</a>; <a href="6-ch-conclusion.html#ref-duffy_apple_2019" role="doc-biblioref">Duffy 2019</a>)</span>, race <span class="citation">(<a href="6-ch-conclusion.html#ref-larson_how_2016" role="doc-biblioref">Larson et al. 2016</a>)</span>, and age <span class="citation">(<a href="6-ch-conclusion.html#ref-diaz_addressing_2018" role="doc-biblioref">Díaz et al. 2018</a>)</span>) in any model. For instance, models regularly pick up on biases in the training data that have observed influence on the response (output) feature, which is then built into the model. Feature-level (variable-level) interpretability of models is essential in evaluating such biases. It is also generally important for many problems, where accurate prediction is insufficient. Still, one must explain which predictors are most responsible for generating a response value.</p>
<!-- Interpretability & data drift -->
<p>Another concern is data drift, a shift in support or domain of the explanatory features (variable or predictors). Non-linear models are typically more sensitive and do not extrapolate well outside the support of the training data. Maintaining variable interpretability is essential to being able indentify and address issues arising from data drift.</p>
<!-- Local explanations -->
<p>Explainable Artificial Intelligence (XAI) is an emerging field of research that tries to increase the interpretability of black-box models. A common approach is to use <em>local explanations</em>, which attempt to approximate linear feature importance at the location of each instance (observation), or the predictions at a specific point in the data domain. Because these are point-specific, it is challenging to visualize them to understand a model comprehensively.</p>
</div>
<div id="sec:explanations" class="section level2" number="2.7">
<h2><span class="header-section-number">2.7</span> Local explanations</h2>
<!-- Reminder of local explanation -->
<p>Consider a highly non-linear model. It can be hard to determine whether small changes in a feature’s value will make a class prediction change group or identify which features contribute to an extreme residual. Local explanations shed light on these situations by approximating linear feature importance in the vicinity of a single instance.</p>
(ref:ch2fig6-cap) Illustration of a non-linear classification model. An analyst may want to know the variable importance at the vicinity of the highlighted red cross. Knowing this attribution elucidates how the variable are influence this point which is precariously close to the classification boundary. Local explaination approximate this linear attribution in the vicinity of one observation. Figure from <span class="citation"><a href="6-ch-conclusion.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro, Singh, and Guestrin</a> (<a href="6-ch-conclusion.html#ref-ribeiro_why_2016" role="doc-biblioref">2016</a>)</span>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch2fig6"></span>
<img src="figures/ch2_fig6_ribeiro16fig.PNG" alt="(ref:ch2fig6-cap)" width="50%"  />
<p class="caption">
Figure 2.6: (ref:ch2fig6-cap)
</p>
</div>
<!-- Taxonomy of local explanations -->
<p>A comprehensive summary of the taxonomy and literature of explanation techniques is provided in Figure 6 of <span class="citation"><a href="6-ch-conclusion.html#ref-arrieta_explainable_2020" role="doc-biblioref">Arrieta et al.</a> (<a href="6-ch-conclusion.html#ref-arrieta_explainable_2020" role="doc-biblioref">2020</a>)</span>. It includes a large number of model-specific explanations such as deepLIFT <span class="citation">(<a href="6-ch-conclusion.html#ref-shrikumar_not_2016" role="doc-biblioref">Shrikumar et al. 2016</a>; <a href="6-ch-conclusion.html#ref-shrikumar_learning_2017" role="doc-biblioref">Shrikumar, Greenside, and Kundaje 2017</a>)</span>, a popular recursive method for estimating importance in neural networks. There are fewer model-agnostic explanations, of which LIME, <span class="citation">(<a href="6-ch-conclusion.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>)</span> SHAP, <span class="citation">(<a href="6-ch-conclusion.html#ref-lundberg_unified_2017" role="doc-biblioref">Lundberg and Lee 2017</a>)</span>, and their variants are popular.</p>
<!-- Uses of local explanations -->
<p>These instance-level explanations are used in various ways depending on the data. In image classification, where pixels correspond to predictors, saliency maps overlay or offset a heatmap indicating important pixels <span class="citation">(<a href="6-ch-conclusion.html#ref-simonyan_deep_2014" role="doc-biblioref">Simonyan, Vedaldi, and Zisserman 2014</a>)</span>. For instance, pixels corresponding to snow may be highlighted when distinguishing if a picture contains a wolf or husky. In text analysis, word-level contextual sentiment analysis can be used to highlight the sentiment and magnitude of influential words <span class="citation">(<a href="6-ch-conclusion.html#ref-vanni_textual_2018" role="doc-biblioref">Vanni et al. 2018</a>)</span>. In the case of numeric regression, they are used to explain feature additive contributions from the model intercept to the instance’s prediction <span class="citation">(<a href="6-ch-conclusion.html#ref-ribeiro_why_2016" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>)</span>.</p>
<!-- Segue out -->
<p>The work below covers manual tours <span class="citation">(<a href="6-ch-conclusion.html#ref-cook_manual_1997" role="doc-biblioref">Cook and Buja 1997</a>)</span> in Chapter <a href="3-ch-spinifex.html#ch-spinifex">3</a>. Radial tours are one type of manual tour where the contribution of one variable is extended radially to a full contribution, removed completely, then restored to its original contribution. Chapter <a href="4-ch-userstudy.html#ch-userstudy">4</a> compares the efficacy of the radial tour as compared with PCA and the grand tour in a user study. Lastly, Chapter <a href="5-ch-cheem.html#ch-cheem">5</a> extends the use of the radial tour to evaluate the local explanation of black-box models.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="1-ch-introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-ch-spinifex.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
