---
chapter: 5
knit: "bookdown::render_book"
---

# Scrutinizing the linear variable importance of local explanations of non-linear models with animated linear projections {#ch-cheem}

<!-- Segue -->
Now we can be confident that the radial tour leads to better analysis of variable-level attribution to features identified in a projection. We want to increase the interpretability of complex models. Specifically, I suggest a using the radial tour to explore variable sensitivity to the structure identified in linear local explanations of non-linear models.

<!-- Abstract -->
Artificial Intelligence (AI) has seen a revitalization in recent years from the use of increasingly hard-to-interpret black-box models. In such models, increased predictive power comes at the cost of the opacity of factor analysis and variable interpretability. The loss of interpretability has led to the field of eXplainable AI (XAI). XAI attempts to shed light on these models by providing means for interpreting them. A _local explanation_ of a non-linear model gives a point-estimate of linear variable importance in the vicinity of one observation. After creating a model and exacting explanations for each observation, we explore the original variables, the explanation's attribution, and model information side-by-side with interactively display. After identifying an observation of interest, its local explanation is used as a 1D projection basis. We then manipulate the magnitude of a variable's contribution with a _tour_ technique. The tour animates many projections over small changes in the projection basis as it changes the contribution of one variable. Doing so allows an analyst to visually explore the data through the lens of this local explanation and test the variable support where the explanation seems valid. The interactive application and code facilitating this methodology is available are the __R__ package __cheem__, available on CRAN.

## Introduction {#sec:intro}
<!-- Higher-level topics -->

<!-- Introduce explanatory vs predictive modeling -->
There are different reasons and emphases to fit a model. Breiman and Shmueli [@breiman_statistical_2001; @shmueli_explain_2010] introduce the idea of distinguishing modeling based on its purpose; _explanatory_ modeling is done for some inferential purpose, while _predictive_ modeling focuses more on the predictions of out-of-sample observations. The intended use has important implications for model selection and development. In explanatory modeling, interpretability is vital for drawing inferential conclusions. While predictive modeling may opt for more accurate non-linear models. The use of black-box models is becoming increasingly common, but not without their share of controversy [@oneil_weapons_2016; @kodiyan_overview_2019]. However, the loss of interpretation should not be taken lightly.

<!-- Interpretability important & baises -->
Interpretability is vital for exploring and protecting against potential biases in any model. For instance, models regularly pick up on biases in the training data that have observed influence on the response variable, which is then built into the model. Such biases include sex [@dastin_amazon_2018; @duffy_apple_2019], race [@larson_how_2016], and age [@diaz_addressing_2018]. Variable-level interpretability of models is essential in the evaluation models for such biases.

<!-- Interpretability & data drift -->
Another concern is that of data drift. Data drift is an unexpected shift in support of the variables. Non-linear models are regularly more sensitive to these changes. The interpretability of the model means more transparency to see where models' predictions may be plausible or completely unreliable.

<!-- Local explanations -->
Explainable Artificial Intelligence (XAI) is a recent branch of research that tries to increase the interpretability of black-box models. One way to do so is to use _local explanations_, which take a variable attribution approach to bring transparency to a model. Local explanations attempt to approximate linear variable importance at the location of one observation. Even with a linear space or local approximation, the number of terms is usually sizable, thus challenging to visualize comprehensively.

<!-- Data visualization tours -->
In multivariate data visualization, a _tour_ [@asimov_grand_1985; @buja_grand_1986; @lee_state_2021] is a sequence of linear projections of data onto a lower-dimensional space. Tours are viewed as an animation over minor changes to the projection basis. Structure in a projection can then be explored visually to see which variables contribute to the formation of that structure. The intuition is similar to watching the shadow of a hidden 3D object change as the object is rotated; watching the shape of the shadow change conveys information of the structure and features of the object.

<!-- Manual tours --> 
There are various types of tours distinguished by the generation of projection bases. In a _manual_ tour [@cook_manual_1997; @spyrison_spinifex_2020], the path is defined by changing the contribution of a selected variable. <!--tours and models --> Applying tours to models has been done in a couple of contexts. Specifically for exploring various statistical model fits and classification boundaries [@wickham_visualizing_2015], and using tree- and forest-based approaches as a projection pursuit index to generate a tour basis paths [@lee_pptree_2013; @da_silva_projection_2021].

<!-- Purposed approach -->
The proposed approach uses the radial, manual tour to scrutinize a local explanation. After identifying an observation of interest, its explanation can be evaluated by testing the support of the structure identified by changing variable contribution with the radial tour. We provide a free and open-source R package __cheem__ with an interactive application to facilitate analysis. We supply toy and modern datasets case studies for classification and regression tasks.

<!-- What-if & counterfactual analysis -->
The change in the projection basis might feel similar to counterfactual, what-if analysis, such as _ceteris paribus_ [@biecek_ceterisparibus_2020]. This phrase, Latin for “other things held constant” or “all else unchanged”, shows how an observation's prediction would change from a marginal change in one explanatory variable given that other variables are held constant. It ignores correlations of the variables and imagines a case that was not observed. In contrast, our approach is a geometric explanation of the factual; it varies contributions of the variables by rotating the basis, a reorientation of the data object. Another difference is that the basis must remain orthonormal. That is to say, when the contribution of one variable decreases, the contributions of others necessarily increase such that there is a complete component in that direction.

<!-- Paper structure -->
The remainder of this paper is organized as follows. The following section, [Local explanation statistics](#sec:explanations), covers the background of the local explanation, SHAP, and the traditional visuals produced from it. [Tours and the radial tour](#sec:tour) digs deeper into these animations of continuous linear projections. The section [Application Design](#sec:applicationdesign) discusses the visual layouts, how they facilitate analysis, data preprocessing, and package infrastructure. The section [Case Studies](#sec:casestudies) illustrates several applications of this method. We conclude with a [Discussion](#sec:cheemdiscussion) of the insights we draw from classification and regression tasks.


## Local explanation statistics {#sec:explanations}

<!-- Reminder of local explanation -->
Consider a highly non-linear model. At face value, it is hard to say which variable(s) are sensitive to crossing a classification boundary or identify which variables caused an observation to have a relatively extreme residual. Local explanations shed light on these cases by approximating linear variable importance in the vicinity of one observation.

<!-- Taxonomy of local explanations -->
Figure six of @arrieta_explainable_2020 gives comprehensive summarization of the taxonomy and literature of explanation techniques. The figure includes a large number of model-specific explanations such as deepLIFT, [@shrikumar_not_2016; @shrikumar_learning_2017] a popular recursive method for estimating importance in neural networks. There are fewer model-agnostic explanations, of which LIME, [@ribeiro_why_2016] SHAP, [@lundberg_unified_2017] and their variants are popular.

<!-- Uses of local explanations -->
These instance-level explanations are used in various ways depending on the data. In images, saliency maps overlay or offset a heatmap indicating necessary pixels [@simonyan_deep_2014]. For instance, snow may be highlighted when distinguishing if a picture contains a wolf or husky. In text analysis, word-level contextual sentiment analysis can be used to highlight the sentiment and magnitude of influential words [@vanni_textual_2018]. In the case of numeric regression, they are used to explain variable additive contributions from the model intercept to the observation's prediction [@ribeiro_why_2016].


### SHAP and tree SHAP

<!-- SHAP and history -->
SHaply Additive exPlanations (SHAP) approximates the variable importance in the vicinity of one observation by taking the median importance of a subset of permutations in the explanatory variables. This idea stems from the field of game theory, where @shapley_value_1953 devised a method to evaluate an individual's contribution to cooperative games by permuting the players that contribute to the score.

<!-- Fifa example -->
To illustrate SHAP and its original use, explaining the difference between the intercept and an observation's prediction, we use soccer data from FIFA 2020 season [@leone_fifa_2020]. We have 5000 observations of nine skill measures (after aggregating highly correlated variables). A random forest model is fit to regress the log wages, in 2020 Euros, from the skill measures. We then extract the SHAP values of a star offensive player (L. Messi) and defensive player (V. van Dijk). We expect to see a difference in the attribution of the variable importance across the two positions of the players.

```{r ch5fig1, echo=F, out.width = "100%", fig.align='center', fig.cap = "Illustration of the distribution of SHAP attributions and a break-down plot. From FIFA 2020 data, a random forest model regresses wages from nine skill attributes for a star offensive and defensive player. The players have very different salaries, but a) shows the distributions of 25 permutations in the explanatory variables. The medians of these distributions are the final SHAP values. The variable importance differs across the exogenous information of player position. These explanations make sense; the variable importances seem realistic given the player's positions. b) Break-down plots of the observations using their explanations to additively cover the difference between the model intercept and the observation predictions."}
nsSafeIncGraphic("./figures/ch5_fig1_shap_distr_bd.png")
```

<!-- Illustration -->
Figure \@ref(fig:ch5fig1) shows the SHAP values of these players. Panel a) shows these players receive a sizable difference in wages. Panel b) shows the underlying distribution of the SHAP attributions while permuting the explanatory variables, with the medians being the SHAP values. In the light of the player position, the difference in the variable importance makes sense; offense and movement skills are more important for the offensive player, while defensive and power skills are more informative to the model for explaining the prediction of the defensive player. We would likewise expect the profile of variable importance to be unique for star players of other positions, such as goalkeepers or middle fielders. Panel b) shows a simplified break-down plot [@gosiewska_ibreakdown_2019], where a local explanation is used to additively explain the difference from the intercept to the observation's prediction. Such additive approaches will show asymmetry in the variable ordering, so we opt to fix the order to panel a) by decreasing the sum of the SHAP values.

<!-- Segue -->
In summary, this figure highlights how local explanations bring interpretability to a model, at least in the vicinity of their observations. In this instance, two players with different positions receive different profiles of variable importance to explain the prediction of their wages. In application, we apply _tree SHAP_, a variant of SHAP enjoys a lower computational complexity [@lundberg_consistent_2018]. Tree SHAP is only compatible with tree-based models; we illustrate random forests. The following section will use normalized explanations as the starting projection basis to further scrutinize the explanation.


## Tours and the radial tour

<!-- Tours intro -->
A data visualization _tour_ animates many linear projections over small changes in the basis. One of the critical features of the tour is the object permanence of the data points; one can track the relative changes of observations as the basis moves, as opposed to discretely jumping to an orthogonal view with no intermediate information. There are various types of tours that are distinguished by selecting their basis paths [@lee_state_2021; @cook_grand_2008].


### Manual tours and its radial case

<!-- Manual tour -->
The manual tour [@cook_manual_1997] defines its basis path by manipulating a selected variable's contribution to the basis. A manipulation dimension is appended onto the projection plane, giving a full contribution to the chosen variable. The bases are then selected based on rotating this newly created manipulation space. A crucial feature of the manual tour is that it allows users to control the variable contributions of the basis. Such manipulations can be selected and queued in advance or selected on the spot for human-in-the-loop analysis [@karwowski_international_2006]. However, this navigation is relatively time-consuming due to the vast volume of the display space. It is advisable to use this method to explore the sensitivity of the variable contribution to a previously identified feature of interest. In this case, the projection of the normalized explanations is the feature of interest.

<!-- Radial tour -->
More generally, the manual tour can change the contribution of a variable to the display dimensions. We will apply a more directed interaction, namely, a _radial_ tour. In a radial tour, the selected variable is allowed to change its magnitude of contribution but not its angle; it must move along the direction of its original contribution.


## Cheem viewer {#sec:applicationdesign}

Below we illustrate the two primary displays of the cheem viewer application: the global view and the tour view. Then we cover what we take away from the classification and regression tasks. Lastly, we discuss the preprocessing of the data before application runtime.


### Global view

<!-- Purpose -->
The global view provides an essential context of all observations and facilitates the exploration of the separability of the data- and attribution-spaces. While the comparison of these spaces is interesting, the global view's purpose is to enable the selection of observations. The local explanation of these points will be explored in more detail.

<!-- Approximations of the spaces, PC1:2 -->
An approximation of these spaces is given as the first two principal components of their respective spaces. Model information, the observed response by its prediction, is also provided. The orientation and magnitude of the variables are inscribed on a unit circle. <!-- Interactions -->Misclassified observations are circled in red if applicable. Linked brushing between the plots and tabular display of select points facilitates exploration of the spaces and the model. A single 2D projection will not encompass all of the structure of higher-dimensional space. However, it is a reasonable summarization given the real task at hand; the selection of observations to explore further.


### Radial cheem tour

<!-- A function of the obs selected -->
The global view facilitated the selection of a primary and optional comparison observation. The variable-level attribution of the primary observation is normalized and used as the initial 1D basis in a radial tour. This is an approximation of the contributions of the linear variables that best explain the difference between the model intercept and an observation's prediction, not the local shape of the model surface.

<!-- Tour start frame -->
The initial frame is the normalized SHAP values of the primary observation. The current projection basis is depicted as the width of a bar, the variable's contribution to the horizontal axis. The normalized values of all observations are shown as vertical parallel coordinate plots.

<!-- Tour animation -->
The radial tour creates a basis path by varying the contribution of a selected variable, fully into and out of a projection frame. Doing so tests an individual variable's sensitivity to the structure identified by the local explanation. The default variable selected has the largest discrepancy between the attribution of primary and comparison observations. The following sections elaborate on the takeaways from applying this approach in classification and regression tasks. <!-- Segue to classification -->Now that we have introduced the global view and corresponding cheem radial tour, let us discuss the differences between the classification and regression cases.


### Classification task

What information do we glean from using this method on a classification task? Typically we select a misclassified observation compared to a correctly classified point nearby in data space. The initial frame is the linear attribution of that observation's local explanation. By default, the manual tour varies the contribution of the variable with the largest difference between the primary and comparison observation; we can test the sensitivity of each variable to structure identified by the local explanation; we are exploring the support of the explanation, evaluating the support or robustness of the prediction.

```{r ch5fig2, echo=F, out.width="100%", fig.align="center", fig.cap = "Display illustrating the classification case. Plots are colored on predicted class, and red circles indicate misclassified observations. The radial tour is a 1D projection starting at the normalized tree SHAP values of the primary point. The first frame is the linear-variable importances that best describe the difference from model intercept to this observation's prediction. We probe the support of the variable contributions by selecting a variable to vary the contribution."}
nsSafeIncGraphic("./figures/ch5_fig2_app_classification.PNG")
```


### Regression task

In the regression case, the global view can be colored on a statistic to highlight the explanation space's structure. For this purpose, we include residuals, log Mahalanobis distance of data space (a measure of outlyingness), and the correlation of the attribution projection with the observed response. In the radial tour, the horizontal positions are the same, the basis projection of the radial tour. The vertical position is fixed to the observed response variable and residuals in the middle and right panels. Correspondingly, the display changes from univariate density to 2D scatterplot. The basis is still one component (horizontal) independent of the vertical position.

```{r ch5fig3, echo=FALSE, out.width="100%", fig.align="center", fig.cap = "Display of the regression task. The global view can be colored on the correlation of the attribution projection and observed response. In the tour, the horizontal values are the same as the classification case; the projection through the basis. The vertical position is now mapped to the observed y and residuals."}
nsSafeIncGraphic("./figures/ch5_fig3_app_regression.PNG")
```


### Interactive features

<!-- Reactive vs exploration interactions -->
The application has several reactive inputs that affect the data used, aesthetic display, and tour manipulation. These reactive inputs make the software flexible and extensible. The application also has more exploratory interactions to help link points across displays and reveal structure found in different spaces.

<!-- Exploratory interactions -->
A tooltip displays observation number/name and classification information while the cursor hovers over a point. Linked brushing allows the selection of points (left click and drag) where those points will be highlighted across plots. The information corresponding to the selected points is populated on a dynamic table. These interactions aid exploration of the spaces and, finally, identification of a primary and comparison observation.

```{r ch5fig4, echo=F, out.width="100%", fig.cap = "Illustration of data explorations interactions in the global view. This view has linked brushing, where observations selected in one facet are highlighted in the other facets and populate an interactive tabular display below. Tooltips display when hovering over an observation."}
nsSafeIncGraphic("./figures/ch5_fig4_app_interactions.PNG")
```


### Preprocessing

It is vital to mitigate the render time of visuals, especially when users may want to iterate many times. All computational operations should be prepared before runtime. The work remaining when an application is ran is solely reacting to inputs and rendering of visuals and tables. Below we discuss the steps and details of the reprocessing.

(ref:citeRf) @liaw_classification_2002
(ref:citeTs) @kominsarczyk_treeshap_2021
\begin{itemize}
	\item \textbf{Data:} a complete numerical matrix; explanatory and response variable. An optional categorical variable can be mapped to the color and shape of observations. Explanatory variables are scaled in visualization after modeling or creating local explanations. 
	\item \textbf{Model:} any model can be used with this method. Currently, we apply random forest models via the package \textbf{randomForest} [(ref:citeRf)] for compatibility with the local explanation, which requires tree-based models.
	\item \textbf{Local explanation:} any model-compatible linear explanation could be used. We apply tree SHAP, a more computationally efficient variant of SHAP, compatible with tree-based models. Tree SHAP was calculated with the package \textbf{treeshap} [(ref:citeTs), hosted on GitHub only]. The global view shows all observations in attribution space, requiring the variable importance from \emph{all} observations rather than just one.
\end{itemize}

<!-- Note on time of execution -->
The time to preprocess the data will vary significantly with the model and local explanation. For reference, the FIFA data, 5000 observations of nine explanatory variables, took 2.9 seconds to fit a random forest model of modest hyperparameters. Extracting the tree SHAP values of each observation took 254 seconds combined. PCA and statistics of the variables and attributions took 0.6 seconds. These runtimes were from a non-parallelized R session on a modern laptop, but suffice to say that the bulk of the time will be spent on the local attribution. An increase in model complexity or data dimensionality will quickly become an obstacle. With its reduced computational complexity, this makes tree SHAP a good candidate to start with. Alternatively, the package __fastshap__ [@greenwell_fastshap_2020] claims extremely low runtimes, which are attributed to fewer calls to the prediction function, partial implementation in C++, and efficient use of logical subsetting.


### Package infrastructure {#sec:infrastructure}

The above-described method and application are implemented as an open-source __R__ package, __cheem__ available on [CRAN](https://CRAN.R-project.org/package=cheem). Preprocessing was facilitated with models created via __randomForest__ [@liaw_classification_2002], and explanations calculated with __treeshap__ [@kominsarczyk_treeshap_2021]. The application was made with __shiny__ [@chang_shiny_2021]. The tour visual is built with __spinifex__ [@spyrison_spinifex_2020]. Both views are created first with first with __ggplot2__ [@wickham_ggplot2_2016] and then rendered as interactive HTML widgets with __plotly__ [@sievert_interactive_2020]. __DALEX__ [@biecek_dalex_2018] and the free ebook, _Explanatory Model Analysis_ [@biecek_explanatory_2021] were a huge boon to understanding local explanations and how to apply them.


### Installation and getting started

The following __R__ code will help getting up and running:

```{r eval=FALSE, echo=TRUE}
## Download the package
install.packages("cheem", dependencies = TRUE)
## Restart the R session so the IDE has the correct directory structure
restartSession()
## Load cheem into session
library("cheem")
## Try the app
run_app()

# Processing your data
## Install treeshap from github, to use as a local explainer
remotes::install_github('ModelOriented/treeshap') ## Local 
## Follow the examples in cheem_ls()
?cheem_ls
```


## Case studies {#sec:casestudies}

To illustrate the use of the cheem method, we apply it to modern datasets, two classification examples and then two of regression.


### 1) Penguin, species classification

Palmer penguins data [@gorman_ecological_2014; @horst_palmerpenguins_2020] consist of 330 observations across four physical measurements of three species of penguins foraging near Palmer Station, Antarctica. A random forest model was fit, classifying the species of the penguin given the physical measurements.

(ref:casepenguins) Species classification of Palmer penguin data. We select a chinstrap penguin that is mislabeled as an adelie. By varying the contribution to bill length, we observe the explanation does not hold when bill length has a significant contribution. The .mp4 animation of this tour can be found at [github.com/nspyrison/cheem_paper/blob/main/figures/case_penguins.mp4](https://github.com/nspyrison/cheem_paper/blob/main/figures/case_penguins.mp4)
```{r casepenguins, echo=F, out.width="100%", fig.cap = "(ref:casepenguins)"}
nsSafeIncGraphic("./figures/ch5_fig5_case1_penguins.png")
```

In figure \@ref(fig:casepenguins), a misclassified point is contrasted with a correctly classified point of its observed class nearby in data-space. The attribution space from the tree SHAP local explanations is a more separable space, where the comparison is squarely in the middle of the orange distribution. The primary observation is between the predicted and observed clusters, a sign of uncertainty in the prediction. The tour varies the contribution of bill length (b_l) as this variable differs most from the contribution of the comparison observation. Downplaying the contribution of bill length is crucial to the linear explanation of this observation being misclassified.


### 2) Chocolates, milk/dark chocolate classification

The chocolates dataset consists of 88 observations of 10 nutritional measurements from their labels. Each of which was labeled as being either milk or dark chocolates. We can see if a manufacturer accurately portrays the chocolate with this data. We are curious to see if chocolates that nutritionally look like milk chocolates are labeled as dark chocolates, which may hold a higher market value. We should note that not all chocolates consist wholly of chocolate. The addition of other ingredients will decrease the predictive power of the model nutritional explanatory variable. A random forest model is fit classifying the type of chocolate. We selected a chocolate labeled dark, through predicted to be milk chocolate compared with a chocolate labeled 85% cocoa.

(ref:casechocolates) Chocolates data type classification (milk or dark). We select a chocolate labeled as dark though a random forest model predicts it to be milk chocolate in light of the values on the nutritional label. We vary the contribution of calories from fat. Animated tour can be found at [github.com/nspyrison/cheem_paper/blob/main/figures/case_chocolates.mp4](https://github.com/nspyrison/cheem_paper/blob/main/figures/case_chocolates.mp4).
```{r casechocolates, echo=F, out.width="100%", fig.cap = "(ref:casechocolates)"}
nsSafeIncGraphic("./figures/ch5_fig6_case2_chocolates.png")
```

Figure \@ref(fig:casechocolates) similarly shows that attribution-space is more separable than data-space. Interestingly, the class imbalance that we suspected was not observed; there are only six chocolates labeled as dark and predicted as milk, while eight of the inverse case. Calories from fat is the variable with the largest difference in treeshap attribution between these points. In this case, it feels strange to call the selected observation a misclassification of the model. There are plausible reasons that a manufacturer has incentives to cut corners and label their products different than what they are. This feels more like a measurement theory-related problem. In this case, the candy is being sold as dark chocolate, while nutritional value more closely resembles milk chocolates.


### 3) FIFA, wage regression

The 2020 season FIFA data [@leone_fifa_2020; @biecek_dalex_2018] contains many skill measurements of soccer/football players and wage information. After aggregation of the skill measurements, we regress the log wages [2020 euros] given just the skill aggregates. The model was fit from 5000 observations of the nine skill aggregates before being thinned to 500 players to mitigate occlusion and render time. We compare a leading offensive fielder (L. Messi) with that of a top defensive fielder (V. van Dijk), the same observations were used in figure \@ref(fig:shapdistrbd).

(ref:casefifa) FIFA 2020, regressing log wages [2020 Euros] from aggregations of skill measurements. The primary observation is a star offensive player (L. Messi) compared with a top defensive player (V. van Dijk). The animate radial tour can be found at [github.com/nspyrison/cheem_paper/blob/main/figures/case_fifa.mp4](https://github.com/nspyrison/cheem_paper/blob/main/figures/case_fifa.mp4)
```{r casefifa, echo=F, out.width="100%", fig.cap = "(ref:casefifa)"}
nsSafeIncGraphic("./figures/ch5_fig7_case3_fifa.png")
```
With figure \@ref(fig:casefifa), we will test the premise of the local explanation. If we remove reaction and movement skills from the basis, offense skills are almost singularly important for explaining the offensive player. We vary the contribution of offensive skills. Offensive skills are removed in the tour (panel b, frame 3), and Messi is no longer separated from the group. We also notice that accuracy has rotated into the frame, maintaining some separability.


### 4) Ames housing 2018, sales price regression

Ames 2018, housing data was subset to North Ames (the neighborhood with the most house sales). The remaining are 338 house sales across nine variables. Using interaction from the global view, we select a house with an extreme negative residual and an accurate observation close to it in the data.

(ref:caseames) Ames housing 2018 regressing log sales price [2018 USD]. Because the SHAP values are relatively well distributed across the variables, there is a lot of redundant information to explain the gap between select observations. This case may not be ideal for the cheem methodology, though we did find a near singular frame in the manual tour not, something we came looking for, but certainly, an exciting feature to find. The corresponding animation is at [github.com/nspyrison/cheem_paper/blob/main/figures/case_ames2018.mp4](https://github.com/nspyrison/cheem_paper/blob/main/figures/case_ames2018.mp4)
```{r caseames, echo=F, out.width="100%", fig.cap = "(ref:caseames)"}
nsSafeIncGraphic("./figures/ch5_fig8_case4_ames2018.png")
```

Figure \@ref(fig:caseames) shows the global view and extrema of the tour. The horizontal distance in the tour did not show a significant disparity between our selected points. This is not particularly surprising as most variables have a sizable contribution. Rotating any one variable out of the frame will rotate other vital variables into the frame, preserving most of the distance from intercept to prediction. However, the tour has revealed an interesting feature worth discussing. Notice that the observations pivot about the origin, the basis roughly halfway between bases in frames one and two of panel b) the data is near a singular profile. This means that there is a basis orthogonal to this point that describes sizable variation. Knowing these singular bases can point toward others with meaningful data variation.


## Discussion {#sec:cheemdiscussion}

The need to maintain the interpretability of black-box models is evident. One aspect uses local explanations of the model in the vicinity of an observation. Local explanations approximate the linear variable importance to the model. Our contribution is to assess explanations by examining the support by varying the contributions with a radial tour. First, a global view visualizes approximations of the data space, explanation space, model predictions side-by-side, using dynamic interaction to compare and contrast and identify primary and comparison observations of interest. The normalized linear importance from the explanation of the primary observation becomes the feature of interest to further explore with the radial tour. The tours explore the variable sensitivity to the structure identified in the explanation.

We have illustrated this method on random forest models using the tree SHAP local explanation, while it could be generally used with any compatible model-explanation pairing. We apply it to the classification and regression tasks. We have created an open-source __R__ package __cheem__, available on [CRAN](https://CRAN.R-project.org/package=cheem), to facilitate preprocessing and exploration with the described interactive application. Toy and real data are provided, or upload your data after preprocessing.


## Acknowledgments

We would like to thank Professor Przemyslaw Biecek for his input early in the project and to the broader $\text{MI}^\text2$ lab group for the __DALEX__ ecosystem of __R__ and __Python__ packages. This research was supported by Australian Government Research Training Program (RTP) scholarships. Thanks to Jieyang Chong for helping proofread the article.

The namesake, Cheem, refers to a fictional race of humanoid trees from Doctor Who lore. __DALEX__ pulls on from that universe, and we initially apply tree SHAP explanations specific to tree-based models.

## References

<!-- Adds a bib section at the end of every chapter -->