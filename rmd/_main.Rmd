```{r include=FALSE}
knitr::opts_chunk$set(
  echo  = FALSE, 
  cache = FALSE,
  cache.lazy = FALSE,
  out.width = "100%",
  out.extra = '',fig.show='hold',fig.align='center'
)
```

`r if (knitr::is_html_output()) '<!--'`

# Copyright notice {- #ch-copyright}

\textcopyright { }Nicholas Spyrison (\number\the\year).

I certify that I have made all reasonable efforts to secure copyright permissions for third-party content included in this thesis and have not knowingly added copyright content to my work without the owner's permission.

\newpage

`r if (knitr::is_html_output()) '-->'`

<!--chapter:end:00-a-compyright.Rmd-->

---
knit: "bookdown::render_book"
---

# Abstract {-}

Visualizing data space is crucial to exploratory data analysis, checking model assumptions and validating model performance. Yet visualization quickly becomes difficult as the dimensionality of the data or features increases. Traditionally, static, low-dimensional linear embeddings are used to mitigate this complexity. Such embedded space is a lossy approximation of the full space. However, by viewing many embeddings with interactive supporting views, maximizes the information conveied. Data visualization _tours_ are a class of dynamic linear projections that animate many linear projections over small changes to the projection basis.

Tours are distinguished the path of their projection bases. _Manual tours_ allow for User-Controlled Steering (UCS) of bases path, where the contributions of individual variables can be controlled. First I create a free, open-source __R__ package, __spinifex__ that facilitates the creation of such tours. These manipulations of the variables can be precomputed or made in real-time. The animations are interoperable with the existing package __tourr__ and can be rendered with recent graphics interfaces __plotly__ and __gganimate__. The former offers interactive use with HTML widgets while the latter can render to static formats such as .gif or .mp4 files. An interactive application is inclused to illustration use of the manual tour.

Theoretically, UCS should enable an analyst to better explore explore the variable attribution to the structure identified in an embedding. To test this I conduct a within-participant, crowd-sourced user study. The primary variable of interest is the visualization method where manual tours are compared against the benchmarks of Principal Component Analysis (PCA) and an alternative tour variant, the grand tour. Each of the $n=108$ participants performs two trials with each visual for 648 total trials. The task is a supervised classification problem asking participants which variables attribution to the separation of 2 clusters. I find strong evidence to support that use of the radial tour leads to a large improvement in the accuracy of the variable attribution for this task.

Modern modeling techniques are sometimes refered to as black-box models due the un-interpretable nature of model terms which are often many and non-linear. Recent research in Explainable Artificial Intelligence (XAI) tries to bring interpretability to these models through the use of _local explanations_. Local explanations are a class of techniques that approximate the linear-variable importance at one point in the data. I purpose interactively exploring data- and explanation-spaces side-by-side to identify two observation to compare. Then the explanations can be evaluated to see how well they hold up under the varying contributions with the manual tour. I have created the free open-sourced __R__ package __cheem__ provides preprocessing defaults and in interactive application to explore.

<!-- 
The following line is required to re-set page numbering after preliminary material. Do not remove
-->
\clearpage\pagenumbering{arabic}\setcounter{page}{0}

<!--chapter:end:00-b-abstract.Rmd-->

---
knit: "bookdown::render_book"
---

# Declaration {-}

I hereby declare that this thesis contains no material which has been accepted for the award of any other degree or diploma at any university or equivalent institution and that, to the best of my knowledge and belief, this thesis contains no material previously published or written by another person, except where due reference is made in the text of the thesis.

Three publications are included in this thesis, one of which has been accepted for publication in the _The R Journal_ [@spyrison_spinifex_2020]. The user study described in Chapter \ref{ch:efficacy_radial_tour} will shortly be submitted to the *Journal of Data Science, Statistics, and Visualisation*. The reseach covered in Chapter \ref{ch:cheem} will shortly be submitted to *IEEE Transactions on Knowledge and Data Engineering*.

All of the papers in the thesis were conceptualized, developed, and written by myself, the student, while working in the Department of Human-Centred Computing under the supervision of professor Kimbal Marriott and professor Dianne Cook. When co-authors are listed, it indicates that the work was produced through active collaboration between researchers and that their contributions to team-based research have been recognized. The following table details the work, including my and my fellow co-authors' contributions.

\begin{table}
\centering\footnotesize\tabcolsep=0.12cm
\begin{tabular}{|p{1cm}|p{2cm}|p{1.5cm}|p{3.5cm}|p{3.5cm}|p{1.5cm}|}
\hline
\RaggedRight\textbf{Thesis Chapter}  &
\RaggedRight\textbf{Publication Title}  &
\RaggedRight\textbf{Status (published, in press, accepted or returned for revision)}  & \RaggedRight\textbf{Nature and} {\%} \RaggedRight\textbf{of student contribution} & \RaggedRight\textbf{Co-author name(s) Nature and} {\%} \RaggedRight\textbf{of Co-authorâ€™s contribution} &
\RaggedRight\textbf{Co-author(s), Monash student Y/N} \\ \hline
3 & spinifex: An R Package for Creating a Manual Tour of Low-dimensional Projections of Multivariate Data & Published & 75\%. Concept, development, writing first draft and software development & (1) Dianne Cook, concept, methodology, writing 25\% & N \\ \hline
4 & The benefit of user-controlled radial tour for understanding variable contributions to structure in linear projections & To be submitted & 80\%. Concept, development, writing first draft, application, and conducting user study & (1) Dianne Cook, Concept, methodology, editing 12\% (2) Kimbal Marriott, concept, methodology, editing 8\% & N \\ \hline
5 & Interrogating the linear variable importance of local explanations of
black-box models with animated linear projections & To be submitted & 85\%. Concept, development, writing first draft and software development & (1) Dianne Cook, methodology, editing 10\% (2) Kimbal Marriott, methodology, editing 5\% & N \\ \hline
\end{tabular}
\end{table}

I have renumbered and updated sections of submitted or published papers in order to generate a consistent presentation within the thesis.

\textbf{Student name:} Nicholas Spyrison

\textbf{Student signature:}

```{r, out.width = "50%", fig.align = "left"}
knitr::include_graphics("./figures/signature.jpg")
```

\textbf{Date:} `r as.Date("2021-12-12")`

`r if (knitr::is_html_output()) '-->'`
\vspace{2em}
\noindent
The undersigned hereby certify that the above declaration correctly reflects the nature and extent of the student's co-authors' contribution to this work.
In instances where I am not the responsible author I have consulted with the responsible author to agree on the respective contributions of the authors.

\textbf{Main Supervisor name:} Kimbal Marriott

\textbf{Main Supervisor signature:}

\vspace*{2cm}

\textbf{Date:} `r as.Date("2021-12-12")`



<!--chapter:end:00-c-declaration.Rmd-->

---
knit: "bookdown::render_book"
---

# Acknowledgements {-}

<!-- Firstly,  -->
I would like to express my sincere gratitude to my supervisors professor Kimbal Marriott, professor Dianne Cook for their support of my Ph.D studies and research, their subject expertise, and their careers of supervising and performing research. Thank you for continuously pushing me and my research to new levels. I have enjoyed teaching data visualization which has been strongly shaped by Di's practical, data-first, visualization-often style. I will continue to imagine hearing Kim's persistent question "what did we learn from this?" as a reminder not to get lost in the details of implementation, but also to regularly step-back and analize if this is the correct aspect to be checking.

A special thanks to professor Przemyslaw Biecek for giving his input in the formulation stages of my project and his easy to understand explanations of complex modeling aspects. Thank you for responding to cold emails and being available for collaboration.

I thank my fellow Ph.D students, lab members, and pomodoro partners especially on the occasional of stimulating discussions and for the positive peer-pressure of knowing others are around and working. Thanks immensely to those that have empathized with me and others especially though the hard-ships of studies and COVID-19. Gratitude to Ying Zhou for her enduring support though the thick of my studies and wavering mental health.

Last but not the least, I would like to thank my parents, Doug and Terry, for their support and concerns at odd hours of the day. Thanks to Alan and Claire for their companionship and support now and in our more formative years. I am looking forward to be seeing you all in person shortly.

<!--chapter:end:00-d-acknowledge.Rmd-->

---
knit: "bookdown::render_book"
---

# Preface {-}

I recognize that terminology is often overburdened by ambiguous use, with several changing meanings coming from different fields. My background comes from Statsistics and I will default to terms from statistics and geometry. I provide terminology and notation definition upon first use and in the Glossary \ref{ch:glossary}.

<!--chapter:end:00-e-preface.Rmd-->

---
chapter: 1
knit: "bookdown::render_book"
---

# Introduction {#ch:introduction}

<!-- Exploratory data analysis -->
Exploratory data analysis (EDA) is a term which encompass the initial summarization and visualization of a data set. This is a critical first step of checking for realistic values, finding improper data formats and revealing insights [@tukey_exploratory_1977]. Early and frequent data visualization is key to the data analyst's workflow cycle [@wickham_r_2017]. As modern datasets grow in complexity use of multivariate data is ubiquitous. The number of variables in the data increases the difficulty of creating and portraying an accurate representation of the data which is central to the iterated workflow.

(ref:dataanalysisworkflow-cap) Data analysis workflow [@wickham_r_2017]. This work focuses primary on multivariate data visualization, but also facilitates transforming data and interpretation of models.

```{r dataanalysisworkflow, echo=F, out.width='100%', fig.cap = "(ref:dataanalysisworkflow-cap)"}
knitr::include_graphics("./figures/data_analysis_workflow.png")
```

A data visualization _tour_ [@cook_grand_2008; @lee_review_2021] are a class linear projections that animate small changes to the projection basis. Intuitively, they show a projection of the data object down to lower dimensional space, in the same way that a 3D object casts a 2D shadow. A key feature of the tour is the persistence and tractability of the points through many frames, the rotation of the data object. In the shadow analogy an object, such as a bar stool may be casting a circular shadow that could come from any number of shapes. However, if the stool were rotated the legs would show in the shadow quickly giving an intutive interpretation of the object. In this way and aided with linked brushing and other interactions such continous tours excel at extending the interpretability of multivariate spaces. Chapter \ref{ch:motivation} will motiveate why we decide to use such linear dimension reduction techniques. 


## Research questions
<!-- Hypothesis statement -->

The over-arching question of interest can be stated as:

**Can the radial tour with user interaction help analysts understand linear projections, and explore the sensitivity of structure in the projection to the variables contributing to the projection?**

Understanding variable sensitivity to the structure in a projection frame is crucial factor analysis after identifying a feature of interest. If an identified feature is the _what_ of an analysis then its variable attributions are its _how_. The user interaction afforded by the radial tour should allow for more precise exploration of this structure.

RQ 1. **How do we define user interaction for the radial tours to add and remove variables smoothly from a 2D linear projection of data?**\
@cook_manual_1997 described an algorithm for manually controlling a tour, to rotate a variable into and out of a 2D projection. This algorithm provides the start to human-controlled radial tours. The work [@spyrison_spinifex_2020] was adapted so that the user has more control of the interpolation. The user can set the full range of the contribution from $[-1, 1]$, and output to a device that allows the user to reproduce motions and animate or rock the rotation backward and forwards. These fine-tuned controls provide a better tool for sensitivity analysis.

RQ 2. **Do analysts understand the relationship between variables and structure in a 2D linear projection better when the radial tour is available?**\
We performed an $N=108$, within-participant user study comparing accuracy and time with the primary factor as the type of data visualization. Each participant performed 2 evaluations with either Principal Component Analysis (PCA, which is user control, but discrete), grand tour (continuous interpolated frames, but lack user control), or radial tour (user control and continuous animation). We find strong evidence that the radial tour increases accuracy. We also show the effects from the other experimental factors of location, shape data dimensionality, and the random effects from the data and that of the participants.

RQ 3. **Can the radial tours be used in conjunction with the local explanation, SHAP, to improve the interpretability of black-box models?**\
The tension from the trade-off between accuracy and interpretability of black-box models is rising. There is a clear need to be able to explain black-box models. We use SHAP to extract local explanations which form the projection basis to perform radial tours. We explore how misclassified observations behave relative to nearby correctly classified observations.


## Methodology
<!-- HOW YOU WILL GO ABOUT EACH PROJECT -->

The research corresponding with RO #1 entails _algorithm design_ adapting the algorithm from @cook_manual_1997. This allows for interactive control of 2D projections and serves as a foundation for the remaining work to follow.

To address RO #2, a controlled _experimental study_ has explored the efficacy of interactive radial tours as compared with 2 benchmark methods: Principal Component Analysis (PCA, @pearson_liii._1901) and the grand tour [@asimov_grand_1985]. This was a within-participant user study where each participant experienced each visual. Trials were balanced across 3 other experimental factors: location of the signal, the shape of the cluster distributions, and the dimensionality of the data.

The research for RO #3 involves _fundamental visualization design_. We know that the SAHP value is a local explanation for one observation. This SHAP value will also serve as the 1D basis for the radial tour. While using SHAP as a projection basis is novel it is not particularly insightful by itself. We provide tracking marks on the tour as well as showing the within-class distributions of the SHAP components as parallel coordinate marks on the basis. We also offer a global view and quantitative analysis evaluating the sensitivity of the SHAP-space relative to the sensitivity of the original data space.


## Contributions

1. __spinifex__ a package offering consistent framework for performing radial tour and rendering all tours to various formats:
   - Perform radial tours to explore the variable senstiveity to structure
   - Transform numeric variable in the data
   - Extract various bases exposing features of the data
   - Interoperable with tours generated with __tour__ [@wickham_tourr:_2011]
   - Layered interface for producing tours that mirrors the layered approach of __ggplot2__ [@wickham_ggplot2_2016]
2. A user study comparing the efficacy of the radial tour against the alternatives of PCA the grand tour including:
   - Creation of supervised classification task to assess the variable attribution to the separation of cluster performed under various levels of experimental factors: location, shape, and dimensionality.
   - Definition of an accuracy measure to evaluate this task.
   - Results: strong evidence that the radial tour increases the accuracy of this task by a large amount while the task is performed fastest with the grand tour.
   - Attribution of the error, by performing a mixed model regression helps to explain the source of the error term into the variability of participants skill aptitude for this task and variability of the difficultly of a simulation by random chance.
3. __cheem__ a package applying the radial tour to local explanations of black-box models, including:
   - Preproccing; creation of a random forest model, the extraction of all observation's tree SHAP local explanation, and extraction of other statistics for display.
   - Visualization of approximations of the data- and attribution-space side-by-side with linked brushing, hover tooltips, and tabular display of selected points.
   - Evaluating the local explanations with use of the radial tour we can explore the sensitivity to the structure identified better see what variable support the explanation holds realistic.


## Thesis structure

The remainder of the thesis is organized as follows: Chapter \ref{ch:motivation} covers the motivation for such research. The research chapters 3 through 5 will cover their respective own backgrounds and follow-up discussion. Chapter \ref{ch:efficacy_radial_tour} discusses a user study evaluating the efficacy of the radial, manual tour. Chapter \ref{ch:cheem} extends the use of manual tours to extend the interpretability of black-box models. Lastly Chapter \ref{ch:conclusion} concludes with some take aways and discussion of possible extensions.



<!--chapter:end:01-introduction.Rmd-->

---
chapter: 2
knit: "bookdown::render_book"
---

```{r, include=FALSE}
library(spinifex)
library(gganimate)
library(ggplot2)
library(magrittr)
library(knitr)
library(kableExtra)
```

<!-- motivation or background?? -->
# Motivation {#ch:motivation}

The following chapter discusses the current research in two primary areas: dynamic linear projections (collectively known as tours) and multivariate data visualization in stereoscopic 3D. After each section, research gaps are highlighted.


<!-- better than numerical summarization  -->
Visualization is much more robust than numerical summarization alone [@anscombe_graphs_1973; @matejka_same_2017]. In these studies the data have the same summary statistics, yet contains obvious visual trends and shapes that could go completely unheeded if plotting is foregone. Data visualization is fundamental to EDA and quickly evaluating data supports ensuring that models are suitability.

(ref:matejka17fig-cap) Starting with the datasarus plot, observations are allows to drift (by iterated simulated annealing) toward 12 patterns provided that they stay close to the originally listed statistics [@matejka_same_2017].

```{r matejka17fig, echo=F, out.width='70%', fig.cap = "(ref:matejka17fig-cap)"}
knitr::include_graphics("./figures/matejka17fig.png")
```


## Tours, animated linear projections {#sec:tour}

The introduction established that visualizing data-space is an important aspect of exploratory data analysis and data analysis in general. Yet, there is an inherent difficulty as the dimensionality of data increases. In univariate data sets histograms or smoothed density curves are employed to visualize data. In bivariate data, $XY$ scatterplots and contour plots (2D density) can be employed. In three dimensions, the two most common techniques are 3D scatterplot\footnote{Graphs depicting three dimensions are typically viewed on a 2D display, in print or with a standard monitor. These are 2D images of monocular 3D spaces, sometimes referred to as 2.5D data visualization, more on this in section \ref{sec:3d-terminology}.} or 2D scatterplot with the 3rd variable as an aesthetic (such as color, size, or height). These aesthetic cues convey some information but are not a sufficient replacement for axes for use with continuous variables.

As dimensionality of the data, $p$, increases the visualization of data-space quickly becomes complex. It is common that visualizing data-space is dropped in favor of graphing model-space (for example residuals), parameter-space (in fewer dimensions), or worse yet: long tables of statistics without visuals [@wickham_visualizing_2015]. To preserve the visualization of data-space, a solution that scales with the dimensionality of data is needed; this is where dimensionality reduction comes in. This work will focus on a group of dynamic linear projection techniques collectively known as *tours*. The scope of the project lies within the dynamic linear projections; a broader review of dimensionality reduction techniques is discussed in the review paper by  @grinstein_high-dimensional_2002. Tours are used for a couple of salient features: the use of linear projections maintains transparency back to the original variable space (which non-linear projections lose) and the use of many projections shows more variation than a single linear projection. Employing the breadth of tours extends the dimensionality of visualizations, and with it, the intrinsic understanding of the structure and distribution of data that is more succinct or beyond the reach of summary statistics alone.

Let $p$ be the dimensionality of the data, and $d$ be the dimension of the projection space. Tours perform linear dimensionality reduction, orthogonally projecting $p$-space down to $d(\leq p)$ dimensions. Many such projections are interpolated, each making small rotations in $p$-space. These frames are then viewed in order as an animation of the lower dimensional embedding changing as the original variable space is manipulated. Shadow puppets offer a useful analogy to aid in conceptualizing tours. Imagine a fixed light source facing a wall. When an object is introduced it projects a 2D shadow onto the wall. This is a physical representation of a simple projection, that from $p=3$ down to $d=2$. If the object rotates then the shadow correspondingly changes. Observers watching only the shadow are functionally watching a 2D tour as the 3D object is manipulated. Some orientations explain more information about the shape of the object than others while watching an animation of the shadow changing gives a more robust understanding than looking at any one frame. More complex structures generally require more time to comprehend the nature of the geometry. These features hold for tours as well.

*An extended tour notation is listed in the appendix section \ref{sec:notation}.*

<!-- ### Notation {#sec:terminology_tours} -->
<!-- Notation moved to appendix A in 90-appA.Rmd. -->

### History

The first tour was introduced was the *grand tour* in @asimov_grand_1985 at the Stanford Linear Accelerator, Stanford University. Asimov suggested three types of grand tours: torus, at-random, and random-walk. The original application of tours was performed on high energy physics on the PRIM-9 system.

Before choosing projection paths randomly, an exhaustive search of $p-$space was suggested by @mcdonald_interactive_1982, also at the Stanford Linear Accelerator. This was later coined *little tour*.
<!-- Buja and Asimov were in the acknowledgments. -->

@friedman_projection_1974 and later @huber_projection_1985 recommended projection pursuit (also referred to as PP). Projection pursuit optimizes an objective function before it removes a single component/variable and then iterates in this newly embedded subspace. Within each subspace, the projection seeks for a local extremum via a hill climbing algorithm on an objective function. This formed the basis for *guided tours* suggested by @hurley_analyzing_1990.

The grand and little tours have no input from the user aside from the starting basis. Guided tours allow for an index to be selected. The bulk of tour development since has largely been around the dynamic display, user interaction, geometric representation, and application.
<!--Note c2 paper, pointing to: Buja & Asimov 1986, Hurley & Buja 1990, Wegman 1991, Cook, Buja, Cabrera, & Hurley 1995, Buja, Cook Asimov & Hurley 1997,Cook & Buja 1997. -->


### Path generation {#sec:path_generation}

A fundamental aspect of tours is the path of rotation. There are four primary distinctions of tour path generation [@buja_computational_2005]: random choice, data-driven, precomputed choice, and manual control.

* Random choice, *grand tour*, constrained random walks $p$-space. Paths are constrained for changes in direction small enough to maintain continuity and aid in user comprehension
    + torus-surface [@asimov_grand_1985]
    + at-random [@asimov_grand_1985]
    + random-walk [@asimov_grand_1985]
    + *local tour* [@wickham_tourr_2011], a sort of grand tour on a leash, such that it goes to a nearby random projection before returning to the original position and iterating to a new nearby projection.
* Data-driven, *guided tour*, optimizing some objective function/index within the projection-space, called projection pursuit (PP) [@hurley_analyzing_1990], including the following indexes:
    + holes [@cook_projection_1993] - moves points away from the center.
    + cmass [@cook_projection_1993] - moves points toward the center. 
    + lda [@lee_projection_2005] - linear discriminant analysis, seeks a projection where 2 or more classes are most separated.
    + pda [@lee_projection_2010] - penalized discriminant analysis for use in highly correlated variables when classification is needed.
    + convex [@laa_using_2019] - the ratio of the area of convex and alpha hulls.
    + skinny [@laa_using_2019] - the ratio of the perimeter distance to the area of the alpha hull.
    + stringy [@laa_using_2019] - based on the minimum spanning tree (MST), the diameter of the MST over the length of the MST.
    + dcor2D [@grimm_mbgraphic:_2017; @laa_using_2019] - distance correlation that finds linear and non-linear dependencies between variables.
    + splines2D [@grimm_mbgraphic:_2017; @laa_using_2019] - measure of non-linear dependence by fitting spline models.
    + other user-defined objective indices can be applied to the framework provided in the *tourr* package @wickham_tourr_2011.
    + Another data-drive tour is the *dependence tour*, a combination of $n$ independent 1D tours. A vector describes the axis each variable will be displayed on. for example $c(1, 1, 2, 2)$ is a 4- to 2D tour with the first 2 variables on the first axis, and the remaining on the second.
        - *correlation tour* [@buja_data_1987], a special case of the dependence tour, analogous to canonical correlation analysis.
* Precomputed choice, *planned tour*, in which the path has already been generated or defined.
    + *little tour* [@mcdonald_interactive_1982], where every permutation of variables is stepped through in order, analogous to brute-force or exhaustive search.
    + a saved path of any other tour, typically an array of basis targets to interpolate between.
* Manual control, *manual tour*, a constrained rotation on selected manipulation variable and magnitude [@cook_manual_1997]. Typically used to explore the local area after identifying an interesting feature, perhaps via guided tour.


<!-- ### Interpolation -->

<!-- NOTE: Search 'Interpolator' in Wickhamâ€™s tourr paper--> 
<!-- After target bases are identified, the frames in-between need to be filled in. There are several methods to do so: -->

<!-- * Geodesic - via Gram-Schmidt process -->
<!-- * Givens rotations -->
<!-- * Householder reflections -->

### Path evaluation

Consider projection down to 2D, then each projection is called a 2-frame (each spanning a 2-plane). Mathematically, a Grassmannian is the set of all possible unoriented 2-frames in $p$-space, $\textbf{Gr}(2,~p)$. @asimov_grand_1985 pointed out that the unique 2-frames of the grand tour approaches $\textbf{Gr}(2,~p)$ as time goes to infinity. The *density* of a tour is defined as the fraction of the Grassmannian explored. Ideally, an exploring tour will be dense, but the time taken to become dense vastly increases as variable space increases dimensionality. *Rapidity* is then defined as how quickly a tour encompasses the Grassmannian. Due to the random selection of a grand tour, it will end up visiting homomorphisms of previous 2-frames before all unique values are visited, leading sub-optimal rapidity. 

The little tour introduced in @mcdonald_interactive_1982, on the other hand, is necessarily both dense and rapid, performing essentially an exhaustive search on the Grassmannian. However, this path uninteresting and with long periods of similar projections strung together. The Grassmannian is necessarily large and increases exponentially at the rate of $p$. Viewing of the whole Grassmannian is time-consuming, and interesting projections are sparse, there was a clear space for computers to narrow the search space.

Guided tours [@hurley_analyzing_1990] optimize an objective function generating path will be a relatively small subset of the Grassmannian. As they are not used for exploration, density and rapidity are poor measures. On the other hand, they excel at finding interesting projections quickly. Recently, @laa_using_2019, compared projection pursuit indices with the metrics: *smoothness, squintability, flexibility, rotation invariance* and *speed*. 


### Geometric display by dimensionality {#sec:geom_display}

Up to this point, 2D scatterplots have been the primary display discussed, they offer a logical display for viewing embeddings of high-dimensional point clouds. However, other geometrics offer perfectly valid projections as well.

* 1D geometrics (geoms)
    + 1D densities: such as histogram, average shifted histograms [@scott_averaged_1985], and kernel density [@scott_incorporating_1995].
    + image (pixel): [@wegman_pixel_2001].
    + time series: where multivariate values are independently lagged to view peak and trough alignment. Use case is discussed in [@cook_manual_1997]. 
* 2D geoms
    + 2D density (available on GitHub at https://github.com/nspyrison/tourr)
    + $XY$ scatterplot
* 3D geoms
    + Anaglyphs, sometimes called stereo, where red images are positioned for the left channel and cyan for the right when viewed with corresponding filter glasses give a perception of depth to the image.
    + Depth, which gives depth cues via aesthetic mappings, most common size and/or color of data points.
* $d$-dimensional geoms
    + Andrews curves [@andrews_plots_1972], smoothed variant of parallel coordinate plots, discussed below. 
    + Chernoff faces [@chernoff_use_1973], variables linked to the size of facial features. The idea is to apply the human facial-perception for rapid, cursory, like-ness comparisons between observations.
    + Parallel coordinate plots [@ocagne_coordonnees_1885; wegman_hyperdimensional_1990], where any number of variables are plotted in parallel with observations linked to their corresponding variable value by polylines.
    + Scatterplot matrix (SPLOM) [@chambers_graphical_1983], showing a triangle matrix of bivariate scatterplots with 1D density on the diagonal.
    + Radial glyphs, radial variants of parallel coordinates including radar, spider [@duffin_spiders:_1994], and star glyphs [@siegel_surgical_1972]. 

### Tour software

Tours have yet to be widely adopted, due in part, to the fact that print and static .pdf output does not accommodate dynamic viewing. Conceptual abstraction and technically density have also hampered user growth. Due to low levels of adoption and the rapid advancement of technology support and maintenance of such implementations give them a particularly short life span. Despite the small user base, there have been a fair number of tour implementations, including:
<!-- See Wickhamâ€™s thesis and C2 paper for partial lists. -->

* spinifex [github.com/nspyrison/spinifex](https://github.com/nspyrison/spinifex) -- R package, all platforms.
* tourr [@wickham_tourr_2011] -- R package, all platforms.
* CyrstalVision [@wegman_visual_2003] -- for Windows.
* GGobi [@swayne_ggobi:_2003] -- for Linux and Windows.
* DAVIS [@huh_davis:_2002] -- Java based, with GUI.
* ORCA [@sutherland_orca:_2000] -- Extensible toolkit built in Java.
* VRGobi [@nelson_xgobi_1998] -- for use with the C2, tours in stereoscopic 3D displays.
* ExplorN [@carr_explorn:_1996] -- for SGI Unix.
* ExploRe [@hardle_xplore:_1995]
* XGobi [@swayne_xgobi:_1991] -- for Linux, Unix, and Windows (via emulation).
* XLispStat [@tierney_lisp-stat:_1990] -- for Unix and Windows.
* Explor4 [@carr_explor4:_1988] -- Four-dimensional data using stereo-ray glyphs.
* Prim-9 [@asimov_grand_1985;@fisherkeller_prim-9:_1974] -- on an internal operating system.


### Research gaps

Dynamic projections offering UCS have not incorporated recent animation frameworks (**RO #1**). This leaves the class of dynamic linear projections without the most precise, fine-scale control of rotating $p$-space. This should be implemented with an eye on extensibility and maintainability.

A comparative study outlining the benefits of UCS vs alternatives is also absent from the literature (**RO #2**). The benefits of dynamic linear projections hold in theory, but a direct comparison with popular alternatives should be made. Barriers to adoption and sharing should also be kept in mind as the dynamic display is not easy to display on print and in .pdf documents.


## Multivariate data visualization in 3D {#sec:3d}

The scope of this research pertains to numeric multivariate data, a wider overview of 3D data visualization is discussed in chapter 2 of @marriott_immersive_2018. Terminology for 3D visuals is in the glossary section \ref{sec:terminology}.


### A rocky start

Scientific visualization has readily adopted mixed realities as a large amount of the science exists in three spatial dimensions, lending itself well to virtual immersion [@marriott_immersive_2018]. Data visualization, on the other hand, has been slow to utilize graphics above 2.5D, (and haptic interaction) primarily due to the mixed results of over-hyped of 3D visuals from the 1980s and '90s [@munzner_visualization_2014]. Since then, however, there have been several promising studies suggesting that it is time for data visualization to revisit and adopt the use of 3D visuals for specific combinations of displays and depth cues.


### 3D rotated projections vs 3 2D orthogonal projections 

Three orthogonal 2D views can represent the three face-on views of 3D shapes. When 3D representations are used with binocular cues, they are found to have more accurate perception than 2D counterparts [@lee_effects_1986].

Between 3D and split view 2D of three-dimensional economics, data @wickens_implications_1994 asked participants questions integrating several variables, finding that 3D visuals resulted in faster answers when questions involved three dimensions, while the speed was similar when questions involved fewer dimensions.

Using 3D rotated projections gives more precise (relative to 2D) estimates of the height a ball is suspended above complex box shapes, while combinations of 2D and 3D give the most precise orientation and positioning information [@tory_visualization_2006, depicted in figure \@ref(fig:tory06fig)].


(ref:tory06fig-cap) Screen capture from @tory_visualization_2006: "fig. 1 (a) 2D, (b) 3D Rotated, (c) 3D Shadow, (d) Orientation Icon, and (e) ExoVis displays used in Experiment 1 (position estimation). Participants estimated the height of the ball relative to the block shape. In this example, the ball is at height 1.5 diameters above the block shape."

```{r tory06fig, echo=F, out.width='50%', fig.cap = "(ref:tory06fig-cap)"}
knitr::include_graphics("./figures/tory06fig.PNG")
```


@sedlmair_empirical_2013, depicted in figure \@ref(fig:sedlmair13fig), asked users about cluster separation across 2D scatterplots, 2D scatterplot matrices (SPLOMs) and interactive 3D scatterplots, PCA (linear projection), and t-SNE (non-linear projection) as viewed in monocular 3D from a standard monitor. They conclude that interactive 3D scatterplots perform worse for class separation. This result is surprising as the extra dimension theoretically allows for the clustering structure to be seen and explored more clearly.


(ref:sedlmair13fig-cap) Screen capture of "figure 5. example of a mesh display" from @sedlmair_empirical_2013: "fig. 5. (a)-(d): Screenshots of the entangled dataset `entangled1-3d-3cl-separate` designed to show the most possible benefits for i3D. (a),(b) two viewpoints of the same i3D PCA scatterplot. An accompanying video shows the full 3D rotation. (c) 2D PCA projection. (d) t-SNE untangles this class structure in 2D. (e)-(f): 2D scatterplots of the reduced `entangled2-15d-adjacent` dataset which we designed to have a ground truth entangled class structure in 15D. (e) Glimmer MDS cannot untangle the classes, neither can PCA and robPCA (see supplemental material). (f) t-SNE nicely untangles and separates the ground truth classes in 2D."

```{r sedlmair13fig, echo=F, out.width='100%', fig.cap = "(ref:sedlmair13fig-cap)"}
knitr::include_graphics("./figures/sedlmair13fig.PNG")
```


### Comparing 3D and 2D embeddings of multivariate data

@nelson_xgobi_1998, depicted in figure \@ref(fig:nelson98fig), had $n=15$ participants perform brushing and identification tasks (of clusters, structure, and data dimensionality) in 3D with head-tracked binocular VR. 3D proved to have a substantial advantage for cluster identification and some advantage in identifying the shape. Brushing did take longer in VR, perhaps due to the lower familiarity of manipulating 3D spaces. 


(ref:nelson98fig-cap) Screen capture from @nelson_xgobi_1998: "figure 4: This is a picture of a 3-D room, running VRGobi. Data is plotted in the center, with painting tools to the right and variable spheres to the left. In the viewing box, the data can be seen to contain three clusters, and one is being brushed."

```{r nelson98fig, echo=F, out.width='70%', fig.cap = "(ref:nelson98fig-cap)"}
knitr::include_graphics("./figures/nelson98fig.PNG")
```


Another study, @gracia_new_2016 performed dimensionality reduction down to 2- and 3D scatterplots, both displayed in monocular 3D on a standard monitor. Users were found to more accurately compare distances between points and identify outliers on 3D scatterplots. However, both tasks were performed slower with the use of the 3D scatterplots and statistical significance was not reported.


@wagner_filho_immersive_2018, depicted in figure \@ref(fig:wagner18fig), performed an $n=30$ empirical study on PCA embedded projections, measuring perception error across 4 tasks and 3 display types: 2D, static 3D, and immersive 3D. Overall task error was less in static and immersive 3D relative to 2D. According to the user Likert-scale survey, 2D is slightly easier to navigate and slightly more comfortable, while, static and immersive 3D displays are slightly easier to interact with and moderately easier to find information on.

(ref:wagner18fig-cap) Screen capture from @wagner_filho_immersive_2018, original captions contained in the capture.

```{r wagner18fig, echo=F, out.width='50%', fig.cap = "(ref:wagner18fig-cap)"}
knitr::include_graphics("./figures/wagner18fig.PNG")
```


### Immersive analytics platform in VR

Immersive analytics is an emerging field, where data visualization and analysis is facilitated in an intuitive, immersive virtual reality environment [@chandler_immersive_2015; @cordeil_immersive_2017]. An example of which is shown in @cordeil_imaxes:_2017 introduces a collaborative space for immersive data analysis. Where axes are displayed and intuitively interacted with while responding to proximity to other variable axes and popping into place changing the resulting geometric display. For example, three variables can be placed as the $x,~y,~z-$ axes for a 3D scatterplot or stood up right next to each other for a parallel coordinate plot. The subsequent work in @cordeil_iatk:_2019 builds from the previous reference and refines it for the next iteration in interactive, scalable data visualization in virtual spaces.


(ref:cordeil2017fig-cap) Screen capture of figure 15 from @cordeil_imaxes:_2017.

```{r cordeil2017fig, echo=F, out.width='50%', fig.cap = "(ref:cordeil2017fig-cap)"}
knitr::include_graphics("./figures/cordeil2017fig.PNG")
```


### Research gaps

When comparing between 2- and 3D orthogonal views, in general, studies show that perception accuracy is better in 3D, though manipulation speed is generally slower. The speed discrepancy is confounded by the difference in users familiar with manipulating 2D vs 3D spaces [@lee_effects_1986; @wickens_implications_1994; @tory_visualization_2006; counterexample @sedlmair_empirical_2013]. 

Similar results have been shown in static, 3D projected spaces [@gracia_new_2016; @wagner_filho_immersive_2018] and in dynamic 2D embedded spaces depicted in immersive 3D [@nelson_xgobi_1998]. Modern VR hardware brings about a steady improvements to quality, resolution while driving down cost, making VR more easily accessible than ever. It's timely to review dynamic 3D projections and do so in immersive spaces to quantify the corresponding benefits (**RO #3 & 4**).

<!--chapter:end:02-motivation.Rmd-->

---
chapter: 3
knit: "bookdown::render_book"
---
```{r, include=FALSE}
## Original R journal paper:
if(F)
  file.edit("../spinifex_paper/paper/spyrison-cook.Rmd")
## Creating figures in script (not ran upon compilation)
if(F)
  file.edit("./figures_from_script/_ch3_spinifex.r")
```

# spinifex: An R Package for Creating a Manual Tour of Low-dimensional Projections of Multivariate Data {#ch:spinifex}

TODO:XXX Segue. Bring over abstract?

## Introduction

<!-- Tours -->
Exploring multivariate spaces is a challenging task, increasingly so as dimensionality increases. Traditionally, static low-dimensional projections are used to display multivariate data in two dimensions including principal component analysis, linear discriminant spaces or projection pursuit. These are useful for finding relationships between multiple variables, but they are limited because they show only a glimpse of the high-dimensional space. An alternative approach is to use a tour [@asimov_grand_1985] of dynamic linear projections to look at many different low-dimensional projections. Tours can be considered to extend the dimensionality of visualization, which is important as data and models exist in more than 3D. The package {tourr} [@wickham_tourr:_2011] provides a platform for generating tours. It can produce a variety of tours, each paired with a variety of possible displays. A user can make a grand, guided, little, local or frozen tour, and display the resulting projected data as a scatterplot, density plot, histogram, or even as Chernoff faces if the projection dimension is more than 3.

<!-- Manual tour -->
This work adds a manual tour to the collection. The manual tour was described in @cook_manual_1997 and allows a user to control the projection coefficients of a selected variable in a 2D projection. The manipulation of these coefficients allows the analyst to explore their sensitivity to the structure within the projection. As manual tours operate on only one variable at a time, they are particularly useful once a feature of interest has been identified. 

<!-- interesting features and guided tours -->
One way to identify "interesting" features is with the use of a guided tour [@cook_grand_1995]. Guided tours select a very specific path, which approaches a projection that optimizes an objective function. The optimization used to guide the tour is simulated annealing [@kirkpatrick_optimization_1983]. The direct optimization of a function allows guided tours to rapidly identify interesting projection features given the relatively large parameter-space. After a projection of interest is identified, an analyst can then use the "finer brush" of the manual tour to control the contributions of individual variables to explore the sensitivity they have on the structure visible in the projection.

<!-- Paper outline -->
The paper is organized as follows. Section \ref{sec:algorithm} describes the algorithm used to perform a radial manual tour as implemented in the package {spinifex}. Section \ref{sec:display} explains how to generate an animation of the manual tour using the animation frameworks offered by {plotly} [@sievert_interactive_2020] and {gganimate} [@pedersen_gganimate_2020]. Package functionality and code usage following the order applied in the algorithm follows in section \ref{sec:usage}. Section \ref{sec:application} illustrates how this can be used for sensitivity analysis applied to multivariate data collected on high energy physics experiments [@wang_mapping_2018]. Section \ref{sec:discussion} summarizes this paper and discusses potential future directions.


<!-- Algorithm outline -->
## Algorithm {#sec:algorithm}

The algorithm to conduct a manual tour interactively, by recording mouse/cursor motion, is described in detail in @cook_manual_1997. Movement can be in any direction and magnitude, but it can also be constrained in several ways:

- *radial*: fix the direction of contribution, and allow the magnitude to change.
- *angular*: fix the magnitude, and allow the angle or direction of the contribution to vary.
- *horizontal*, *vertical*: allow rotation only around the horizontal or vertical axis of the current 2D projection.

The algorithm described here produces a **radial** tour as an *animation sequence*. It takes the current contribution of the chosen variable, and using rotation brings this variable fully into the projection, completely removes it, and then returns to the original position.


### Notation

The notation used to describe the algorithm for a 2D radial manual tour is as follows:

- $\textbf{X}$, the data, an $n \times p$ numeric matrix to be projected to 2D.
- $\textbf{B} = (B_1,~ B_2)$, any 2D orthonormal projection basis, $p \times 2$ matrix, describing the projection from $\mathbb{R}^p \Rightarrow \mathbb{R}^2$. This is called this the "original projection" because it is the starting point for the manual tour.
- $k$, is the index of the variable to manipulate, called the "manip var".

- $\textbf{e}$, a 1D basis vector of length $p$, with 1 in the $k$-th position and 0 elsewhere.
- $\textbf{M}$ is a $p \times 3$ matrix, defining the 3D subspace where data rotation occurs and is called the manip(ulation) space.
- $\textbf{R}$, the 3D rotation matrix, for performing unconstrained 3D rotations within the manip space, $\textbf{M}$.
- $\theta$, the angle of in-projection rotation, for example, on the reference axes; $c_\theta, s_\theta$ are its cosine and sine.
- $\phi$, the angle of out-of-projection rotation, into the manip space; $c_\phi, s_\phi$ are its cosine and sine. The initial value for animation purposes is $\phi_1$.
- $\textbf{U}$, the axis of rotation for out-of-projection rotation orthogonal to $\textbf{e}$.
- $\textbf{Y}$, the resulting projection of the data through the manip space, $\textbf{M}$, and rotation matrix, $\textbf{R}$.

<!-- operate on bases -->
The algorithm operates entirely on projection bases and incorporates the data only when making the projected data plots, in light of efficiency.


### Steps

#### Step 0) Setup

<!-- describe data. -->
The flea data (@lubischew_use_1962), available in the {tourr} package, is used to illustrate the algorithm. The data contains 74 observations on 6 variables, which are physical measurements made on flea beetles. Each observation belongs to one of three species.

<!-- Projection basis -->
An initial 2D projection basis must be provided. A suggested way to start is to identify an interesting projection using a projection pursuit guided tour. Here the holes index is used to find a 2D projection of the flea data, which shows three separated species groups. Figure \ref{fig:ch3fig1} shows the initial projection of the data. The left panel displays the projection basis ($\textbf{B}$) and can be used as a visual guide of the magnitude and direction that each variable contributes to the projection. The right panel shows the projected data, $\textbf{Y}_{[n,~2]} ~=~ \textbf{X}_{[n,~p]} \textbf{B}_{[p,~2]}$. The color and shape of points are mapped to the flea species. This plot is made using the `view\_basis()` function in {spinifex}, which generates a {ggplot2} [@wickham_ggplot2_2016] object. 

```{r ch3fig1, fig.cap = "Biplot of the initial 2D projection: representation of the basis (left) and resulting data projection (right) of standardized flea data. The color and shape of data points are mapped to the species of flea beetle. The basis was produced by a projection pursuit guided tour, with the holes index. The contribution of the variables aede2 and tars1 approximately contrasts the other variables. The visible structure in the projection are the three clusters corresponding to the three species."}
knitr::include_graphics("./figures_from_script/ch3_fig1_biplot.pdf")
```


#### Step 1) Choose manip variable

<!-- select a manip var-->
In figure \ref{fig:ch3fig1} the contribution of the variables tars1 and aede2 mostly contrast the contribution of the other four variables. These two variables combined contribute in the direction of the projection where the purple cluster is separated from the other two clusters. The variable aede2 is selected as the manip var, the variable to be controlled in the tour. The question that will be explored is: how important is this variable to the separation of the clusters in this projection?


#### Step 2) Create the 3D manip space

<!-- Zero Vect, manip sp -->
Initialize the coordinate basis vector as a zero vector, $\textbf{e}$, of length $p$, and set the $k$-th element to 1. In the example data, aede2 is the fifth variable in the data, so $k=5$, set $e_5=1$. Use a Gram-Schmidt process to orthonormalize the coordinate basis vector on the original 2D projection to describe a 3D manip space, $\textbf{M}$.

\begin{align*}
  e_k &\leftarrow 1 \\ 
  \textbf{e}^*_{[p,~1]} &= \textbf{e} - \langle \textbf{e}, \textbf{B}_1 \rangle \textbf{B}_1 - \langle \textbf{e}, \textbf{B}_2 \rangle \textbf{B}_2 \\ 
  \textbf{M}_{[p,~3]} &= (\textbf{B}_1,\textbf{B}_2,\textbf{e}^*)
\end{align*}

<!-- What the manip space provides -->
The manip space provides a 3D projection from $p$-dimensional space, where the coefficient of the manip var can range completely between [0, 1]. This 3D space serves as the medium to rotate the projection basis relative to the selected manipulation variable. Figure \ref{fig:ch3fig2} illustrates this 3D manip space with the manip var highlighted. This representation is produced by calling the `view\_manip\_space()` function. This diagram is purely used to help explain the algorithm.

```{r ch3fig2, fig.cap = "Illustration of a 3D manip space, the projection plane is shown as blue circle extending into and out of the display. A manipulation direction is initalized, the red circle, orthogonal to the projection plane. This allows the selected variable, aede2, a means to change it's contribution back to the projection plane. The other variables contributions rotate into this space as well, preserving the orthagonal structure, but are omitted in the manipulation dimension for simplicity."}
knitr::include_graphics(
  "./figures_from_script/ch3_fig2_manip_sp.pdf")
```


#### Step 3) Defining a 3D rotation

<!-- illustration of axis manip -->
The basis vector corresponding to the manip var (red line in Figure \ref{fig:ch3fig2}), can be operated like a lever anchored to the origin. This is the process of the manual control, that rotates the manip variable into and out of the 2D projection (Figure \ref{fig:ch3fig3}). As the variable contribution is controlled, the manip space rotates, and the projection onto the horizontal projection plane correspondingly changes. This is a manual tour. Generating a sequence of values for the rotation angles produces a path for the rotation of the manip space.

<!-- describe manip var path -->
For a radial tour, fix $\theta$, the angle describing rotation within the projection plane, and compute a sequence for $\phi$, defining movement out of the plane. This will change $\phi$ from the initial value, $\phi_1$, the angle between $\textbf{e}$ and its shadow in $\textbf{B}$, to a maximum of $0$ (manip var fully in projection), then to a minimum of $\pi/2$ (manip var out of projection), before returning to $\phi_1$.

<!-- define the rotation matrix -->
Rotations in 3D can be defined by the axes they pivot on. Rotation within the projection, $\theta$, is rotation around the $Z$ axis. Out-of-projection rotation, $\phi$, is the rotation around an axis on the $XY$ plane, $\textbf{U}$, orthogonal to $\textbf{e}$. Given these axes, the rotation matrix, $\textbf{R}$ can be written as follows, using Rodrigues' rotation formula (originally published in @rodrigues_lois_1840):

  \begin{align*}
    \textbf{R}_{[3,~3]} 
    &= \textbf{I}_3 + s_\phi\*\textbf{U} + (1-c_\phi)\*\textbf{U}^2 \\
        &=
    \begin{bmatrix}
      1 & 0 & 0 \\ 
      0 & 1 & 0 \\ 
      0 & 0 & 1 \\
    \end{bmatrix} +
    \begin{bmatrix}
      0 & 0 & c_\theta s_\phi \\
      0 & 0 & s_\theta s_\phi \\
      -c_\theta s_\phi & -s_\theta s_\phi & 0 \\
    \end{bmatrix} +
    \begin{bmatrix}
      -c_\theta (1-c_\phi) & s^2_\theta (1-c_\phi) & 0 \\
      -c_\theta s_\theta (1-c_\phi) & -s^2_\theta (1-c_\phi) & 0 \\
      0 & 0 & c_\phi-1 \\
    \end{bmatrix} \\
    &= 
    \begin{bmatrix}
      c_\theta^2 c_\phi + s_\theta^2 &
      -c_\theta s_\theta (1 - c_\phi) &
      -c_\theta s_\phi \\
      -c_\theta s_\theta (1 - c_\phi) &
      s_\theta^2 c_\phi + c_\theta^2 &
      -s_\theta s_\phi \\
      c_\theta s_\phi &
      s_\theta s_\phi &
      c_\phi
    \end{bmatrix} \\
\end{align*}

\noindent where

\begin{align*}
  \textbf{U} &= (u_x, u_y, u_z) =
  (s_\theta, -c_\theta, 0) \\ 
  &=
  \begin{bmatrix}
  0 & -u_z & u_y  \\
  u_z & 0 & -u_x \\
  -u_y & u_x & 0 \\
  \end{bmatrix} =
  \begin{bmatrix}
    0 & 0 & -c_\theta \\
    0 & 0 & -s_\theta \\
    c_\theta & s_\theta & 0 \\
  \end{bmatrix} \\
  \end{align*}
  
<!-- The term $(1-c_\phi)$ is used by convention, but $2sin^2(\phi/2)$ is more computationally robust. -->


#### Step 4) Creating an animation of the radial rotation

<!--  phi transform and animation -->
The steps outlined above can be used to create any arbitrary rotation in the manip space. To use these for sensitivity analysis, the radial rotation is built into an animation where the manip var is rotated fully into the projection, completely out, and then back to the initial value. This involves allowing $\phi$ to vary between $0$ and $\pi/2$, call the steps $\phi_i$. 


```{r ch3fig3, fig.cap = "Select frames highlight the animation of a radial manual tour manipulating aede2: (1) original projection, (2) full contribution, (3) zero contribution, before returning to the original original contribution."}
knitr::include_graphics(
  "./figures_from_script/ch3_fig3_filmstrip.pdf")
```


<!-- Sequence of $\phi_i$ -->
1. Set initial value of $\phi_1$ and $\theta$: $\phi_1 = \cos^{-1}{\sqrt{B_{k1}^2+B_{k2}^2}}$, $\theta = \tan^{-1}\frac{B_{k2}}{B_{k1}}$. Where $\phi_1$ is the angle between $\textbf{e}$ and its shadow in $\textbf{B}$.
2. Set an angle increment ($\Delta_\phi$) that sets the step size for the animation, to rotate the manip var into and out of the projection. (Note: Using angle increment, rather than a number of steps, to control the movement, is consistent with the tour algorithm as implemented in the {tourr}).
3. Step towards $0$, where the manip var is completely in the projection plane.
4. Step towards $\pi/2$, where the manip variable has no contribution to the projection.
5. Step back to $\phi_1$.

In each of the steps 3-5, a small step may be added to ensure that the endpoints of $\phi$ ($0$, $\pi/2$, $\phi_1$) are reached. 

#### Step 5) Projecting the data {#sec:display}

<!-- the reminder of basis operation and now apply data -->
The operation of a manual tour is defined on the projection bases. Only when the data plot needs to be made is the data projected into the relevant basis. 

\begin{align*}
  \textbf{Y}^{(i)}_{[n,~3]} &= \textbf{X}_{[n,~p]} \textbf{M}_{[p,~3]} \textbf{R}^{(i)}_{[3,3]}
\end{align*}

<!-- plot XY in seq for animation -->
\noindent where $\textbf{R}^{(i)}_{[3,3]}$ is the incremental rotation matrix, using $\phi_i$. To make the data plot, use the first two columns of \textbf{Y}. Show the projected data for each frame in sequence to form an animation. 

<!-- <!-- Animation on flea data -->
<!-- Figure \ref{<FIG_REMOVED>} illustrates a manual tour sequence having 15 steps. The projection axes are displayed on the top half, which corresponds to the projected data in the bottom half. When aede2 is removed from the projection, the purple cluster overlaps with the green cluster. This suggests that aede2 is important for distinguishing between these species.  -->

Tours are typically viewed as an animation. The animation of this tour can be viewed online at \url{https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/flea_radialtour_mvar5.gif}. The page may take a moment to load. Animations can be produced using the function `play\_manual\_tour()`.

```{r FigNotAppearingInThisProj,  fig.cap = "Radial manual tour manipulating aede2 of standardized flea data. The axis for aede2 increases in contribution to the projection, from its initial value to 1, decreasing to 0 and then returning to the initial value. This effects the separation between the purple and green clusters. This shows that aede2 is important for distinguishing the purple species, because the separation disappears when aede2 is not contributing to the projection."}
## There was a figure with 15 frame of the tour, I think leave it out for here, didn't come through very well there.
```

## Package structure and functionality

This section describes the functions available in the package, and how to use them.


### Installation

The {spinifex} is available from CRAN, and can be installed by: 
```{r installCODEEXAMPLE, eval=F, echo=T}
# Setup:
install.package("spinifex") ## Install from CRAN
library("spinifex") ## Read into session

# Getting started:
## Shiny app for visualizing basic application
run_app("intro")
## View the code vignette
vignette("spinifex_vignette")
```


### Functions

<!-- in terms of workflow -->
Table \ref{tab:functionsTable} lists the primary functions and their purpose. These are grouped into four types: construction for building a tour path, render to make the plot objects, animation for running the animation, and specialty for providing illustrations used in the algorithm description.

```{r functionsTable, echo=F}
library(kableExtra)

funcs <- rbind(
  c("construction", "create_manip_space", "forms the 3D space of rotation"),
  c("construction", "rotate_manip_space", "performs 3D rotation"),
  c("construction", "manual_tour", "generates sequence of 2D frames"),
  c("", "", ""),
  c("render", "array2df", "turn the tour path array into long form, for plotting"),
  c("render", "render_", "render long form as a ggplot2 objection for animation"),
  c("render", "render_plotly", "render the animation as a plotly object (default)"),
  c("render", "render_gganimate", "render the animation as a gganimate object"),
  c("", "", ""),
  c("animation", "play_tour_path", "composite function animating the specified tour path"),
  c("animation", "play_manual_tour", "composite function animating the specified manual tour"),
  c("", "", ""),
  c("specialty", "print_manip_space", "table of the rotated basis and manip space"),
  c("specialty", "oblique_frame", "display the reference axes of a given basis"),
  c("specialty", "view_manip_space", "illustrative display of any manip space")
)
colnames(funcs) <- c("Type", "Function", "Description")
  
kable(funcs, "latex", caption = "Summary of available functions.", booktabs = T, linesep="") 
```


### Usage {#sec:usage}

Using the flea data from the {tourr} package, we will illustrate generating a manual tour to explore the sensitivity of the cluster structure is to the variable aede2.

\noindent The `play\_manual\_tour()` function is a composite function handling interaction between `manual\_tour()`, `array2df()`, and `render\_plotly()`. This will generate an html animation using plotly. Switching the renderer to `render\_gganimate()` alternatively creates an animated gif. Each of these formats allows for the animation to be made available on a web site, or directly visible in an html formatted document.


### Making illustrations

The function `oblique_frame` can be used to show a projection of the basis, or with the data overlaid. For example, the plots in Figures \ref{fig:ch3fig1} and \ref{fig:ch3fig3} were made with code similar to this:

```{r eval=FALSE}
## View a basis and projected data 
oblique_frame(basis = f_basis, 
              data = f_data,
              color = f_clas,
              shape = f_clas)
```

\noindent An illustration of the manip space (as shown in Figure \ref{fig:ch3fig2}) is made with the `view_manip_space` function, as follows: 

<!-- step 2 disp code. -->
```{r step2CODEEXAMPLE, eval=F, echo=T}
## Displays the projection plane and manipulation space for the 
view_manip_space(basis = f_basis, 
                 manip_var = f_mvar, 
                 lab = colnames(f_data))
```


## Application {#sec:application}

<!-- Introduction of data and original paper -->
@wang_mapping_2018 introduces a new tool, PDFSense, to visualize the sensitivity of hadronic experiments to nucleon structure. The parameter-space of these experiments lies in 56 dimensions, $\delta \in \mathbb{R}^{56}$, and are visualized as 3D subspaces of the 10 first principal components in linear (PCA) and non-linear (t-SNE) embeddings. 

<!-- grand tours on the same data -->
@cook_dynamical_2018 illustrates how to learn more about the structures using a grand tour. Tours can better resolve the shape of clusters, intra-cluster detail, and better outlier detection than PDFSense & TFEP (TensorFlow embedded projections) or traditional static embeddings. This example builds from here, illustrating how the manual tour can be used to examine the sensitivity of structure in a projection to different parameters. The specific 2D projections passed to the manual tour were provided in their work. 

<!-- Data structure -->
The data has a hierarchical structure with top-level clusters; DIS, VBP, and jet. Each cluster is a particular class of experiments, each with many experimental datasets which,  each have many observations of their own. In consideration of data density, we conduct manual tours on subsets of the DIS and jet clusters. This explores the sensitivity of the structure to each of the variables in turn and we present the subjectively best and worst variable to manipulate for identifying dimensionality of the clusters and describing the span of the clusters.


### Jet cluster

<!-- jet cluster, explain dimensionality -->
The jet cluster resides in a smaller dimensionality than the full set of experiments with four principal components explaining 95% of the variation in the cluster [@cook_dynamical_2018]. The data within this 4D embedding is further subsetted, to ATLAS7old and ATLAS7new, to focus on two groups that occupy different parts of the subspace. Radial manual tours controlling contributions from PC4 and PC3 are shown in Figures \ref{fig:ch3fig4} and \ref{fig:ch3fig5}, respectively. The difference in shape can be interpreted as the experiments probing different phase-spaces. Back-transforming the principal components to the original variables can be done for a more detailed interpretation.

<!-- discussion of findings and which is more insightful -->
When PC4 is removed from the projection (Figure \ref{fig:ch3fig4}) the difference between the two groups is removed, indicating that it is important for distinguishing experiments. However, removing PC3 from the projection (Figure \ref{fig:ch3fig5}) does not affect the structure, indicating it is not important for distinguishing experiments. Animations for the remaining PCs can be viewed at the following links: [PC1](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/jetcluster_manualtour_pc1.gif), [PC2](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/jetcluster_manualtour_pc2.gif), [PC3](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/jetcluster_manualtour_pc3.gif), and [PC4](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/jetcluster_manualtour_pc4.gif). It can be seen that only PC4 is important for viewing the difference in these two experiments.

<!-- JetClusterGood -->
```{r ch3fig4, fig.cap="Select frames from a radial tour of PC4 within the jet cluster, with color indicating experiment type: ATLAS7new (green) and ATLAS7old (orange). When PC4 is removed from the projection (frame 10) there is little difference between the clusters, suggesting that PC4 is important for distinguishing the experiments."}
knitr::include_graphics("./figures_from_script/ch3_fig4_jet_better_pc4.pdf")
```
<!-- JetClusterBad -->
```{r ch3fig5, fig.cap = "Frames from the radial tour manipulating PC3 within the jet cluster, with color indicating experiment type: ATLAS7new (green) and ATLAS7old (orange).  When the contribution from PC3 is changed there is little change in the separation of the clusters, suggesting that PC3 is not important for distinguishing the experiments."}
knitr::include_graphics("./figures_from_script/ch3_fig5_jet_worse_pc3.pdf")
```


### DIS cluster

<!-- introduce DIS cluster -->
Following @cook_dynamical_2018, to explore the DIS cluster, PCA is recomputed and the first six principal components, explaining 48% of the full sample variation, are used. The contributions of PC6 and PC2 are explored in Figures \ref{fig:ch3fig6} and  \ref{fig:ch3fig7}, respectively. Three experiments are examined: DIS HERA1+2 (green), dimuon SIDIS (purple), and charm SIDIS (orange).

<!-- comparison of the DIS cluster -->
Both PC2 and PC6 contribute to the projection similarly. When PC6 is rotated into the projection, variation in the DIS HERA1+2 is greatly reduced. When PC2 is removed from the projection, dimuon SIDIS becomes more clearly distinct. Even though both variables contribute similarly to the original projection, their contributions have quite different effects on the structure of each cluster, and the distinction between clusters. Animations of all of the principal components can be viewed from the links: [PC1](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/discluster_manualtour_pc1.gif), [PC2](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/discluster_manualtour_pc2.gif), [PC3](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/discluster_manualtour_pc3.gif), [PC4](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/discluster_manualtour_pc4.gif), [PC5](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/discluster_manualtour_pc5.gif), and [PC6](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/discluster_manualtour_pc6.gif).

<!-- DISclusterGood -->
```{r ch3fig6, fig.cap = "Select frames from a radial tour exploring the sensitivity that PC6 has on the structure of the DIS cluster, with color indicating experiment type: DIS HERA1+2 (green), dimuon SIDIS (purple), and charm SIDIS (orange). DIS HERA1+2 is distributed in a cross-shaped plane, charm SIDIS occupies the center of this cross, and dimuon SIDIS is a linear cluster crossing DIS HERA1+2. As the contribution of PC6 is increased, DIS HERA1+2 becomes almost singular in one direction (frame 5), indicating that this cluster has very little variability in the direction of PC6."}
knitr::include_graphics("./figures_from_script/ch3_fig6_DIS_better_pc6.pdf")
```

<!-- DISclusterBad -->
```{r ch3fig7, fig.cap = "Frames from the radial tour exploring the sensitivity PC2 to the structure of the DIS cluster, with color indicating experiment type: DIS HERA1+2 (green), dimuon SIDIS (purple), and charm SIDIS (orange). As contribution from PC2 is decreased, dimuon SIDIS becomes more distinguishable from the other two clusters, indicating that in its absence PC2 is important for separating this cluster from the others."}
knitr::include_graphics("./figures_from_script/ch3_fig7_DIS_worse_pc2.pdf")
```

## Discussion {#sec:discussion}

<!-- summary of spinifex  -->
Dynamic linear projections of numeric multivariate data, tours, play an important role in data visualization; they extend the dimensionality of visuals to peek into high-dimensional data and parameter spaces. This research has taken the manual tour algorithm, specifically the radial rotation, used in GGobi [@swayne_ggobi:_2003] to interactively rotate a variable into or out of a 2D projection, and modified it to create an animation that performs the same task. It is most useful for examining the importance of variables, and how the structure in the projection is sensitive or not to specific variables. This functionality available in package {spinifex}. The work complements the methods available in the {tourr} package.

<!-- summary of application  -->
This work was motivated by problems in physics, and thus the usage was illustrated on data comparing experiments of hadronic collisions, to explore the sensitivity of cluster structure to different principal components. These tools can be applied quite broadly to many multivariate data analysis problems.

<!-- Constraints -->
The manual tour is constrained in the sense that the effect of one variable is dependent on the contributions of other variables in the manip space. However, this can be useful to simplify a projection by removing variables without affecting the visible structure. Defining a manual rotation in high dimensions is possible using Givens rotations and Householder reflections as outlined in @buja_computational_2005. This would provide more flexible manual rotation, but more difficult for a user because they have the choice (too much choice) of which directions to move. 

<!-- 3D and function vis -->
Another future research topic could be to extend the algorithm for use on 3D projections. With the current popularity and availability of 3D virtual displays, this may benefit the detection and understanding of the higher dimensional structure, or enable the examination of functions.

<!-- dynamic interaction/gui -->
Having a graphical user interface would be useful for making it easier and more accessible to a general audience. This is possible to implement using {shiny} [@chang_shiny_2020]. The primary purposes of the interface would be to allow the user to interactively change the manip variable easily, and the interpolation step for more or less detailed views.


## Acknowledgments

This article was created in R, using {knitr} [@xie_knitr_2020] and {rmarkdown} [@allaire_rmarkdown_2020], with code generating the examples inline. The source files for this article be found at [github.com/nspyrison/spinifex_paper/](https://github.com/nspyrison/spinifex_paper/). The animated gifs can also be viewed at this site, and also in the supplementary material for this paper. The source code for the {spinifex} package can be found at [github.com/nspyrison/spinifex/](https://github.com/nspyrison/spinifex/).

<!-- Adds a bib section at the end of every chapter -->


<!--chapter:end:03-spinifex.Rmd-->

---
chapter: 4
knit: "bookdown::render_book"
editor_options:
  chunk_output_type: console
---

# The benefit of user-controlled radial tour for understanding variable contributions to structure in linear projections {#ch:efficacy_radial_tour}

<!-- current draft title: -->
<!-- # A study examining the benefit of the -->
<!-- user-controlled radial tour for -->
<!-- understanding variable contributions to -->
<!-- structure visible in linear projections of -->
<!-- high-dimensional data -->

TODO:XXX Segue. Bring over abstract?

## Introduction

<!-- Multivariate spaces and EDA - black-box models-->
Multivariate data underlies most classification problems. Yet exploratory data analysis [EDA, @tukey_exploratory_1977] of such spaces is difficult, increasingly so as dimension increases, and often leads to the consideration of models for these problems being considered to be black-boxes. There is increasing emphasis on the need to provide explainers to improve the interpretability for black-box models [@biecek_dalex_2018; @biecek_explanatory_2021; @lundberg_unified_2017; @ribeiro_why_2016; @wickham_visualizing_2015]. Visualization is an important part of providing interpretations [@anscombe_graphs_1973; @coleman_geometric_1986; @goodman_dirty_2008; @matejka_same_2017]. Tour methods [@lee_review_2021; @cook_grand_2008] provide ways to visualize linear projections of high-dimensional spaces, to obtain an overview of shape (distributions and associations) and anomalies (outliers, clusters). The recently introduced radial tour [@spyrison_spinifex_2020] provides a user-controlled manual rotation of variables into and out of a projection, which might especially be useful for studying variable importance.


<!-- Research gap -->
Dimension reduction is commonly used in conjunction with visualization to provide informative low-dimensional summaries of high-dimensional data. There have been several user studies
for dimension reduction comparing across embeddings and display dimensionality [@gracia_new_2016; @wagner_filho_immersive_2018]. There are also empirical metrics and comparisons used to describe non-linear reduction and how well and faithfully they embed the data [@bertini_quality_2011; @liu_visualizing_2017; @sedlmair_empirical_2013; @van_der_maaten_visualizing_2008]. There is an absence of studies comparing techniques for assessing variable importance, particularly, how best to convey information to the viewer.

<!-- Overview of the study -->
This paper describes a user study conducted to assess the benefit of the radial tour, in comparison with principal component analysis and a grand tour for understanding variable importance. The experiment is a within-participant user study. The type of visualization is the primary factor of the study, corresponding to a null hypothesis that all techniques provide a similar ability for the user to determine variable importance. The techniques are compared by having subjects complete several tasks, where accuracy and speed are recorded. 

<!-- Structure of the paper -->
The paper is structured as follows. Section \@ref(sec:background) provides the background on the visualization methods being compared. Section \@ref(sec:userstudy) describes the user study, the tasks, evaluation, and measures used. The results of the study are in Section \@ref(sec:results). Conclusions and potential future directions are discussed in Section \@ref(sec:conclusion). The software used for the study is described in Section \@ref(sec:spinifex).


## Background {#sec:background}

### Principal component analysis

Principal component analysis is a good baseline of comparison for linear projections because of its frequent and broad use across disciplines. Principal component analysis [PCA, @pearson_liii._1901] creates new components that are linear combinations of the original variables. The creation of these variables is ordered by decreasing variation which is orthogonally constrained to all previous components. While the full dimensionality is intact, the benefit comes from the ordered nature of the components. The first 2 or 3 components are typically used to approximate the variation multivariate data set, while the rest are discarded.


```{r ch4fig1, fig.cap = "Scatterplot matrix of the first 3 principal components for simulated data. Traditional multivariate visualization of *data*-space involes static display of the first several principal components."}
  knitr::include_graphics(
    "./figures_from_script/ch4_fig1_pca_splom.pdf")
```


### Scatterplot matrix

An extension to showing only a few components is to show pairs of components as a scatterplot matrix [@chambers_graphical_1983], where all pairs of the components are displayed on the upper or lower triangles of the matrix. This is a convenient way to fit many plots onto a page, but when there are many variables there is insufficient space on a page to display them all. Figure \@ref(fig:ch4fig1) shows the first three components of simulated data as a scatterplot matrix.


<!-- ## Parallel coordinates plot -->

<!-- <!-- PCP -->
<!-- Another common way to display multivariate data is with a parallel coordinates plot [@ocagne_coordonnees_1885], which displays observations by their quantile values for each variable with connected by lines to the quantile value in subsequent variables. This scales well with dimensionality but suffers from different interpretations depending on the order of variables on the axis. -->

<!-- <!-- complaints about pcp -->
<!-- <!--a couple of issues. The main issue is the loss of mapping multiple variables to graphic position simultaneously as the $x$-axis is being occupied by variables, which Munzner [@munzner_visualization_2014] suggests is the most important visual  channel for human perception. Another issue is that parallel coordinates are asymmetric, as their interpretation is dependent on variable ordering.-->


### Data visualization tours

<!-- tours intro -->
A data visualization _tour_ uses time to animate local changes in the projection basis. One of the key features of the tour is the object permanence of the data points; that is to say by watching nearby frames one can track the relative changes of observations as the basis moves toward the next target basis. There are various types of tours that are distinguished by the selection or generation of their basis paths [@lee_review_2021; @cook_grand_2008]. To contrast with the discrete orientations of PCA, we compare with continuous changes of linear projection with _grand_ and _radial_ tours.

#### Grand tours
<!-- Grand tour -->
In a grand tour [@asimov_grand_1985] the target bases are selected randomly. The grand tour is the first and most widely known tour. It will serve as an intermediate unit of comparison which has continuity of data points in nearby frames along with the radial tour but lacks the user control enjoyed by PCA and radial tours. This lack of control makes grand tours more of a generalist exploratory tool.


#### Radial (manual) tours

<!-- Manual tour -->
The _manual_ tour [@cook_manual_1997] defines its basis path by manipulating the basis contribution of a selected variable. A manipulation dimension is appended onto the projection plane, with a full contribution given to the selected variable. The target bases are then selected based on rotating this newly created manipulation space. The target bases are then similarly orthogonally restrained, data projected, and rendered into an animation. For the variables to remain independent of each other, the contributions of the other variables must also change, _ie._ dimension space should maintain its orthonormal structure. A key feature of the manual tour is that it affords users a way to control the variable contributions of the next target basis. This means that such manipulations can be selected and queued in advance or select on the spot for human-in-the-loop analysis [@karwowski_international_2006]. However, this navigation is relatively time-consuming due to the huge volume of $p$-space (an aspect of the curse of dimensionality [@bellman_dynamic_1957]) and the abstract method of steering the projection basis. It is advisable to first identify a basis of particular interest and then use a manual tour as a finer, local exploration tool to observe how the contributions of the selected variable do or do not contribute to the feature of interest.

<!-- Radial tour variant -->
To simplify the task and keep its duration realistic, we consider a variant of the manual tour, called a _radial_ tour. In a radial tour, the selected variable is allowed to change its magnitude of contribution, but not its angle; it must move along the direction of its original contribution radius. The radial tour benefits from both continuity of the data alongside grand tours, but also allows the user to steer via choosing the variable to rotate.

The recent implementation of manual tours us the R package {spinifex} [@spyrison_spinifex_2020], which facilitates manual tours (and radial variant). It is also compatible with tours made with {tourr} [@wickham_tourr:_2011] and facilitates exporting to .gif or .html widget, with recent graphic packages. Now that we have a readily available means to produce various tours, we want to see how they fare against traditional discrete displays commonly used with PCA.


## User study {#sec:userstudy}

<!-- Overview of visual -->
An experiment was constructed to assess the performance of the radial tour relative to the tour and scatterplots of principal components for interpreting the importance of variables to class separations.

<!-- Introduce blocks -->
The three methods were examined for three different cluster shapes, using different combinations of contributing variables, and data dimensionality. Data was collected using a specially constructed web app, through crowd-sourced with prolific.co [@palan_prolific_2018].

### Experimental factors {#sec:blocks}

<!-- Introduction to blocks -->
In addition to visual factor, we vary the data across 3 aspects: 1) The _location_ of the difference between clusters, by mixing a signal and a noise variable at different ratios, we vary the number of variables and their magnitude of cluster separation, 2) the _shape_ of the clusters, to reflect different distributions of the data, and 3) the _dimension_-ality of the data.

```{r ch4fig2, fig.cap = "Illustration of the experimental factors: visualization factor, the variable location of the cluster separationm, the shape of the clusters, and the dimensionality (and clusters) of the data."}
knitr::include_graphics("./figures_from_script/ch4_fig2_exp_factors.pdf")
```

<!-- Location mixing -->
The _location_ of the separation of the clusters is a crucial aspect of analysis, it is the variables or combination their of that is important to the explanation of the structure. To test the sensitivity to this we mix a noise-variable with the signal-containing variable such that the difference in the clusters is mixed at the following percentages: 0/100% (not mixed), 33/66%, 50/50% (evenly mixed).

<!-- Shape, vc matrix -->
In selecting the _shape_ of the clusters we follow the convention given by @scrucca_mclust_2016, where 14 variants of model families containing 3 clusters are defined. The name of the model family is the abbreviation of its respective volume, shape, and orientation of the cluster, which are either equal or vary. We use the models EEE, EEV, and EVV, the latter is further modified by moving 4 fifths of the data out in a "V" or banana-like shape. Figure \@ref(fig:ch4fig2) shows the principal component isodensity of the 3 model variants applied here.

<!-- Dimensionality -->
_Dimension_-ality is tested at 2 modest levels, namely, in 4 dimensions containing 3 clusters and 6 dimensions with 4 clusters. We must do so to bound the difficulty and search space to keep the task realistic for crowdsourcing.


### Objective {#sec:objective}

<!-- Rational for factor levels -->
PCA will be used as a baseline for comparison as it is the most common linear embedding. The grand tour will act as a secondary control that will help evaluate the benefit of animation, with the persistence of the data points across changes in basis, but without the ability to influence its path. Lastly, the radial tour should perform best as it benefits both from animation and being able to select an individual variable to change the contribution. Next we cover how we expect them to perform and state the hypothesis to test.

<!-- Prior expectations -->
Then for some subset of tasks, we expect to find that the radial tour performs most accurately, as it enjoys both the persistence of points and input control to explore specific variables. Secondly, it may be the case that grand performs faster than the alternatives with its absence of inputs, users can focus all of their attention on interpreting the fixed path. Conversely, we are less certain about the accuracy of such a limited grand tours as there is no objective function in the target bases; it is possible that, by chance, the planes completely avoid the information needed. However, given that the data dimensionality will be modest, it seems likely that grand tour regularly crosses frames with the correct information to perform the task quickly.

<!-- Explicit hypothesis tests -->
We measure the accuracy and speed over the support of the discussed experimental factors. The null hypotheses can be stated as:

$~~~~~H_0: \text{visualization factor does not impact task } \textit{accuracy}, Y_1. \\$
$~~~~~~H_0: \text{visualization factor does not impact task } \textit{speed}, Y_2. \\$


### Task and evaluation {#sec:task}

<!-- segue to task and evaluation -->
With our hypothesis formulated let's turn our attention to the task and how to evaluate it.
Recall that the display was a 2D scatterplot with axis biplot to its left. Observations were supervised with the cluster level coded by color and shape.

<!-- Geom, clusters, explicit task -->
Participants were asked to 'check any/all variables that contribute more than average to the cluster separation green circles and orange triangles', which was further explained in the explanatory video as 'mark and all variable that carries more than their fair share of the weight, or 1 quarter in the case of 4 variables'.

<!-- Instruction and video -->
The instructions iterated several times in the video was: 1) Use the input controls to find a frame that contains separation between the clusters of green circles and orange triangles, 2) look at the orientation of the variable contributions in the gray circle, a visual depiction of basis, and 3) select all variables that contribute more than average in the direction of the separation in the scatterplot. Regardless of factor and block values participants were limited to 60 seconds for each evaluation of this task.

<!-- Evaluating measure -->
The evaluation measure of this task was designed with a couple of features in mind: 1) the sum of squares of the individual variable marks should be 1, and 2) symmetric about 0, without preference to under- or over-guessing. With these in mind, we define the following measure for evaluating the task.

Let a dataset $\textbf{X}$ be a simulation containing clusters of observations of different distributions. Let $\textbf{X}_k$ be the subset of observations in cluster $k$ containing the $p$ variables.

<!-- Define accuracy measure -->
\begin{align*}
W_{j} &=\frac
{(\overline{X_{j=1, k=1}} - \overline{X_{1, k=2}}, ~...~
(\overline{X_{j=p, k=1}} - \overline{X_{j=p, k=2}})}
{\sum_{j=1}^{p}(|\overline{X_{j,k=1}} - \overline{X_{j,k=2}}|)}
- \frac{1}{p} \\
\\
\text{Accuracy}, Y &= \sum_{j=1}^{p}I(r_j) * sign(w_j) * \sqrt{|w_j|}  \\
\end{align*}

where $I$ is the indicator function. Then the total marks for this task is the sum of this marks vector. We use the time till the last response as a secondary dependent variable $Y_2$.


```{r ch4fig3, fig.cap = "(L), PCA biplot of the components showing the most cluster separation with (R) illustration of the magnitude of cluster separation is for each variable (wide bar) and the weight of the marks given if a variable is selected (red/green line). The horizontal dashed line is 1 / dimensionality, the amount of separation each variable would have if evenly distributed. The weights equal the signed square of the difference between each variable value and the dashed line."}
knitr::include_graphics("./figures_from_script/ch4_fig3_accuracy_measure.pdf")
```


### Visual design standardization {#sec:standardization}

<!-- Background for methodology, application here -->
Section \@ref(sec:background) gives the sources and a description of the visual factors PCA, grand tours, and radial manual tours. The factors are tested within-participant, with each factor being evaluated by each participant. The order that factors are experienced is controlled with the block assignment as illustrated below in Figure \@ref(fig:ch4fig4). Below we cover the visual design standardization, as well the input and display within each factor.

<!-- Aesthetic standardization -->
The visualization methods were standardized wherever possible. each factor was shown as a biplot, with variable contributions displayed on a unit circle. All aesthetic values (colors, shapes, sizes, absence of legend, and absence of axis titles) were held constant. Variable contributions were always shown left of the scatterplot embeddings with their aesthetic values consistent as well. What did vary between factors were their inputs which caused a discrete jump to another pair or principal components, were absent for the grand tour with target bases to animate through selected at random, or for the radial tour which variable should have its contribution animated.

<!-- PCA -->
PCA inputs allowed for users to select between the top 4 principal components for both the x and y-axis regardless of the data dimensionality (either 4 or 6). <!-- Grand tours -->There was no user input for the grand tour, users were instead shown a 15-second animation of the same randomly selected path. Users were able to view the same clip up to 4 times within the time limit. <!-- Radial tours -->Radial tours were also displayed at 5 frames per second within the interpolation step size of 0.1 radians. Users were able to swap between variables, upon which the display would change the start of radially increasing the contribution of the selected variable till it was full, zeroed, and then back to its initial contribution. The complete animation of any variable takes about 20 seconds and is almost fully in the projection frame at around 6 seconds. The starting basis of each is initialized to a half-clock design, where the variables were evenly distributed in half of the circle which is then orthonormalized. This design was created to be variable agnostic while maximizing the independence of the variables.


### Data simulation, task

<!-- Clusters and correlation -->
Each dimension is originally distributed as $\mathcal{N}(2 * I(signal), 1)~|~\text{covariance}~\Sigma$, a function of the shape. Signal variables have a correlation of 0.9 when they have equal orientation and -0.9 when their orientations vary. Noise variables were restricted to 0 correlation. Each cluster is simulated with 140 observations and is offset in a variable that does not separate previous variables. 
 
<!-- Apply shape and location transformations -->
Clusters of the EVV shape are transformed to the banana-chevron shape. Then location mixing is applied by post-multiplying a (2x2) rotation matrix to the signal variable and a noise variable for the clusters in question.

<!-- Preprocess and replicate and save -->
All variables are then standardized by standard deviation. The rows and columns are then shuffled randomly. The observation's cluster and order of shuffling are attached to the data and saved.
<!-- Every permutation of _location_, _shape_, and _dimension_ is replicated 3 times. -->

<!-- Iterating over factor -->
Each of these replications are then iterated with each level of the factor. For PCA, every pair of the top 4 principal components and saved as 12 plots. For the grand tour, we first save 2 basis paths (for 4 and 6 dimension), each replication is then projected through the common basis path as the variable(s) containing the were previously shuffled. The resulting animations were saved as .gif files. The radial tour starts at either the 4 or 6-variable "half-clock" basis, where each variable has a uniform contribution, and no variable contributing in the opposite direction (to minimize variable dependence), a radial tour is then produced for each variable and saved as a .gif.


### Data collection, and factor assignment

<!-- Introduction -->
Now, with simulation and their artifacts in hand. We explain how the experimental factors are assignment, and illustrate how this is experienced from a participant's perspective.

<!-- periods, block assignment -->
We section the study into 3 periods, each period is linked to a randomized level of both the  factor visualization and the location. The order of dimension and shape are of secondary interest and are held constant in increasing order of difficultly; 4 then 6 dimensions and EEE, EEV, then EVV-banana respectively.

<!-- training and evaluation -->
The period starts with an untimed training task at the simplest remaining block parameterization; location = 0/100%, shape = EEE, and 4 dimensions with 3 clusters. This serves to introduce and familiarize participants with input and visual differences. After the training, the participant is evaluated on 2 tasks with the same factor \* location level, across the increasing difficulty of dimension \* shape. These evaluations removed the plot after 60 seconds, though this limit was rarely reached by participants.

<!-- factor*location nested latin square -->
The order of the levels of the factor and location is randomized with a nested Latin square where all levels of factor are exhausted before advancing to the next level of location. That means we need $3!^2 = 36$ participants to perform a full block evaluation. This randomization is important to control for any potential learning effects the participant may receive. Figure \@ref(fig:ch4fig4) illustrates how an arbitrary participant experiences the experimental factors.

<!-- Nested latin square assignment -->
```{r ch4fig4, echo = F, out.width = '100%', fig.cap = "Illustration of the nested latin square and how a hypothetical participant 63 is assigned factor and block parameterizations. Each of the 6-factor permutations is exhausted before iterating to the next permutation of location."}
## This is a manual .pttx screen cap, .png ok.
knitr::include_graphics(
  "./figures_from_script/ch4_fig4_randomization_MANUAL.png")
```

<!-- Pilot study; 3 even evaluations of each -->
Through pilot studies sampled by convenience (information technology and statistics Ph.D. students attending Monash University), we predict that we need 3 full evaluations to properly power our study; we set out to crowdsource $N = 3 * 3!^2 = 108$ participants.


### Recruiting subjects {#sec:subjects}

We recruited $N = 108$ participants via prolific.co [@palan_prolific_2018]. We filtered participants based on their claimed education requiring that they have completed at least an undergraduate degree (some 58,700 of the 150,400 users at the time); we apply this filter under the premise that linear projections and biplot displays used will not be regularly used for consumption by general audiences. There is also the implicit filter that Prolific participants must be at least 18 years of age and location/language bias associated with. Participants were compensated for their time at \pounds 7.50 per hour, whereas the mean duration of the survey was about 16 minutes. We can't preclude previous knowledge or experience with the factors, but validate this assumption in the follow-up survey where we ask about familiarity with the factors (see Figure \@ref(fig:ch4fig6)). The appendix contains a heatmap distribution of age and education paneled across preferred pronouns of the participants that completed the survey, who are relatively young and well educated.


### Collecting participant data

<!-- app, data collection, network issues -->
Data were recorded by a {shiny} application and were written to a Google Sheet after each third of the study. Especially at the start of the study, participants experienced adverse network conditions due to the volume of participants hitting the application with modest allocated resources. In addition to this, API read/write limitations further hindered data collection. To mitigate this we throttled the volume of participant and over-collect survey trials until we had received our target 3 evaluations of our 36 permutation levels.

<!-- Preprocessing steps -->
The processing steps were minimal. First, we format to an analysis ready form, decoding values to a more human-readable state, and add a flag to indicate if the survey had complete data. We filter to only the latest 3 complete studies of each block parameterization, those which should have experienced the least adverse network conditions. Of the studies removed the bulk were partial data and a few of over sampled permutations. This brings us to the 108 studies described in the paper, from which models and aggregation tables were built. The post-study surveys were similarly decoded to human-readable format and then filtered to include only those 84 surveys that were associated with the final 108 studies.

The code, response files, their analyses, and the study application are publicly available at on GitHub \url{https://github.com/nspyrison/spinifex_study}.


## Results {#sec:results}

To recap, the primary response variable is task marks as defined in section \@ref(sec:task), and the log of response time will be used as a secondary response variable. We have 2 primary data sets; the user study evaluations and post-study survey. The former is contains the 108 trials with explanatory variables: visual factor, location of the cluster separation signal, the shape of variance-covariance matrix, and the dimension-ality of the data. Block parameterization and randomization were discussed in section \@ref(sec:blocks). The survey was completed for 84 of these 108 trials and contains demographic information (preferred pronoun, age, and education), and subjective measures for each of the factors (preference, familiarity, ease of use, and confidence).

<!-- Segue -->
Below we look at the marginal performance of the block parameters and survey responses. After that, we build a battery of regression models to explore the variables and their interactions. Lastly, we look at the subjective measures between the factors.


##### Random effect regression against marks

<!-- Note that we list the measures at the top of the Results section -->
<!-- Introduce regression model explaining marks, and the random effect term  -->
To more thoroughly examine explanatory variables, we regress against marks. All models have a random effect term on the participant, which captures the effect of the individual participant. After we look at models of the block parameters we extend to compare against survey variables. Last, we compare how adding a random effect for data and regressing against time till last response fares against benchmark models. The matrices for models with more than a few terms quickly become rank deficient; there is not enough information in the data to explain all of the effect terms. In which case the least impactful terms are dropped.

<!-- Building a battery of models -->
In building a set of models to test we include all single term models, a model with all independent terms. We also include an interaction term of factor by location, allowing for the slope of each location to change across each level of the factor, which is feasible. For comparison, an overly complex model with many interaction terms is included.

<!-- Y1 model array -->
$$
\begin{array}{ll}
\textbf{Fixed effects:}          &\textbf{Full model:} \\
\alpha                           &\widehat{Y_1} = \mu + \alpha_i + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha + \beta + \gamma + \delta &\widehat{Y_1} = \mu + \alpha_i + \beta_j + \gamma_k + \delta_l + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha * \beta + \gamma + \delta &\widehat{Y_1} = \mu + \alpha_i * \beta_j + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha * \beta * \gamma + \delta &\widehat{Y_1} = \mu + \alpha_i * \beta_j * \gamma_k + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha * \beta * \gamma * \delta &\widehat{Y_1} = \mu + \alpha_i * \beta_j * \gamma_k * \delta_l + \textbf{Z} + \textbf{W} + \epsilon
\% \end{array}
\% $$
\% $$
\% \begin{array}{ll}
\text{where } &\mu \text{ is the intercept of the model including the mean of random effect} \\
&\alpha_i \text{, fixed term for factor}~|~i\in (\text{pca, grand, radial}) \\
&\beta_j  \text{, fixed term for location}~|~j\in (\text{0\_1, 33\_66, 50\_50}) \text{ \% noise/signal mixing} \\
&\gamma_k \text{, fixed term for shape}~|~k\in (\text{EEE, EEV, EVV banana}) \text{ model shapes} \\
&\delta_l \text{, fixed term for dimension}~|~l\in (\text{4 variables \& 3 cluster, 6 variables \& 4 clusters}) \\
&\textbf{Z} \sim \mathcal{N}(0,~\tau), \text{ the random effect of participant} \\
&\textbf{W} \sim \mathcal{N}(0,~\upsilon), \text{ the random effect of simulation} \\
&\epsilon   \sim \mathcal{N}(0,~\sigma), \text{ the error of the model} \\
\end{array}
$$


```{r ch4tab1, fig.cap = "<TABLE FIGURES DFEFINED IN THE KABLE>"}
mod_comp <- readRDS("./figures_from_script/ch4_tab1_model_comp_ls.rds") ## accuracy [[1]], log time [[2]]
output <- kableExtra::kbl(
  x = mod_comp[[1]], format = "latex",
  align = c("l", rep("l", 2), rep("c", 5)),
  booktabs = TRUE, linesep = "", caption = "Model comparison of our random effect models regressing marks. Each model includes a random effect term of the participant, which explains the individual's influence on their marks. Complex models perform better in terms of R2 and RMSE, yet AIC and BIC penalize their large number of fixed effects in favor of the much simpler model containing only the visual factor.") %>%
  kableExtra::column_spec(column = 1, width = "1.5cm") %>%
  kableExtra::column_spec(column = c(2:3), width = "1cm") %>%
  kableExtra::column_spec(column = c(4:8), width = "1.5cm") %>%
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = 10)
#output
mod_comp[[1]]
```

```{r ch4tab2, fig.cap = "<TABLE FIGURES DFEFINED IN THE KABLE>"}
mod_coef <- readRDS("./figures_from_script/ch4_tab2_model_coef_ls.rds") ## accuracy [[1]], log time [[2]]
output <- kableExtra::kbl(
  x = mod_coef[[1]],format = "latex", booktabs = TRUE, linesep = "",
                caption = "The task accuracy model coefficients for $\\widehat{Y_1} = \\alpha * \\beta + \\gamma + \\delta$, with factor=pca, location=0/100, and shape=EEE held as baselines. Factor being radial is the fixed term with the strongest evidence in support of the hypothesis. When crossing factor with location radial performs worse with 33/66 percent mixing relative to the PCA with no mixing. The model fit is based on the 648 evaluations by the 108 participants.") %>%
  kableExtra::column_spec(column = 1, width = "5cm") %>%
  kableExtra::column_spec(column = 3, width = "1.6cm") %>%
  kableExtra::column_spec(column = c(2, 4:6), width = "1.4cm") %>%
  kableExtra::pack_rows("factor", 2, 3) %>%
  pack_rows("fixed effects", 4, 8) %>%
  pack_rows("interactions", 9, 12) %>%
  kable_styling(bootstrap_options = "striped", font_size = 10)
## Cause latex error in print, works in /spinifex_study/ so a latex package issue??
#output
mod_coef[[1]]
```

<!-- Conditional effects of variables -->
We also want to visually explore the conditional variables in the model. Figure \@ref(fig:ch4fig5) explores violin plots of marks by factor while faceting on the location (vertical) and shape (horizontal). Radial tends to increase the marks received, and especially so when there is no signal/noise mixing.

<!-- Violin plots and test overlay for Y1 factors -->
```{r ch4fig5, fig.cap = "Violin plots of terms of the model $\\widehat{Y_1} = \\alpha * \\beta + \\gamma + \\delta$. Overlaid with global significance from the Kruskal-Wallis test, and pairwise significance from the Wilcoxon test, both are non-parametric, ranked sum tests suitable for handling discrete data. Participants are more confident and find the radial easier to use relative to the grand tour. Participants claim low familiarity as we expect from crowdsourced participants. Radial is more preferred compared with either alternative for this task."}
knitr::include_graphics(
  "./figures_from_script/ch4_fig5_ABcd_violins.pdf")
### !!!NOTE THAT Y_2, Log time is not covered yet!!! ###
```

<!-- A LITTERAL COMMENTED OUT, COPY AND PASTE OF THE Y_2 LOG TIME CONTENT. -->
<!-- NEEDS TO BE PORTED OVER TO THESIS IF DEISERED. -->

<!-- ## Time regressing models -->

<!-- <!-- Time as secondary intrest, Y2 -->
<!-- As a secondary explanatory variable, we also want to look at time. First, we take the log transformation of time as it is right-skewed. Now we repeat the same modeling procedure, namely: 1) build a battery of all additive and multiplicative models. 2) Compare their performance, reporting some top performers. 3) Select a model to examine its coefficients. -->

<!-- <!-- Y2 model comparisons, continue to use ABcd -->
<!-- ```{r timeCompTbl, fig.cap = "Use the caption arg in kable(), not this."} -->
<!-- ## 1) Eval parameters -->
<!-- kableExtra::kbl( -->
<!--   model_comp_tbl_ls[[2]], "latex", align = c("l", rep("l", 2), rep("c", 5)),  -->
<!--   booktabs = TRUE, linesep = "", caption = "Model comparisons for log(time) models, $\\widehat{Y_2}$ random effect models, where each model includes random effect terms for participants and simulations. We see the same trade-off where the simplest factor model is preferred by AIC/BIC, while R2 and RMSE prefers the full multiplicative model. We again select the model $\\alpha * \\beta + \\gamma + \\delta$ to explore further as it has relatively high marginal $R^2$ while having much less complexity than the full model.") %>% -->
<!--   kableExtra::column_spec(column = 1, width = "1.5cm") %>% -->
<!--   column_spec(column = c(2:3), width = "1cm") %>% -->
<!--   column_spec(column = c(4:8), width = "1.5cm") %>% -->
<!--   kable_styling(bootstrap_options = "striped", font_size = 10) -->
<!-- ``` -->

<!-- <!-- Y2 coeffiecients --> 
<!-- ```{r timeCoefTbl, fig.cap = "Use the caption arg in kable(), not this."} -->
<!-- kableExtra::kbl(coef_ls[[2]], booktabs = TRUE, linesep = "", format = "latex", -->
<!--                 caption = "The log(time) model coeffients for $Y_2 = \\alpha * \\beta + \\gamma + \\delta$, with factor=pca, location=0/100, and shape=EEE held as baselines. Location=50/50 is the fixed term with the strongest evidence and takes less time. In contrast, the interaction term location=50/50:shape=EEV has the most evidence and takes much longer on average.") %>% -->
<!--   kableExtra::column_spec(column = 1, width = "5cm") %>% -->
<!--   kableExtra::column_spec(column = 3, width = "1.6cm") %>% -->
<!--   kableExtra::column_spec(column = c(2, 4:6), width = "1.4cm") %>% -->
<!--   kableExtra::pack_rows("factor", 2, 3) %>% -->
<!--   pack_rows("fixed effects", 4, 8) %>% -->
<!--   pack_rows("interactions", 9, 12) %>% -->
<!--   kable_styling(bootstrap_options = "striped", font_size = 10) -->
<!-- ``` -->
<!-- No Y2 violin plots created, theis of spinifex_study -->


### Subjective measures

<!-- Introduce subjective measures from n=84 survey results, -->
The 84 evaluations of the post-study survey also collect 4 subjective measures for each factor. Figure \@ref(fig:ch4fig6) shows the Likert plots, or stacked percentage bar plots, alongside violin plots with the same non-parametric, ranked sum tests previously used. Participants preferred to use radial for this task. Participants were also more confident of their answers and found radial tours easier to use compared with the grand tour. All factors have reportedly low familiarity something we expect from crowdsourced participants.

```{r ch4fig6, fig.cap = "The subjective measures of the 84 responses of the post-study survey, 5 discrete Likert scale levels of aggrement. (L) Likert plots (stacked percent bar plots) with (R) violin plots of the same measures. Violin plots are overlaid with global significance from the Kruskal-Wallis test, and pairwise significance from the Wilcoxon test, both are non-parametric, ranked sum tests."}
knitr::include_graphics(
  "./figures_from_script/ch4_fig6_subjective_measures.pdf")
```


## Conclusion {#sec:conclusion}

<!-- Recap study -->
Above we discussed an $n=108$, with-in participant user study comparing the efficacy of 3 linear projection techniques. The participants performed a supervised cluster task, specifically the identification of which variables contribute to the separation between 2 target clusters. This was evaluated evenly over 4 block parameterizations. In summary, we find that radial tour increases accuracy while the grand tour decreases the time it takes to perform this task. These effects are large relative to the other block parameterizations, but smaller than the random effect of the participant. Radial tour was subjectively most preferred, lead to more confidence in answers, and is easier to use than alternatives.

<!-- Discussion and going further -->
There are several ways that this study could be extended. In addition to expanding the support of the block parameterizations, more interesting directions include: type of the task, visualizations used, and experience level of the target population. It is difficult to achieve good coverage given the number of possible permutations. Be sure to step back and plan the target support of your block parameters. Keep in mind the volume and quality of responses from participants especially when crowdsourcing. These planning steps are useful for navigating when the complexity of the application details.


## Accompanying tool: radial tour application {#sec:spinifex}

To accompany this study we have produced a more general use tool to perform such exploratory analysis of high dimensional data. The R package, {spinifex}, [@spyrison_spinifex_2020] contains a free, open-source {shiny} [@chang_shiny_2020] application. The application allows users to upload, perform limited preprocessing, and interactively explore their data via interactive radial tour. The .html widget produced is a more interactive variant relative to the application in the user study. Screencaptures and more details are provided in the appendix. Data can be imported from .csv, .rds, or .rda format, and projections. Run the following R code to run the application locally.

```{r getting_started, eval=FALSE, echo=TRUE}
install.packages("spinifex", dependencies = TRUE)
spinifex::run_app("radial_tour")
```

## Acknowledgments {#sec:acknowledgments}

This research was supported by an Australian Government Research Training Program (RTP) Scholarship. This article was created in R [@r_core_team_r:_2020] and {rmarkdown} [@xie_r_2018]. Visuals were prepared with {spinifex}. All packages used are available from the Comprehensive R Archive Network (CRAN) at \url{https://CRAN.R-project.org/}. The source files for this article, application, data, and analysis can be found at \url{https://github.com/nspyrison/spinifex_study/}. The source code for the {spinifex} package and accompanying shiny application can be found at \url{https://github.com/nspyrison/spinifex/}.

<!-- !!!NO spinifex_study/paper/appendix.rmd content was migrated. -->

<!-- If you are looking for a vignette to reproduce animations in for a pdf document from R with base or {gganimate} graphics, see the reproducible example: \url{https://github.com/nspyrison/spinifex_study/blob/master/zDevExamples/animated_pdf.rmd}. -->

<!-- Adds a bib section at the end of every chapter -->

<!--chapter:end:04-efficacy_radial_tour.Rmd-->

---
chapter: 5
knit: "bookdown::render_book"
---
```{r, include=FALSE}
## Most recent content in `cheem_paper`:
```

# Application of the radial tour to model-agnostic local explanations {#ch:cheem}

TODO:XXX Segue.

```{r, echo = TRUE,eval = TRUE}
knitr::opts_chunk$set(
   ### TODO: XXX REMOVE ME FOR CHAP 5 IMAGES ---
  eval = FALSE)

"Error on first evalauated include_image, yet, directory clearly sees them in the right spot..."
dir("./figures_from_script/")
```

## Introduction {#sec:intro}
<!-- WHAT TOPICS -->

<!-- multivariate vis, tours -->
This work is concerned with linear projections of multivariate data. More specifically, we focus on the class of visualizations known as _tours_ [@cook_grand_2008; @lee_review_2021]. Tours are viewed near-continuously through small changes to the projection basis. There are many variants of tours. We focus on one of the variants, _radial tours_ [@cook_manual_1997; @spyrison_spinifex_2020]. Radial tours can be used to control or steer the projection basis, while other tour variants select bases randomly or optimize an objective function. Because of this unique attribute, _user interaction_ is another key aspect of interest in this work.


<!-- radial tour, RO#1 -->
Radial tours change the basis by selecting one variable and specifying how to change its contribution to the current projection. By controlling the contribution of a single variable, a user can explore its sensitivity to the structure of the projection and identify which variables are ultimately most important to the structure in question. In the work addressing RO#1, we improve upon the geodesic interpolator for use in the radial tour, and apply it in an open-source `R` package, `spinifex`, this package facilitates making radial tours and extends the graphics packages interoperability for itself and the tours made from the existing package `tourr`.

<!--user study, radial tour, RO#2 -->
Next, we substantiated the efficacy of radial tours as compared with user-selected, discrete combinations of principal components [@pearson_liii._1901] and continuous projections without interaction with the _grand tour_ [@asimov_grand_1985]. We conducted an $N=108$ within-participant user study, where all participants use each of these visual factors. This is performed over balanced trials across the other experimental factors: location, shape, and dimension of the data. This addresses the second research objective.

<!-- XAI, SHAP values, and RO#3 -->
In our latest work, we want to see if we can apply the radial tour to aid the interpretability of black-box models. Such models have a non-linear space making them hard to interpret or understand. One recent branch in explainable artificial intelligence [XAI, @adadi_peeking_2018, @arrieta_explainable_2020] is the use of local explanations or the attribution of the variables for one observation of a black-box model. We use such an explanation to form a 1D basis and perform radial tours to explore how the SHAP values behave differently for misclassified observations against neighboring correctly classified observations. This work corresponds to the third research objective.


## Motivation
<!-- WHY/IMPORTANCE OF THESE TOPICS -->

<!-- EDA, visuals are important, numeric summarization insufficient -->
The term exploratory data analysis (EDA) was coined by @tukey_exploratory_1977, who leaves it as an intentionally broad term that encompasses the initial summarization and visualization of a data set before a testing hypothesis has been formulated. This is a critical first step for understanding and becoming familiar with data and validating model assumptions. It may be tempting to review a series of summary statistics to check model assumptions. However, there are known datasets where the same summary statistics miss glaringly obvious visual patterns [@anscombe_graphs_1973; @matejka_same_2017]. It is easy to look at the wrong, or incomplete set of statistics needed to validate assumptions. Data visualization is crucial in EDA, it _forces_ you to see details and peculiarities of the data which are opaque to numeric summarization, or more nefariously, obscure their true values. Data visualization does and must remain a primary component of data analysis and model validation.

<!--interaction, wants citations --> studies/support -->
While static documents are the norm, there are sizable benefits of user interaction. Interactive data visualization shift the locus of control back to the user, inviting them to explore and interact with the data, and offers a compact way to explore a wider range of dimensions, questions as they arise, which helped to keep the curiosity and the interest of the user.

<!-- Black box, interprebility vs accuracy -->
With the emerging field of XAI, the constant tension between the interpretability of a model and its predictive power is receiving more attention. Linear models are the champions of interpretability with modest accuracy while increasing complex models improve accuracy but they can scarcely be interpreted even by experienced practitioners. One way to gain insight into a model is to focus on the local vicinity of one observation, and explain the variable weighting around that location, in an agnostic non-linear model. We call this observation level variable weights a _local explanation_. There are various such local explanations, many are tied to specific classes of models, while others are model-agnostic.

<!-- summary -->
We know that data visualization is important in EDA and assumption validation. User interaction allows us to explore widely and quickly while allowing us to explore ideas as they arise. These 2 elements were especially important to consider from the work addressing RO#1 as it forms a foundation to build on in further work. The efficacy of radial tours was supported by a user study in response to the second objective. In the latest work, we apply tours in tandem with local explanations to extend the interpretability of black-box models, to address RO#3.

## Extending the interpretation of black-box models with the use of interactive continuous linear projections

Local explanations describe the linear variable weights in the vicinity of an observation in $p-dim$ space. Local explanations are point-measurements of the weights for each variable describing the importance to the prediction at that point. In a highly non-linear space, the importance of the variables may change rapidly with even a small change in the explanatory variables. There are several *model-agnostic* local explanations such as LIME [@ribeiro_why_2016], and SHAP [@lundberg_unified_2017]. In practice, this application can be used with any model and compatible local explanation. However, below we apply and discuss SHAP values extracted from a random forest model.

#### How a SHAP value is calculated

We use FIFA soccer data [@leone_fifa_2020] to explain SHAP values. We use 5000 player-observations of 9 aggregate skill measures to predict that player's wages. We use SHAP to observe how the skill attribution changes for different players, which should reflect different fielding positions. Fielding position is not explicitly used in the model, but we would expect that SHAP finds a way to change the variable weights across differing skill distributions to improve the accuracy of its wage predictions.

We have trained a random forest model and wish to further explore the weightings of this non-linear model. Following the work in [@biecek_dalex_2018; @biecek_explanatory_2021] we can similarly extract SHAP values, highlighting that different skills are valued differently across player positions within the model. We also show "break down" profiles, that is additive prediction explanations, how much of each player's predicted wages is added by each of the skill evaluations. Figure \@ref(fig:ch5fig1) takes a look at the SHAP and break down profiles of a star offensive and defensive player.

```{r ch5fig1, echo=FALSE, out.width = "100%", fig.align='center', fig.cap = "SHAP values and prediction explanations of an offensive player (Messi, top) and a defensive player (van Dijk). SHAP values show a change in weights at the location of each player. Break down profiles show one order-sensitive explanation for the prediction of that observation."}
dir(".")
dir("./figures_from_script/")
#knitr::include_graphics("./figures_from_script/ch5_fig1_shap_distr_bd.pdf")
```

<!-- Explanation of the wage, SHAP, and breakdown profiles. -->
At the top of figure \@ref(fig:ch5fig1) we see the relative wages of the 2 different players. SHAP uses permutations of the X variables to approximate the variable weights. In the middle, we see the distributions of the variable weightings for each player across 25 such permutations. We use the median of those distributions (large dot) as the SHAP value. We see a difference in the weights across the 2 players that makes sense considering their positions. Reaction skills are important for both players, while offensive and movement are weighted higher of the offensive player. Conversely, defensive, power, and accuracy skills are weighted higher for the defensive player. The bottom used the SHAP values to explain their relative wage predictions. We see a similar trend of the SHAP values additively explaining the difference of the global intercept to their predictions. We would expect that other offensive players have explanations closer to Messi and defensive players to lie closer to van Dijk's. The local explanations of middle fielders and goalkeepers would be expected to be different from both of these.


#### Our application; Trees of Cheem

<!-- transistion to simulated data -->
Above, the FIFA data was used to regress continuous wages. In introducing this application, `Trees of Cheem`, we will be discussing the classification of simulated data. We simulation of 3 spherical clusters on the vertices of a triangle. The difference between the clusters is contained in the first 2 dimensions with no mean separation in another 2 noise dimensions. After extracting all observations' SHAP values, forming a SHAP _matrix_, of the original dimensionality, $(n~\times~p)$.

<!-- global view; pc1:2 approximation -->
We want to show a global view of the SHAP matrix and show how its sensitivity differs from that of the original data space. In figure \@ref(fig:ch5fig2) we create an approximate of the data- and SHAP-spaces  with their first 2 principal components.

This gives an orientation showing where selected observation lies. We facilitate exploration and observation identification by adding a hovering tooltip displaying row number and class (actual & predicted) with linked brushing highlighting selected points and displaying their data tabularly below the plot.

```{r ch5fig2, echo=FALSE, out.width = "100%", fig.align='center', fig.cap = "The first two principal conompents of the data- and SHAP-spaces (left and right respectively) of simulated data. The points are colored and shaped according to their predicted class, misclassified points are identified with a red circle. A target observation '*' is shown in contrast to a comparison point 'x', containing a nearby value in data space, but quite different SHAP space. These same 2 points are tracked in the proceeding tour."}
knitr::include_graphics(
  "./figures_from_script/ch5_fig2_global_space.pdf")
```

Looking at the SHAP space (right) the bulk of the correctly classified points are clustered in relatively small areas. This means that the distribution of their SHAP values is quite similar; the model is selecting a very tight variable contribution to explain the predicted class of each observation. Conversely, misclassified points tend to lie in between 2 clusters of correctly classified points. Using the interactive brushing and hover tooltips we confirm that these points lie between the actual and predicted classes.

Given the global view shown in figure \@ref(fig:ch5fig2), we want to look at the local explanation of primary and comparison points (highlighted as '*/x'). In this case, the primary observation is misclassified while the comparison point is a correctly classified nearby point. These 2 points that are classified differently, but otherwise have very similar values in the explanatory variables, lie quite far apart in SHAP space.

```{r ch5fig3, echo=FALSE, out.width="100%", fig.align='center', fig.cap="Top: 1D projection basis, the normalized SHAP values of the select observation are shown as grey/black bars. The distribution of the other observations is shown with parallel coordinate lines. The primary and comparison points ('*/x' previous) are shown as bolder dashed lines and fainter dotted lines respectively. Bottom: the 1D density and rug marks of the current projection basis."}
knitr::include_graphics(
  "./figures_from_script/cheem_initial_bas.pdf")
```

Now we have the global context and a comparison of the sensitivity of the SHAP spaces let's turn more toward the target observations. Figure \@ref(fig:ch5fig3) highlights the same selected points, bolder dashed and fainter dotted lines (previously '*/x' points respectively). The top of the figure shows the selected observation's SHAP values as grey/black bars. The other values of the SHAP matrix are displayed as short vertical bars and joining parallel coordinate lines and are similarly colored with their predicted class. The majority of the easy-to-classify points form a relatively tight spread in the first 2 variables (signal) and more chaotic behavior in the two noise variables as we would expect. Our dotted comparison lies with the bulk of the purple class. The dashed primary point is actually a green point misclassified as a purple, its weight for V1 looks like a green point, but its V2 weight deviates and is much closer to that of a purple. V2 is the variable that is crucial to the explanation of the model for why this particular point is misclassified. In the absence of V2, our target observation is weighted much closer to the that of its true class. This is the variable we want to explore the structure of in the radial tour as indicated with the black contribution bar.

```{r ch5fig4, echo=FALSE, out.width = "100%", fig.align='center', fig.cap = "The first frame of the radial tour. The SHAP values of the selected observation set the initial basis, shown as the grey and black bars on top. Within class distributions of the SHAP values are shown as parallel coordinate plots next to the variable contributions. The class densities and observation positions of the 1D projection are shown on the bottom. The tour animates over small changes in the basis (top bars) as the variable with the largest contribution (weight) is rotated to have a full contribution, zero contribution, and then back to the initial contribution. Variable 2 is critical to this local explanation; In the initial contribution, both selected observations are in the middle of the purple. With a full contribution of V2 typical of most purple, our primary observation is toward the tail. When there is no contribution of variable 2, the selected observation is in the middle of the green distribution, its true class."}
knitr::include_graphics(
  "./figures_from_script/ch5_fig4_cheem_endpts.pdf")
```

Now that we have identified that V2 is the variable that deviated most from its true peer observation, we select it to rotate with the radial tour. In figure \@ref(fig:cheemTour) illustrates the difference in the extremes of the contribution. The tour starts at the original contributions as described by the SHAP values (left frame). As the tour gives V2 a full contribution (middle frame) the comparison lies in the middle of the purples while the target observation is in the tails of both the purple and green (true class). In the right-most frame, the contribution from variable 2 is zero and the target variable lies in the middle of its true class. That is to say, without the contribution of variable 2 the local explanation does not seem reasonable; the selected observation looks much more like that of its true class.

This application is still maturing and has mostly focused on classification tasks. We have partial implementation of regression which will be demonstrated in the presentation, time permitting. This work is being written up to be submitted to the WHY-21 workshop, part of the NeurIPS 2021 Conference.

#### Discussion

We have used radial tours to improve the interpretability of black-box models. We display 2D approximations of the data to keep the global context in mind and facilitate the selection of points to focus on. After a target point have been identified, its SHAP values are used as the initial projection basis. Where the within-class distributions of the value values are displayed. The default variable selected contains the largest difference from the median of SHAP values within that point's actual classification. That is that variable that deviates the most from what would be expected from a typical correctly classified observation.

Taking a step back, it is important to remember that this visualization and analysis is sensitive to the model and local explanation; it describes them as they are, independent of their validity and quality. The insight gleaned by this is predicated on meaningful selection of model and explanation.

Turning for a minute to real-world application, developing methods to better interpret black-box models is an important and impactful challenge. Private corporations and nation-states increasingly using complex models to classify and predict their customers and citizens from loans and insurance claims to employment and credit scores, the real-world impact of highly non-linear is here to stay. Being able to see the specifics of one observation may seem small in the context of a model, but it is crucial to that instance; if misclassificated, that  observation may receive an outcome a world apart from its actual peers.


# Acknowledgements {#sec:acknowledgements}

I would like to thank Professor Przemyslaw Biecek (Warsaw University of Technology) for his time and input in suggesting to look at SHAP local explanations and try applying to the FIFA dataset.

This research was supported by an Australian government Research Training Program (RTP) scholarship. This article was created in `R` [@r_core_team_r:_2020] and `rmarkdown` [@xie_r_2018].

For transparency and reproducibility, the source files are made available at [github.com/nspyrison/phd_milestones](https://github.com/nspyrison/phd_milestones).

<!-- Adds a bib section at the end of every chapter -->

<!--chapter:end:05-cheem.Rmd-->

---
chapter: 6
knit: "bookdown::render_book"
---
```{r, echo = TRUE,eval = TRUE}
## At one point chap5/cheem images we not working make sure the stop gap is reverted here.
knitr::opts_chunk$set(
  eval = TRUE)

"Error on first evalauated include_image, yet, directory clearly sees them in the right spot..."
dir("./figures_from_script/")
```

# Conclusion {#ch:conclusion}

TODO: XXX

<!-- Adds a bib section at the end of every chapter -->

<!--chapter:end:06-conclusion.Rmd-->

---
knit: "bookdown::render_book"
---

\appendix

# Glossary {#ch:glossary}



## Notation {#sec:notation}

Tour notation varies across articles and authors. In my work, I use the following:

* $n$, number of observations in the data.
* $p$, number of numeric variables, the dimensionality of data space.
* $d$, the dimensionality of projection space.
* $\textbf{X}_{[n,~p]}$, a data matrix in variable-space, $\textbf{X} \in \mathbb{R}^{p}$. Typically centered, scaled, and optionally sphered.
* $\textbf{A}_{[p,~d]}$, an orthonormal matrix (columns are independant, at right angles with eachother and each normalized to a length of 1). This is the linear combination of variables from the orignal space to a lower embedding space. That is, the orientation of the variables, mapping from   $p-$ to $d-$space.
* $\textbf{Y}_{[n,~d]}$, projected data matrix in projection-space, $\textbf{Y} \in \mathbb{R}^{d}$.
    * In chapter 
* Reference axes, a display showing how each variables coefficient(s) contribute to a projection. Either on its own axis (1D) or relative to a unit circle (2D).
* Geometric objects are referred to in generalized dimensions; the use of the term plane is not necessarily a 2D surface, but a hyperplane in the arbitrary dimensions of the projection space.


## Data visualization terminology {#sec:terminology}

* 2D - representation of data in 2 dimensions, without the use of depth perception cues and minimal aesthetic mapping (such as color, size, and height) to data points.
* 2.5D - following the definition given in @ware_designing_2000: visualizations that are essentially 2D but select depth cues are used to provide some suggestion of 3D. However, the term 2.5D is commonly used for several meanings *due to the ambiguous use of 2.5D, this document errs on the side stating 3D with descriptions of depth cues used*.
* 3D - visualizations of 3 dimensions with liberal use of depth cues unless otherwise qualified.
* Depth perception cues - an indication that indicates the depth to an observer, including:
    * linear perspective - the property of parallel lines converging on a vanishing point.
    * aerial perspective - objects that far away have lower contrast and color saturation due to light scattering in the atmosphere.
    * occultation (or interposition) - where closer objects partially block the view of further objects.
    * motion perspective/parallax - closer objects, move across the field of view faster than further objects.
    * accommodation - the change of focal length due to change in the shape of the eye. Effective for distances of less than 2 meters.
    * binocular stereopsis/disparity - the use of 2 images of slightly varied angles from the horizontal distance of the eyes. The disparity for distant objects is small, but it is significant for nearby objects.
    * binocular convergence - The ocular-motor cue due to stereopsis focusing on the same objects. Convergence is effective for distances up to 10 meters.
* Virtual reality (VR) - an immersive experience of computer-generated sensory input.
* Augmented reality (AR) - view of physical spaces with augmenting/ supplementing sensory input of information.
* Mixed reality (MR) - a mix of physical and virtual realities with objects from both interacting in real time. This differs from AR by the flow of interaction; AR augments physical reality while MR has reciprocating interactions.
* Extended reality (XR) - any degree of virtual, augmented, or mixed reality.
* Scatterplot matrices (SPLOMs) - matrix display of pair-wise 2D scatterplots, sometimes with 1D density on the diagonal.
* Human-in-the-loop - any model that requires human interaction [@karwowski_international_2006].

<!--chapter:end:90-appA.Rmd-->

