---
output:
  pdf_document: default
  html_document: default
---
```{r include=FALSE}
knitr::opts_chunk$set(
  echo  = FALSE, 
  cache = FALSE,
  cache.lazy = FALSE,
  out.width = "100%",
  out.extra = '',fig.show='hold',fig.align='center'
)
```

`r if (knitr::is_html_output()) '<!--'`

# Copyright notice {- #ch-copyright}

\textcopyright { }Nicholas Spyrison (\number\the\year).

I certify that I have made all reasonable efforts to secure copyright permissions for third-party content included in this thesis and have not knowingly added copyright content to my work without the owner's permission.

\newpage

`r if (knitr::is_html_output()) '-->'`

<!--chapter:end:00-a-compyright.Rmd-->

<!-- Short abstract for MGRO/Bridges: -->
<!--
Data visualization is an important aspect of Exploratory Data Analysis. As the dimensionality of data increases so does the difficulty of visualizing them. Techniques such as principal component analysis (PCA). Traditional linear dimension reduction defines new components which are viewed as discrete pairs. Tours are a class of linear projections animating over continuous changes to the basis. I create an R package facilitating the manual tour and exporting of all tours. Then I prove the radial tour out performs PCA and the grand tour with an $n=108$ within-participant user study. Local explanations of a non-linear model approximate the variable importance within the vicinity of one observation. I create another R package to explore and evaluate the explanation by exploring its variable sensitivity to changes with the radial tour.
-->

# Abstract {-}

Visualizing data space is crucial to exploratory data analysis, checking model assumptions, and validating model performance. Yet visualization quickly becomes difficult as the dimensionality of the data or features increases. Traditionally, static, low-dimensional linear embeddings are used to mitigate this complexity. Such embedded space is a lossy approximation of the full space. However, viewing many embeddings with interactive supporting views improves the information conveyed. Data visualization _tours_ are a class of dynamic linear projections that animate many linear projections over small changes to the projection basis.

Tours are categorized by the path of their bases. _Manual tours_ allow for User-Controlled Steering (UCS) of bases path, where the contributions of individual variables can be controlled. I create a free, open-source __R__ package, __spinifex__, that facilitates creating such tours. These manipulations of the variables can be precomputed or made in real-time. Display composition and exportable formats are interoperable with the existing package __tourr__ and can be rendered with recent graphics interfaces __plotly__ and __gganimate__. The former offers interactive use with HTML widgets, while the latter can render to static formats such as .gif or .mp4 files. An interactive application is included to illustrate the use of the manual tour.

Theoretically, the UCS of the manual tour should enable an analyst to better explore the variable attribution to the structure identified in an embedding. I find strong evidence to support that radial tour leads to a significant and large increase in accuracy as compared with Principal Component Analysis (PCA) and an alternative tour variant, the grand tour. I conduct a within-participant, crowd-sourced user study comparing these visualization methods. Each of the $n=108$ participants performs two trials with each visual for 648 total trials. The task is a supervised classification problem asking participants which variables attribute to the separation of 2 clusters.

Modern modeling techniques are sometimes referred to as black-box models due to the uninterpretable nature of model terms which are often many and non-linear. Recent research in Explainable Artificial Intelligence (XAI) tries to bring interpretability to these models through  _local explanations_. Local explanations are a class of techniques that approximate the linear-variable importance at one point in the data. I build an __cheem__, an __R__ package to streamline model and local explanation preprocessing, and two novel visuals. First the global view pits the data- and explanation-spaces side-by-side with a residual plot. Interactive features such as linked bushing and tooltips aid in identifying observations to compare. The local explanation from the selected observation becomes a basis used in creating the radial tour. This allows an analyst to evaluate the explanation and explore the sensitivity of the explanation to changes in the variable contribution.

<!-- 
The following line is required to re-set page numbering after preliminary material. Do not remove
-->
\clearpage\pagenumbering{arabic}\setcounter{page}{0}

<!--chapter:end:00-b-abstract.Rmd-->

`r if (knitr::is_html_output()) '<!--'` 

# Declaration {-}

I hereby declare that this thesis contains no material which has been accepted for the award of any other degree or diploma at any university or equivalent institution and that, to the best of my knowledge and belief, this thesis contains no material previously published or written by another person, except where due reference is made in the text of the thesis.

Three publications are included in this thesis, one of which has been accepted for publication in _The R Journal_ [@spyrison_spinifex_2020]. The user study described in Chapter \ref{ch-efficacy_radial_tour} will shortly be submitted to the *Journal of Data Science, Statistics, and Visualisation*. The research covered in Chapter \ref{ch-cheem} will shortly be submitted to *IEEE Transactions on Knowledge and Data Engineering*.

All of the papers in the thesis were conceptualized, developed, and written by myself, the student, while working in the Department of Human-Centered Computing under the supervision of professor Kimbal Marriott and professor Dianne Cook. When co-authors are listed, it indicates that the work was produced through active collaboration between researchers and that their contributions to team-based research have been recognized. The following table details the work, including my and my fellow co-authors' contributions.

\begin{table}
\centering\footnotesize\tabcolsep=0.12cm
\begin{tabular}{|p{1cm}|p{2cm}|p{1.5cm}|p{3.5cm}|p{3.5cm}|p{1.5cm}|}
\hline
\RaggedRight\textbf{Thesis Chapter}  &
\RaggedRight\textbf{Publication Title}  &
\RaggedRight\textbf{Status (published, in press, accepted or returned for revision)}  & \RaggedRight\textbf{Nature and} {\%} \RaggedRight\textbf{of student contribution} & \RaggedRight\textbf{Co-author name(s) Nature and} {\%} \RaggedRight\textbf{of Co-author’s contribution} &
\RaggedRight\textbf{Co-author(s), Monash student Y/N} \\ \hline
3 & spinifex: an R package for creating
user-controlled animated linear projections & 75\%. Concept, development, writing first draft and software development & (1) Dianne Cook, concept, methodology, writing 25\% & N \\ \hline
4 & The benefit of user-controlled radial tour for understanding variable contributions to structure in linear projections & To be submitted & 80\%. Concept, development, writing first draft, application, and conducting user study & (1) Dianne Cook, Concept, methodology, editing 12\% (2) Kimbal Marriott, concept, methodology, editing 8\% & N \\ \hline
5 & Interrogating the linear variable importance of local explanations of non-linear models with animated linear projections & To be submitted & 85\%. Concept, development, writing first draft and software development & (1) Dianne Cook, methodology, editing 10\% (2) Kimbal Marriott, methodology, editing 5\% & N \\ \hline
\end{tabular}
\end{table}

I have renumbered and updated sections of submitted or published papers to generate a consistent presentation within the thesis.

\textbf{Student name:} Nicholas Spyrison

\textbf{Student signature:}

```{r, out.width = "50%", fig.align = "left"}
nsSafeIncGraphic("./figures/ch0_signature.PNG")
```

\textbf{Date:} `r as.Date("2021-12-12")`

\vspace{2em}
\noindent
The undersigned hereby certify that the above declaration correctly reflects the nature and extent of the student's co-authors' contribution to this work.
In instances where I am not the responsible author I have consulted with the responsible author to agree on the respective contributions of the authors.

\textbf{Main Supervisor name:} Kimbal Marriott

\textbf{Main Supervisor signature:}

\vspace*{2cm}

\textbf{Date:} `r as.Date("2021-12-12")`


`r if (knitr::is_html_output()) '-->'`

<!--chapter:end:00-c-declaration.Rmd-->

# Acknowledgments  {-}

<!-- Supervisors -->
I would like to express my sincere gratitude to my supervisors, professor Kimbal Marriott and professor Dianne Cook, for their support of my Ph.D studies and research, their subject expertise, and their careers of supervising and teaching in addition to performing research. Thank you for continuously pushing me and my research to new levels. I have enjoyed teaching data visualization, which has been strongly shaped by Di's practical, data-first, visualization-often pragmatic approach. I will continue to hearing Kim's persistent question "what do we learn from this?" as a reminder not to get lost in the details of implementation, but also to regularly step back and analyze if this is the correct object to change.

<!-- Biecek -->
A special thanks to professor Przemyslaw Biecek for giving his input in the formulation stages of my project and his easy-to-understand explanations of complex modeling aspects. Thank you for responding to cold emails and being available for collaboration.

<!-- PhD students, promodo, Ying -->
I thank my fellow Ph.D students, lab members, and Pomodoro partners especially on the occasional of stimulating discussions and for the positive peer pressure of knowing others are around and working. Thanks immensely to those who empathized with me and others, especially through the hardships of studies and COVID-19. Gratitude to Ying Zhou for her enduring support through the thick of my studies and wavering mental health.

<!-- Family -->
Last but not least, I would like to thank my parents, Doug and Terry, for their support and concerns at odd hours of the day. Thanks to Alan and Claire for their companionship and support now and in our more formative years. I am looking forward to seeing you all in person shortly.

<!--chapter:end:00-d-acknowldge.Rmd-->

# Preface {-}

This thesis has been written using R Markdown with the bookdown package [@xie_bookdown:_2016].  All materials required to compile the thesis are available at https://github.com/nspyrison/thesis_monash_phd. 
<!-- The renv package has been used to create a reproducible environment to build the thesis from source (Ushey, 2019). -->
<!-- An online version of the thesis is available to read at https://sa-lee.github.io/thesis. -->

I recognize that terminology is often overburdened by ambiguous use, with several changing meanings coming from different fields. My background comes from Statistics and I will default to terms from statistics and geometry.
<!-- I provide terminology and notation definition upon first use and in the Glossary \ref{ch-glossary}. -->

<!-- a much abreviated declaration -->
Chapter \ref{ch-spinifex} has been published in _The R Journal_. Chapters \ref{ch-efficacy_radial_tour} and \ref{ch-cheem} have not been submitted yet.


<!--chapter:end:00-e-preface.Rmd-->

---
chapter: 1
knit: "bookdown::render_book"
---

# Introduction {#ch-introduction}

<!-- Exploratory data analysis -->
Exploratory Data Analysis (EDA) is the process of the initial summarization and visualization of a dataset. This is a critical first step of checking for realistic values, finding improper data formats, and revealing insights [@tukey_exploratory_1977]. Early and frequent data visualization is key to the data analyst's workflow cycle [@wickham_r_2017]. As modern datasets grow in complexity use of multivariate data is ubiquitous. The number of variables in the data increases the difficulty of creating and portraying an accurate representation of the data, which is central to the iterated workflow.

(ref:ch1fig1-cap) Data analysis workflow [@wickham_r_2017]. This work focuses primary on multivariate data visualization, but also facilitates transforming data and interpretation of models.

```{r ch1fig1, echo=F, out.width='100%', fig.cap = "(ref:ch1fig1-cap)"}
nsSafeIncGraphic("./figures/ch1_fig1_data_analysis_workflow.PNG")
```

Early attempts at high-dimension visuals in scatterplot matrices [@chambers_graphical_1983] and parallel coordinate plots [@ocagne_coordonnees_1885]. Principal component analysis (PCA, @pearson_liii._1901) reorients the basis to components, linear combinations of the original variables ordered by descending amount of variation explained. These components are typically viewed as discrete orthogonal pairs.

A data visualization _tour_ [@cook_grand_2008; @lee_state_2021] is a class of linear projections that animate small changes to the projection basis. Intuitively, they show a projection of the data object down to lower-dimensional space in the same way that a 3D object casts a 2D shadow. A key feature of the tour is the persistence and tractability of the points through many frames. In the shadow analogy an object, such as a bar stool may be casting a circular shadow that could come from any number of shapes. However, if the stool were rotated, the legs would show in the shadow quickly giving an intuitive interpretation of the object. Similarly, looking at a data object over the rotation of its variables yields information about its structure. In this way and aided with linked brushing and other interactions, such continuous tours excel at extending the interpretability of multivariate spaces. Chapter \ref{ch-background} walks through some of the previous alternatives and why we ultimately continue with this class of animated linear dimension reduction.

Component spaces and tours optimize for specific  objectives or are selected randomly. None of the previous methods allow for an analyst to control the contributions of variables to the basis. This is a crucial component for _human-in-the-loop_ analysis [@karwowski_international_2006]. If the analyst wants to explore what happens to the structure if one variable were removed or contribution of another was increased they had no way to do so.


## Research questions
<!-- Hypothesis statement -->

The over-arching question of interest can be stated as:

**Can the radial tour with user interaction help analysts understand linear projections, and explore the sensitivity of structure in the projection to the variables contributing to the projection?**

Understanding variable sensitivity to the structure in a projection frame is crucial factor analysis after identifying a feature of interest. If an identified feature is the _what_ of an analysis then its variable attributions are its _how_. The user interaction afforded by the radial tour should allow for a more precise exploration of this structure.

RQ 1. **How do we define user interaction for the radial tours to add and remove variables smoothly from a 2D linear projection of data?**\
Neither component spaces nor grand tour provide a means for changing the contributions of a desired variable. To facilitate variable interaction, we need to have a means of manipulating the basis. This is crucial to explore the sensitivity of the variable contributions to the structure in a frame.

<!---@cook_manual_1997 described an algorithm for manually controlling a tour, to rotate a variable into and out of a 2D projection. This algorithm provides the start to user-controlled radial tours. The work [@spyrison_spinifex_2020] was adapted so that the user has more control of the interpolation. The user can set the full range of the contribution from $[-1, 1]$, and output to a device that allows the user to reproduce motions and animate or rock the rotation backward and forwards. These fine-tuned controls provide a better tool for sensitivity analysis. --->

RQ 2. **Does the use of the radial tour improve analysts understanding of the relationship between variables and structure in 2D linear projections?**\
How can we be sure that having a means of control leads to a meaningfully better analysis? Comparing alternative methods should lend insight into which methods perform well under specific tasks.
<!-- We performed an $N=108$, within-participant user study comparing accuracy and time with the primary factor as the type of data visualization. Each participant performed 2 evaluations with either Principal Component Analysis (PCA, which is user control, but discrete), grand tour (continuous interpolated frames, but lack user control), or radial tour (user control and continuous animation). We find strong evidence that the radial tour increases accuracy. We also show the effects from the other experimental factors of location, shape data dimensionality, and the random effects from the data and that of the participants. -->

RQ 3. **Can the radial tours be used in conjunction with the local explanation, SHAP, to improve the interpretability of black-box models?**\
The tension from the trade-off between accuracy and interpretability of black-box models is rising. There is a clear need to be able to explain black-box models. After extrapolating local explanations of all observations, we want to explore that space with the help of the radial tour.


## Methodology

The research corresponding with RQ #1 entails _algorithm/software design_ adapting the algorithm from @cook_manual_1997. This allows for interactive control of 2D projections and serves as a foundation for the remaining work.

To address RQ #2, a controlled _experimental study_ has explored the efficacy of interactive radial tours as compared with two benchmark methods: Principal Component Analysis (PCA, @pearson_liii._1901) and the grand tour [@asimov_grand_1985]. This was a within-participant user study where each participant experienced each visual. Trials were balanced across three other experimental factors: location of the signal, the shape of the cluster distributions, and the dimensionality of the data. Ethics approval was obtained for this work. The approval and disclosure forms are available at the project repository: https://github.com/nspyrison/spinifex_study/tree/master/ethics.

The research for RQ #3 involves _fundamental visualization design_. We know that the SHAP value is a local explanation for one observation. This SHAP value will also serve as the 1D basis for the radial tour. While using SHAP as a projection basis is novel, it is not particularly insightful by itself. With the radial tour we can change variable contributions. We also provide linkage information, the within-class distributions of the SHAP components as parallel coordinate marks on the basis. We also offer a global view and related statistics to map to color to aid in exploring the sensitivity of the SHAP-space relative to the sensitivity of the original data space.


## Contributions

1. __spinifex__, a package offering a consistent framework for performing radial tours and the rendering of any tours to various formats:
   - Perform radial tours to explore the variable sensitivity to structure
   - Transform numeric variable in the data
   - Extract various bases exposing features of the data
   - Interoperable with tours generated with __tour__ [@wickham_tourr:_2011]
   - Layered interface for producing tours that mirror the layered approach of __ggplot2__ [@wickham_ggplot2_2016]
2. A user study comparing the efficacy of the radial tour against the alternatives of PCA the grand tour includes:
   - Creation of supervised classification task to assess the variable attribution to the separation of clusters performed under various levels of experimental factors: location, shape, and dimensionality.
   - Definition of an accuracy measure to evaluate this task.
   - Results: strong evidence that the radial tour increases the accuracy of this task by a sizable amount while the task is performed fastest with the grand tour.
   - Attribution of the error by performing a mixed model regression helps to explain the source of the error term into the variability of participants skill aptitude for this task and variability of the difficultly of a simulation by random chance.
   - Introduces an interactive application to preprocess data and explore. Users can choose from six supplied datasets or upload their own.
3. __cheem__, a package applying the radial tour to local explanations of black-box models, including:
   - Preprocessing; creation of a random forest model, the extraction of all observation's tree SHAP local explanation, and extraction of other statistics for display.
   - Visualization of approximations of the data- and attribution-space side-by-side with linked brushing, hover tooltips, and tabular display of selected points.
   - Evaluating the local explanations with the use of the radial tour we can explore the sensitivity to the structure identified better see what variable support the explanation holds realistic.


<!-- Background?? -->
## Background {#ch-background}

This section starts with setting the scope for the type of data we are focusing on, gives a brief motivation then works through previous multivariate visualizations and their scalability. By the end, the focus narrows to linear projections, and in particular, the class of animated linear projections known as the tour.

<!-- note on data and context -->
For our purposes I will be focusing on the case where data $X_{nxp}$ contains $n$ observations of $p$ variables, is complete with no missing values, variables are numeric (ideally not ordinal levels), and $n>p$ typically many more observations than variables. While I write as though always operating on the original variable space these methods could similarly be applied to feature decomposition of data not fitting this description. In the case of the linear projection let, $Y_{nxd} = X_{nxp} * A_{pxd}$ be the embedding of the data mapped by the basis $A$, where $d<p$. When $p$ is large, say over 10 or 20 variables, the viewing space is quite large. In these cases, a PCA initialization step is commonly used where the variables are approximated as fewer principal components, and this reduced space can be viewed, albeit with the disadvantage of having another linear mapping back to the original space. @wickham_visualizing_2015 also view model spaces and features while urging to have a preference for visualizing data-space directly. 

<!-- Better than numerical summarization alone -->
Visualization is much more robust than numerical summarization alone [@anscombe_graphs_1973; @matejka_same_2017]. In these studies, data sets have the same summary statistics, yet contains obvious visual trends and shapes that could go completely unheeded if plotting is foregone. Data visualization is fundamental to EDA and quickly evaluating data supports ensuring that models are suitable.

(ref:ch1fig2-cap) Starting from the profile of a dinosaur, observations are allowed to drift (by iterated simulated annealing) toward 12 patterns provided that they stay close to the original statistics [@matejka_same_2017]. Visualization of data yields stark patterns that are easy to miss in numerical summarization.

```{r ch1fig2, echo=F, out.width='70%', fig.cap = "(ref:ch1fig2-cap)"}
nsSafeIncGraphic("./figures/ch1_fig2_matejka17fig.PNG")
```

### Scatterplot matrices

<!-- Penguins data -->
The work in @grinstein_high-dimensional_2002 gives a good taxonomy of high-dimensional visualization. We will follow a few examples building up to why we conclude with tour methods. Broadly speaking, we are concerned with the question "How can an analyst visualize arbitrary $p-$ dimensions?". To illustrate some of the options for data I use the penguins data[@gorman_ecological_2014, horst_palmerpenguins_2020]. It contains 333 observations of four physical measurements across three species of penguins observed near Palm Station, Antarctica.

<!-- SPLOM -->
Viewing as many univariate histograms or density curves is one method. Similarly, one could look at all variable pairing as scatter plots. This forms the crux of the scatterplot matrices, also known as SPLOM [@chambers_graphical_1983]. In a scatterplot matrix, variables are displayed across the columns and rows, the diagonal elements show the univariate densities while off-diagonal positions show scatterplot pairs. This is useful for getting a handle on the support and shapes of the variables, but is not going to scale well with dimension and is not a suitable audience-ready display as it is very busy and doesn't draw attention to any one spot. Munzner reminds us to abstract all of the cognitive work out of the visual, allowing the audience to focus on seeing the evidence supporting the claim [@munzner_visualization_2014].

(ref:penguinsplom-cap) Scatterplot matrix of penguins data. This is a good initial step, but will not scale well at $p$ increases.

```{r penguinsplom, echo=F, out.width='100%', fig.cap = "(ref:penguinsmplom-cap)"}
nsSafeIncGraphic("./figures/ch2_fig1_penguin_splom.pdf")
```

### Parallel coordinate plots

<!-- PCP & observation based visuals -->
Alternatively, we could consider a class of observation-based visuals. In parallel coordinate plots [@ocagne_coordonnees_1885] variables are arranged horizontally and observations are connected by lines with the height mapping to the quantile or z-value for each variable. This scales much better with dimensions but poorly with observations. It also suffers from an asymmetry with the variable order, that is, changing the order of the variable will lead people to very different conclusions. The x-axis is also used to display variables rather than the values of the observations. This restricts the amount of information that can be interpreted between variables. Munzner asserts that position is the more human-perceptible channel for encoding information; we should like to reserve it for the values of the observations. The same issues persist across other observation-based displays such as radial variants, pixel-oriented visuals, and Chernoff faces [@keim_designing_2000; @chernoff_use_1973]. These visuals are better suited for the $n<p$ case with more variables than observations.

(ref:penguinpcp-cap) Parallel coordinate plots of penguins data. Does not scale well with observations and suffers from asymmetry with the variable ordering, and horizontal position is used for variable rather than observation levels.

```{r penguinpcp, echo=F, out.width='100%', fig.cap = "(ref:penguinpcp-cap)"}
nsSafeIncGraphic("./figures/ch2_fig2_penguin_pcp.pdf")
```


### Dimension reduction

Ultimately, we will need to turn to dimension reduction to create a compelling visual allowing audiences to focus on features with contributions from multiple variables. Dimension reduction is separated into two categories, linear and non-linear. The linear case spans all affine mathematical transformations, essentially any mapping where parallel lines stay parallel. Non-linear transformations are the complement of the linear case, think transformations containing exponents or interacting terms. Examples in low dimensions are relatable. For instance, shadows are examples of linear projections where a 3-dimensional object casts a 2D projection, the shadow. Our vision at any one instance of time, or a picture, are also a 2D projections. An example of a non-linear transformation is that of 2D representation of the globe. There are many different ways (and features to optimize) to distort the surface to display as a map. The most common may be rectangular displays where the area is distorted more and more with the distance away from the equator. Other distortions are created when the surface is unwrapped as a long ellipse. Yet others create non-continuous gaps in oceans to minimize the distortion of countries.

Non-linear techniques often have hyperparameters that affect how the spaces are distorted to fit into a fewer dimensions. To quote Anastasios Panagiotelis, "All non-linear projections are wrong, but some are useful", a play on George Box's quote about models. Non-linear techniques distort the space in unclear ways, and what is more, they can introduce features not in the data depending on the selection of hyperparameters. The presence of structure in a non-linear model is necessary but not sufficient to conclude the existence of a structure in the data.

Unfortunately, there is no free lunch here. An increase in the original data dimensions will lead to a $p-d$-dimensional viewing space in the linear case, and an increasingly perturbed and distorted space in non-linear techniques. The intrinsic dimensionality of data is the number of variables needed to minimally represent the data [@grinstein_high-dimensional_2002]. This is an important aspect of dimension reduction that does not have to end in visual, but is also a common part of factor analysis and preprocessing data. Consider a Psychology survey consisting of 100 questions about the Big Five personality traits. The data consists of 100 variables, while the theory would suggest the intrinsic dimensionality is five. The data likely picks up on other aspects and may better be summarized in with eight or ten dimensions. If this were the case, reducing the data to this space would be necessary to gate the exponentially increasing view time.


### Tours, animated linear projections

A single linear projection is a resulting space from the data multiplied by the basis where the basis is an orthonormal matrix (the orientation of the unit origin) mapping the data space to a lower dimension. A data visualization tour animates many such projections through small changes in the basis. In the bar stool shadow analogy, structural information about a hidden object was gained by watching its shadow change due to its rotation. An analyst similarly gains information about the data object by watching continuous changes to the basis (the rotation of the data). Originally in a _grand_ tour [@asimov_grand_1985] several target frames are randomly selected and then interpolated along their geodesic path.

(ref:ch1fig3-cap) Illustration of the grand tour selecting random target frames (grey) connected via geodesically interpolated frame (white). Figure from @buja_computational_2005.

```{r ch1fig3, echo=F, out.width='100%', fig.cap = "(ref:ch1fig3-cap)"}
nsSafeIncGraphic("./figures/ch1_fig3_buja05fig.PNG")
```

There are various types of tours which are classified by the generation of their basis paths. A _guided_ tour uses simulated annealing to move progressively closer to an objective function in the embedded space [@hurley_analyzing_1990]. A more comprehensive discussion and review of tours can be found in the works of @cook_grand_2008 and @lee_state_2021.

Tours are used for a couple of salient features: maintains transparency back to the original variable space, and the persistence data points from frame to frame convey more information than looking at discrete jumps to other bases. Because of these features, tours are a good method to extend the visualization of data space as dimensionality increases.

The work below covers manual tours [@cook_manual_1997; @spyrison_spinifex_2020] in Chapter \ref{ch-spinifex}. Radial tours are one type of manual tour that where the contribution of one variable is extended radially to a full contribution, removed completely, then restored to its original contribution. Chapter \ref{ch-efficacy_radial_tour} compares the efficacy of the radial tour as compared with PCA and the grand tour in a user study. Lastly, Chapter \ref{ch-cheem} extends the use of the radial tour to evaluate the local explanation of black-box models.


## Thesis structure

<!-- Chapter \ref{ch-background} covers the scope and type of data we consider visualizing before walking through some of the previous attempts at visualization. -->
The remainder of the thesis is organized as follows: Chapter \ref{ch-spinifex} discusses the theory and implementation of the manual tour in the package __spinifex__. Chapter \ref{ch-efficacy_radial_tour} discusses a user study evaluating the efficacy of the radial, manual tour as compared with PCA and the grand tour. There is ample evidence that using the radial tour increases the accuracy of responses for the supervised variable-attribution task describing the separation between two variables. Chapter \ref{ch-cheem} extends the use of manual tours to improve the interpretability of non-linear models. Where manual tours explore variable sensitivity to the structure identified in local explanations of the model. Lastly, Chapter \ref{ch-conclusion} concludes with some takeaways and a discussion of possible extensions.


<!--chapter:end:01-introduction.Rmd-->

---
chapter: 3
knit: "bookdown::render_book"
---
```{r, include=FALSE}
## Original R journal paper:
if(F)
  file.edit("../spinifex_paper/paper/spyrison-cook.Rmd")
## Creating figures in script (not ran upon compilation)
if(F)
  file.edit("./figures/_ch3_spinifex.r")
```

# spinifex: an R package for creating user-controlled animated linear projections {#ch-spinifex}

<!-- Segue and disclose change -->
This chapter introduces manual tours that allows analysts to influence the contributions to a projection. This feature is absent from previous linear embeddings. This work has changed from its publication, primarily to incorporate and discuss the subsequent 'ggproto' API that extends the flexibility of the geometric displays of tours.

<!-- Abstract -->
Dynamic low-dimensional linear projections of multivariate data known as _tour_ provide an essential tool for exploring multivariate data and models. The R package __tourr__ provides functions for several types of tours: grand, guided, little, local, and frozen. Each of these can be viewed in a development environment, or their basis array can be saved for later consumption. This paper describes a new package, __spinifex__, which provides a manual tour of multivariate data. In a manual tour an analyst controls the contribution of a variable to the projection. Controlled manipulation is important to explore a variables sensitivity to structure of an identified feature. The use of the manual tour is applied to particle physics data to illustrate the sensitivity of structure in a projection to specific variable contributions. Additionally, we create a 'ggproto' API for composing any tour that mirrors the layered additive approach of __ggplot2__. Tours can then be animated and exported to various formats with __plotly__ or __gganimate__.


## Introduction

<!-- Tours -->
Exploring multivariate spaces is a challenging task, increasingly so as dimensionality increases. Traditionally, static low-dimensional projections are used to display multivariate data in two dimensions, including principal component analysis, linear discriminant spaces or projection pursuit. These are useful for finding relationships between multiple variables, but they are limited because they show only a glimpse of the high-dimensional space. An alternative approach is to use a tour [@asimov_grand_1985] of dynamic linear projections to look at many different low-dimensional projections. Tours can be considered to extend the dimensionality of visualization, which is important as data and models exist in more than three dimensions. The package __tourr__ [@wickham_tourr:_2011] provides a platform for generating tours. It can produce a variety of tours, each paired with a variety of possible displays. A user can make a grand, guided, little, local or frozen tour, and display the resulting projected data as a scatterplot, density plot, histogram, or even as Chernoff faces if the projection dimension is more than 3.

<!-- Manual tour -->
This work adds a manual tour to the collection. The manual tour was described in @cook_manual_1997 and allows a user to control the projection coefficients of a selected variable in a 2D projection. The manipulation of these coefficients enable the analyst to explore their sensitivity to the structure within the projection. As manual tours operate on only one variable, they are particularly useful once a feature of interest has been identified. 

<!-- interesting features and guided tours -->
One way to identify "interesting" features is using a guided tour [@cook_grand_1995]. Guided tours select a very specific path, which approaches a projection that optimizes an objective function. The optimization used to guide the tour is simulated annealing [@kirkpatrick_optimization_1983]. The direct optimization of a function allows guided tours to rapidly identify interesting projection features given the relatively large parameter-space. After a projection of interest is identified, an analyst can then use the "finer brush" of the manual tour to control the contributions of individual variables to explore the sensitivity they have on the structure visible in the projection.

<!-- Paper outline -->
The paper is organized as follows. Section \@ref(sec:algorithm) describes the algorithm used to perform a radial manual tour implemented in the package __spinifex__. Section \@ref(sec:pkgstructure) discussed the functions, their usage composing tours with __ggplot2__ [@wickham_ggplot2_2016] and their animation by __plotly__ [@sievert_interactive_2020] or __gganimate__ [@pedersen_gganimate_2020]. Package functionality and code usage following the order applied in the algorithm follows in section \@ref(sec:usage). Section \@ref(sec:usecases) illustrates how this can be used for sensitivity analysis applied to multivariate data collected on high-energy physics experiments [@wang_mapping_2018]. Section \@ref(sec:discussion) summarizes this paper and discusses potential future directions.


<!-- Algorithm outline -->
## Algorithm {#sec:algorithm}

The algorithm to conduct a manual tour interactively by recording mouse/cursor motion is described in detail in @cook_manual_1997. Movement can be in any direction and magnitude, but it can also be constrained in several ways:

- *radial*: fix the direction of contribution, and allow the magnitude to change.
- *angular*: fix the magnitude, and allow the angle or direction of the contribution to vary.
- *horizontal*, *vertical*: allow rotation only around the horizontal or vertical axis of the current 2D projection.

The algorithm described here produces a **radial** tour as an *animation sequence*. It takes the current contribution of the chosen variable, and using rotation brings this variable fully into the projection, completely removes it, before returning to the original position.


### Notation

The notation used to describe the algorithm for a 2D radial manual tour is as follows:

- $\textbf{X}$, the data, an $n \times p$ numeric matrix to be projected.
- $\textbf{A}$, any orthonormal projection basis, $p \times d$ matrix, describing the projection from $\mathbb{R}^p \Rightarrow \mathbb{R}^d$.
- $k$, is the index of the manipulation variable or manip var for short.
- $\textbf{e}$, a 1D basis vector of length $p$, with 1 in the $k$-th position and 0 elsewhere.
- $\textbf{M}$ is a $p \times 3$ matrix, defining the 3D subspace where data rotation occurs and is called the manip(ulation) space.
- $\textbf{R}$, the $d+1$-D rotation matrix, for performing unconstrained 3D rotations within the manip space, $\textbf{M}$.
- $\theta$, the angle of in-projection rotation, for example, on the reference axes; $c_\theta, s_\theta$ are its cosine and sine.
- $\phi$, the angle of out-of-projection rotation, into the manip space; $c_\phi, s_\phi$ are its cosine and sine. The initial value for animation purposes is $\phi_1$.
- $\textbf{U}$, the axis of rotation for out-of-projection rotation orthogonal to $\textbf{e}$.
- $\textbf{Y} = \textbf{X} \times \textbf{A}$, the resulting projection of the data through the manip space, $\textbf{M}$, and rotation matrix, $\textbf{R}$.

<!-- operate on bases -->
The algorithm operates entirely on projection bases and incorporates the data only when making the projected data plots in light of efficiency.


### Steps

#### Step 0) Setup

<!-- describe data. -->
The flea data (@lubischew_use_1962), available in the __tourr__ package, is used to illustrate the algorithm. The data contains 74 observations of six variables, physical measurements of flea beetles. Each observation belongs to one of three species.

<!-- Projection basis -->
An initial 2D projection basis must be provided. A suggested way to start is to identify an interesting projection using a projection pursuit guided tour. Here the holes index is used to find a 2D projection of the flea data, which shows three separated species groups. Figure \@ref(fig:ch3fig1) shows the initial projection of the data. The left panel displays the projection basis ($\textbf{B}$) and can be used as a visual guide of the magnitude and direction that each variable contributes to the projection. The right panel shows the projected data, $\textbf{Y}_{[n,~2]} ~=~ \textbf{X}_{[n,~p]} \textbf{B}_{[p,~2]}$. The color and shape of points are mapped to the flea species.

```{r ch3fig1, fig.cap = "Biplot of the initial 2D projection: representation of the basis (left) and resulting data projection (right) of standardized flea data. The color and shape of data points are mapped to the species of flea beetle. The basis was produced by a projection pursuit guided tour with the holes index. The contribution of the variables aede2 and tars1 approximately contrasts the other variables. The visible structure in the projection are the three clusters corresponding to the three species."}
nsSafeIncGraphic("./figures/ch3_fig1_biplot.pdf")
```


#### Step 1) Choose manip variable

<!-- select a manip var-->
In figure \@ref(fig:ch3fig1) the contribution of the variables tars1 and aede2 mostly contrast the contribution of the other four variables. These two variables combined contribute in the direction of the projection where the purple cluster is separated from the other two clusters. The variable aede2 is selected as the manip var, the variable to be controlled in the tour. The question that will be explored is: how important is this variable to the separation of the clusters in this projection?


#### Step 2) Create the 3D manip space

<!-- Zero Vect, manip sp -->
Initialize the coordinate basis vector as a zero vector, $\textbf{e}$, of length $p$, and set the $k$-th element to 1. In the example data, aede2 is the fifth variable in the data, so $k=5$, set $e_5=1$. Use a Gram-Schmidt process to orthonormalize the coordinate basis vector on the original 2D projection to describe a 3D manip space, $\textbf{M}$.

\begin{align*}
  e_k &\leftarrow 1 \\ 
  \textbf{e}^*_{[p,~1]} &= \textbf{e} - \langle \textbf{e}, \textbf{B}_1 \rangle \textbf{B}_1 - \langle \textbf{e}, \textbf{B}_2 \rangle \textbf{B}_2 \\ 
  \textbf{M}_{[p,~3]} &= (\textbf{B}_1,\textbf{B}_2,\textbf{e}^*)
\end{align*}

<!-- What the manip space provides -->
The manip space provides a 3D projection from $p$-dimensional space, where the coefficient of the manip var can range completely between [0, 1]. This 3D space serves as the medium to rotate the projection basis relative to the selected manipulation variable. Figure \@ref(fig:ch3fig2) illustrates this 3D manip space with the manip var highlighted. This representation is produced by calling the `view_manip_space()` function. This diagram is purely used to help explain the algorithm.

```{r ch3fig2, fig.cap = "Illustration of a 3D manip space, the projection plane is shown as a blue circle extending into and out of the display. A manipulation direction is initialized, the red circle, orthogonal to the projection plane. This allows the selected variable, aede2, to change its contribution back to the projection plane. The other variables contributions rotate into this space as well, preserving the orthogonal structure, but are omitted in the manipulation dimension for simplicity."}
nsSafeIncGraphic("./figures/ch3_fig2_manip_sp.pdf")
```


#### Step 3) Defining a 3D rotation

<!-- illustration of axis manip -->
The basis vector corresponding to the manip var (red line in Figure \@ref(fig:ch3fig2)), can be operated like a lever anchored to the origin. This is the process of the manual control, that rotates the manip variable into and out of the 2D projection (Figure \@ref(fig:ch3fig3)). As the variable contribution is controlled, the manip space turns, and the projection onto the horizontal projection plane correspondingly changes. This is a manual tour. Generating a sequence of values for the rotation angles produces a path for the rotation of the manip space.

<!-- describe manip var path -->
For a radial tour, fix $\theta$, the angle describing rotation within the projection plane, and compute a sequence for $\phi$, defining movement out of the plane. This will change $\phi$ from the initial value, $\phi_1$, the angle between $\textbf{e}$ and its shadow in $\textbf{B}$, to a maximum of $0$ (manip var fully in projection), then to a minimum of $\pi/2$ (manip var out of projection), before returning to $\phi_1$.

<!-- define the rotation matrix -->
Rotations in 3D can be defined by the axes they pivot on. Rotation within the projection, $\theta$, is rotation around the $Z$-axis. Out-of-projection rotation, $\phi$, is the rotation around an axis on the $XY$ plane, $\textbf{U}$, orthogonal to $\textbf{e}$. Given these axes, the rotation matrix, $\textbf{R}$, can be written as follows, using Rodrigues' rotation formula (originally published in @rodrigues_lois_1840):

  \begin{align*}
    \textbf{R}_{[3,~3]} 
    &= \textbf{I}_3 + s_\phi\*\textbf{U} + (1-c_\phi)\*\textbf{U}^2 \\
        &=
    \begin{bmatrix}
      1 & 0 & 0 \\ 
      0 & 1 & 0 \\ 
      0 & 0 & 1 \\
    \end{bmatrix} +
    \begin{bmatrix}
      0 & 0 & c_\theta s_\phi \\
      0 & 0 & s_\theta s_\phi \\
      -c_\theta s_\phi & -s_\theta s_\phi & 0 \\
    \end{bmatrix} +
    \begin{bmatrix}
      -c_\theta (1-c_\phi) & s^2_\theta (1-c_\phi) & 0 \\
      -c_\theta s_\theta (1-c_\phi) & -s^2_\theta (1-c_\phi) & 0 \\
      0 & 0 & c_\phi-1 \\
    \end{bmatrix} \\
    &= 
    \begin{bmatrix}
      c_\theta^2 c_\phi + s_\theta^2 &
      -c_\theta s_\theta (1 - c_\phi) &
      -c_\theta s_\phi \\
      -c_\theta s_\theta (1 - c_\phi) &
      s_\theta^2 c_\phi + c_\theta^2 &
      -s_\theta s_\phi \\
      c_\theta s_\phi &
      s_\theta s_\phi &
      c_\phi
    \end{bmatrix} \\
\end{align*}

\noindent where

\begin{align*}
  \textbf{U} &= (u_x, u_y, u_z) =
  (s_\theta, -c_\theta, 0) \\ 
  &=
  \begin{bmatrix}
  0 & -u_z & u_y \\
  u_z & 0 & -u_x \\
  -u_y & u_x & 0 \\
  \end{bmatrix} =
  \begin{bmatrix}
    0 & 0 & -c_\theta \\
    0 & 0 & -s_\theta \\
    c_\theta & s_\theta & 0 \\
  \end{bmatrix} \\
  \end{align*}
  
<!-- The term $(1-c_\phi)$ is used by convention, but $2sin^2(\phi/2)$ is more computationally robust. -->


#### Step 4) Creating an animation of the radial rotation

<!-- Phi transform and animation -->
The steps outlined above can be used to create any arbitrary rotation in the manip space. To use these for sensitivity analysis, the radial rotation is built into an animation where the manip var is rotated fully into the projection, completely out, and then back to the initial value. This involves allowing $\phi$ to vary between $0$ and $\pi/2$, call the steps $\phi_i$. 


```{r ch3fig3, fig.cap = "Select frames highlight the animation of a radial manual tour manipulating aede2: (1) original projection, (2) full contribution, (3) zero contribution, before returning to the original contribution."}
nsSafeIncGraphic("./figures/ch3_fig3_filmstrip.pdf")
```


<!-- Sequence of $\phi_i$ -->
1. Set initial value of $\phi_1$ and $\theta$: $\phi_1 = \cos^{-1}{\sqrt{B_{k1}^2+B_{k2}^2}}$, $\theta = \tan^{-1}\frac{B_{k2}}{B_{k1}}$. Where $\phi_1$ is the angle between $\textbf{e}$ and its shadow in $\textbf{B}$.
2. Set an angle increment ($\Delta_\phi$) that sets the step size for the animation, to rotate the manip var into and out of the projection. Uses of angle increment, rather than a number of steps to control the movement is consistent with the tour algorithm as implemented in the __tourr__.
3. Step towards $0$, where the manip var is entirely in the projection plane.
4. Step towards $\pi/2$, where the manip variable has no contribution to the projection.
5. Step back to $\phi_1$.

In each of the steps 3-5, a small step may be added to ensure that the endpoints of $\phi$ ($0$, $\pi/2$, $\phi_1$) is reached. 

#### Step 5) Projecting the data {#sec:display}

<!-- the reminder of basis operation and now apply data -->
The operation of a manual tour is defined on the projection bases. Only when the data plot needs to be made the data projected into the relevant basis. 

\begin{align*}
  \textbf{Y}^{(i)}_{[n,~3]} &= \textbf{X}_{[n,~p]} \textbf{M}_{[p,~3]} \textbf{R}^{(i)}_{[3,3]}
\end{align*}

<!-- plot XY in seq for animation -->
\noindent where $\textbf{R}^{(i)}_{[3,3]}$ is the incremental rotation matrix, using $\phi_i$. To make the data plot, use the first two columns of \textbf{Y}. Show the projected data for each frame in sequence to form an animation. 

Tours are typically viewed as an animation. The animation of this tour can be viewed online on [GitHub](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/flea_radialtour_mvar5.gif). The page may take a moment to load.


## Package structure {#sec:pkgstructure}

This section describes the functions available in the package, their usage, and how to install and get up and running.


### Functions

Table \@ref(tab:functionsTable) lists the primary functions and their purpose. These are grouped into four types: processing the data, production of tour path, the composition of the tour display, and its animation.

```{r functionsTable, echo=F, eval=T}
library(kableExtra)

funcs_tib <- tibble::tribble(
  ~Family,      ~Function,            ~`Related to`,         ~Description,
  "processing", "scale_01/sd",        "-",                   "scale each column to [0,1]/std dev away from the mean",
  "processing", "basis_pca/olda/...", "Rdimtools::do.*",     "basis of orthogonal component spaces",
  "processing", "basis_half_circle",  "-",                   "basis with uniform contribution across half of a circle",
  "processing", "basis_guided",       "tourr::guided_tour",  "silently return the basis from a guided tour",
  "tour path",  "manual_tour",        "-",                   "basis and interpolation information for a manual tour",
  "tour path",  "save_history",       "tourr::save_history", "silent, extended wrapper returning other tour arrays",
  "display",     "ggtour",            "ggplot2::ggplot",     "canvas and initialization for a tour animation",
  "display",     "proto_point/text",  "geom_point/text",     "adds observation points/text",
  "display",     "proto_density/2d",  "geom_density/2d",     "adds density curve/2d contours",
  "display",     "proto_hex",         "geom_hex",            "adds hexagonal heatmap of observations",
  "display",     "proto_basis/1d",    "-",                   "adds adding basis visual in a unit-circle/-rectangle",
  "display",     "proto_origin/1d",   "-",                   "adds reference mark in the center of the data",
  "display",     "proto_default/1d",  "-",                   "wrapper for proto_* point + basis + origin",
  "display",     "facet_wrap_tour",   "ggplot2::facet_wrap", "facets on the levels of variable",
  "display",     "append_fixed_y",    "-",                   "add/overwrite a fixed vertical position",
  "animation",  "animate_plotly",     "plotly::ggplotly",    "render as an interactive hmtl widget",
  "animation",  "animate_gganimate",  "gganimate::animate",  "render as a .gif, .mp4, or other video format",
  "animation",  "filmstrip",          "-",                   "static gpplot faceting on the frames of the animation"
)

if(knitr::is_html_output()) fmt <- "html" else fmt <- "latex"
if(knitr::is_html_output()) fnt <- 12L else fnt <- 8L
kableExtra::kable(funcs_tib, fmt, caption = "Summary of primary functions.", 
                  booktabs = TRUE, linesep = "") %>%
  kableExtra::kable_styling(font_size = fnt)
```


### Usage {#sec:usage}

Using the penguins data , available in the package, to illustrate a manual tour, we will illustrate generating a manual tour to explore the sensitivity of a variable separating two clusters. The composition of the tour display echos the additive layered approach of __ggplot2__, while abstracting away the complexity of dealing with changing number of frames and their animation.

```{r eval=F, echo=T}
## Process penguins data
dat     <- scale_sd(penguins[1:4])
clas    <- penguins$species
bas     <- basis_olda(data = dat, class = clas)

## The tour path of the bases
mt_path <- manual_tour(basis = bas, manip_var = 1, data = dat)

## Composing the display of the tour
ggt <- ggtour(mt_path, angle = .15) +
  proto_point(aes_args      = list(color = clas, shape = clas),
              identity_args = list(alpa = .8, size = 1.5)) +
  proto_basis("left") +
  proto_origin()

## Animating
animate_plotly(ggt, fps = 10)
```


### Installation

The __spinifex__ is available from CRAN, the following code will help to get up and running:

```{r eval=F, echo=T}
# Installation:
install.package("spinifex") ## Install from CRAN
library("spinifex") ## Load into session

# Getting started:
## Shiny app for visualizing basic application
run_app("intro")
## View the code vignette
vignette("getting_started_with_spinifex")
## More about proto_* functions
vignette("ggproto_api")
```


## Use cases {#sec:usecases}

<!-- Introduction of data and original paper -->
@wang_mapping_2018 introduce a new tool, PDFSense, to visualize the sensitivity of hadronic experiments to nucleon structure. The parameter-space of these experiments lies in 56 dimensions, and are approximated as the ten first principal components. 

<!-- grand tours on the same data -->
@cook_dynamical_2018 illustrates how to learn more about the structures using a grand tour. Tours can better resolve the shape of clusters, intra-cluster detail, and better outlier detection than PDFSense & TFEP (TensorFlow embedded projections) or traditional static embeddings. This example builds from here, illustrating how the manual tour can be used to examine the sensitivity of structure in a projection to different parameters. The specific 2D projections passed to the manual tour were provided in their work. 

<!-- Data structure -->
The data has a hierarchical structure with top-level clusters; DIS, VBP, and jet. Each cluster is a particular class of experiments, each with many experimental datasets which each have many observations of their own. In consideration of data density, we conduct manual tours on subsets of the DIS and jet clusters. This explores the sensitivity of the structure to each of the variables in turn and we present the subjectively best and worst variable to manipulate for identifying dimensionality of the clusters and describing the span of the clusters.


### Jet cluster

<!-- jet cluster, explain dimensionality -->
The jet cluster resides in a smaller dimensionality than the full set of experiments, with four principal components explaining 95% of the variation in the cluster [@cook_dynamical_2018]. The data within this 4D embedding is further subset to ATLAS7old and ATLAS7new, to focus on two groups that occupy different parts of the subspace. Radial manual tours controlling contributions from PC4 and PC3 are shown in Figures \@ref(fig:ch3fig4) and \@ref(fig:ch3fig5), respectively. The difference in shape can be interpreted as the experiments probing different phase spaces. Back-transforming the principal components to the original variables can be done for a more detailed interpretation.

<!-- discussion of findings and which is more insightful -->
When PC4 is removed from the projection (Figure \@ref(fig:ch3fig4)), the difference between the two groups is removed, indicating that PC4 is essential for the separation of experiments. However, eliminating PC3 from the projection (Figure \@ref(fig:ch3fig5)) does not affect the structure, meaning PC3 is not important for distinguishing experiments. Animations for the remaining PCs can be viewed at the following links: [PC1](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/jetcluster_manualtour_pc1.gif), [PC2](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/jetcluster_manualtour_pc2.gif), [PC3](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/jetcluster_manualtour_pc3.gif), and [PC4](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/jetcluster_manualtour_pc4.gif). It can be seen that only PC4 is important for viewing the difference in these two experiments.

<!-- JetClusterGood -->
```{r ch3fig4, fig.cap="Select frames from a radial tour of PC4 within the jet cluster, with color indicating experiment type: ATLAS7new (green) and ATLAS7old (orange). When PC4 is removed from the projection (frame 10), there is little difference between the clusters, suggesting that PC4 is important for distinguishing the experiments."}
nsSafeIncGraphic("./figures/ch3_fig4_jet_better_pc4.pdf")
```
<!-- JetClusterBad -->
```{r ch3fig5, fig.cap = "Frames from the radial tour manipulating PC3 within the jet cluster, with color indicating experiment type: ATLAS7new (green) and ATLAS7old (orange).  When the contribution from PC3 is changed, there is little change in the separation of the clusters, suggesting that PC3 is not important for distinguishing the experiments."}
nsSafeIncGraphic("./figures/ch3_fig5_jet_worse_pc3.pdf")
```


### DIS cluster

<!-- introduce DIS cluster -->
Following @cook_dynamical_2018, to explore the DIS cluster, PCA is recomputed and the first six principal components, explaining 48% of the full sample variation, are used. The contributions of PC6 and PC2 are explored in Figures \@ref(fig:ch3fig6) and \@ref(fig:ch3fig7), respectively. Three experiments are examined: DIS HERA1+2 (green), dimuon SIDIS (purple), and charm SIDIS (orange).

<!-- comparison of the DIS cluster -->
Both PC2 and PC6 contribute to the projection similarly. When PC6 is rotated into the projection, variation in the DIS HERA1+2 is greatly reduced. When PC2 is removed from the projection, dimuon SIDIS becomes more distinct. Even though both variables contribute similarly to the original projection their contributions have quite different effects on the structure of each cluster, and the distinction between clusters. Animations of all of the principal components can be viewed from the links: [PC1](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/discluster_manualtour_pc1.gif), [PC2](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/discluster_manualtour_pc2.gif), [PC3](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/discluster_manualtour_pc3.gif), [PC4](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/discluster_manualtour_pc4.gif), [PC5](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/discluster_manualtour_pc5.gif), and [PC6](https://github.com/nspyrison/spinifex_paper/blob/master/paper/gifs/discluster_manualtour_pc6.gif).

<!-- DISclusterGood -->
```{r ch3fig6, fig.cap = "Select frames from a radial tour exploring the sensitivity that PC6 has on the structure of the DIS cluster, with color indicating experiment type: DIS HERA1+2 (green), dimuon SIDIS (purple), and charm SIDIS (orange). DIS HERA1+2 is distributed in a cross-shaped plane, and charm SIDIS occupies the center of this cross, and dimuon SIDIS is a linear cluster crossing DIS HERA1+2. As the contribution of PC6 is increased, DIS HERA1+2 becomes almost singular in one direction (frame 5), indicating that this cluster has very little variability in the direction of PC6."}
nsSafeIncGraphic("./figures/ch3_fig6_DIS_better_pc6.pdf")
```

<!-- DISclusterBad -->
```{r ch3fig7, fig.cap = "Frames from the radial tour exploring the sensitivity PC2 to the structure of the DIS cluster, with color indicating experiment type: DIS HERA1+2 (green), dimuon SIDIS (purple), and charm SIDIS (orange). As the contribution of PC2 is decreased, dimuon SIDIS becomes more distinguishable from the other two clusters, indicating that in the absence of PC2 is important for separating this cluster from the others."}
nsSafeIncGraphic("./figures/ch3_fig7_DIS_worse_pc2.pdf")
```

## Discussion {#sec:discussion}

<!-- Summary of spinifex -->
Dynamic linear projections of numeric multivariate data, tours, play an important role in data visualization; they extend the dimensionality of visuals to peek into high-dimensional data and parameter spaces. This research has taken the manual tour algorithm, specifically the radial rotation, used in GGobi [@swayne_ggobi:_2003] to interactively rotate a variable into or out of a 2D projection, and modified it to create an animation that performs the same task. It is most useful for examining the importance of variables and how the structure in the projection is sensitive or not to specific variables. This functionality is made available in the package __spinifex__. Which also extends the geometric display and export formats interoperable with the __tourr__ package.

<!-- Summary of application -->
This work was motivated by problems in physics, and thus the usage was illustrated on data comparing experiments of hadronic collisions to explore the sensitivity of cluster structure to different principal components. These tools can be applied quite broadly to many multivariate data analysis problems.

<!-- Constraints -->
The manual tour is constrained in the sense that the effect of one variable is dependent on the contributions of other variables in the manip space. However, this can be useful to simplify a projection by removing variables without affecting the visible structure. Defining a manual rotation in high dimensions is possible using Givens rotations and Householder reflections as outlined in @buja_computational_2005. This would provide more flexible manual rotation but more difficult for a user because they have the choice (too much choice) of which directions to move. 

<!-- 3D and function vis -->
Another future research topic could be to extend the algorithm for use on 3D projections. With the current popularity and availability of 3D virtual displays, this may benefit the detection and understanding of the higher dimensional structure or enable the examination of functions.

<!-- Dynamic interaction/gui -->
Having a graphical user interface would be useful for making it easier and more accessible to a general audience. This is possible to implement using __shiny__ [@chang_shiny_2020]. The primary purposes of the interface would be to allow the user to interactively change the manip variable easily and the interpolation step for more or less detailed views.


## Acknowledgments

This article was created in R, using __knitr__ [@xie_knitr_2020] and __rmarkdown__ [@allaire_rmarkdown_2020], with code generating the examples inline. The source files for this article be found at [github.com/nspyrison/spinifex_paper/](https://github.com/nspyrison/spinifex_paper/). The animated gifs can also be viewed at this site. The source code for the __spinifex__ package can be found at [github.com/nspyrison/spinifex/](https://github.com/nspyrison/spinifex/).

<!-- Adds a bib section at the end of every chapter -->


<!--chapter:end:02-spinifex.Rmd-->

---
chapter: 4
knit: "bookdown::render_book"
editor_options:
  chunk_output_type: console
---

# The benefit of user-controlled radial tour for understanding variable contributions to structure in linear projections {#ch-efficacy_radial_tour}

<!-- WARNING, XXX:TO ADRESS -->
_THE CHAPTER IS NOW SLIGHTLY OUTDATED, Bring content over again after Intro changes and proofreading._

<!-- Segue -->
Now we have the means to perform radial tours. Should we be confident that this method with user control will lead to better analysis than traditional alternatives? This chapter discusses the user study to elucidate the efficacy of the radial tour.

<!-- Abstract -->
Principal component analysis is a long-standing go-to method for exploring multivariate data. Data visualization _tours_ are a class of linear projections animated over small changes in the projection basis. The radial tour is one instance that rotates the contribution of a select variable. Does this variable-level control help an analyst explore multivariate data? This paper describes a user study evaluating the efficacy of using the radial tour in comparison to two existing methods, principal component analysis, and the grand tour. We devise a supervised classification task where participants evaluate variable attribution of the separation between two classes. Accuracy and response time are measured as response variables. Data were collected from 108 crowdsourced participants, who performed two trials of each visual for 648 trials in total. There is strong evidence that the radial tour increases accuracy. Participants also preferred to use the radial tour over alternatives for this task.

## Introduction

<!-- Multivariate spaces and EDA -->
Multivariate data is ubiquitous. Exploratory Data Analysis [EDA, @tukey_exploratory_1977] is essential for understanding the data and testing model assumptions. Data visualization is more robust and informative than statistical summarization alone [@anscombe_graphs_1973; @matejka_same_2017]. Focusing on hypothesis testing can result in tunnel vision of an analyst, causing them to miss visual particularities in the data [@yanai_hypothesis_2020]. Data visualization is integral to EDA and our comprehension of the data. But how do we know which methods to use to visualize data best?

<!--- black-box models-->
Models are becoming increasingly complex models involving many features and terms causing an opaqueness to their interpretability. Exploratory Artificial Intelligence (XAI, @adadi_peeking_2018; @arrieta_explainable_2020) attempt to make black-box models more transparent by offering techniques to increase their interpretability. Multivariate data visualization is a similarly important part of exploring features spaces and communicating interpretations of models [@biecek_dalex_2018; @biecek_explanatory_2021; @wickham_visualizing_2015]. 

<!-- Research gap -->
Dimension reduction is commonly used with visualization to provide informative low-dimensional summaries of high-dimensional data. There have been several user studies for dimension reduction comparing across embeddings and display dimensionality [@gracia_new_2016; @wagner_filho_immersive_2018]. There are also empirical metrics and comparisons used to describe non-linear reduction and how well and faithfully they embed the data [@bertini_quality_2011; @liu_visualizing_2017; @sedlmair_empirical_2013; @van_der_maaten_visualizing_2008]. There is an absence of studies comparing techniques for assessing variable attribution across visualization methods.

<!-- Overview of the study -->
This paper describes a crowdsourced (by prolific.co), user study conducted to assess the efficacy of different visualization methods. Cluster data is simulated under several additional experimental factors, namely: location, shape, and dimensionality. We task participants with identifying the variables attributing to the separation between two clusters. We define an accuracy measure for this task and use response time as a response variable of secondary interest. We perform mixed-model regression on these measures and find radial tour has strong evidence for a large improvement in accuracy from using radial tour over alternatives Principal Component Analysis (PCA). We also find evidence for relatively small differences in response time with PCA being the fastest then grand then radial.

<!-- Structure of the paper -->
The paper is structured as follows. Section \@ref(sec:background) discusses several visualization methods, including the ones compared in the study. Section \@ref(sec:userstudy) describes the experimental factors, tasks, and evaluation measures used. The results of the study are discussed in Section \@ref(sec:results). Conclusions and potential future directions are discussed in Section \@ref(sec:conclusion). The software used for the study is described in Section \@ref(sec:spinifex).


## Background {#sec:background}

Here, we discuss common multivariate techniques, including the methods before settling on PCA, grand tour, and the radial, manual tour.

### Scatterplot matrix

One could consider looking at $p$ histograms or univariate densities. This will miss features in two or more dimensions. Similarly, all combinations of variables can be viewed as a scatterplot matrix [@chambers_graphical_1983]. Figure \@ref(fig:ch4fig1) shows the first three components of simulated data as a scatterplot matrix. Looking at $p$ univariate densities or many bivariate scatterplots at once quickly becomes burdensome as dimensionality increases, and only displays information containing in 2 orthogonal dimensions, that is features that require in three dimensions will never be resolved.


### Parallel coordinates plot

<!-- PCP -->
Another common way to display multivariate data is with a parallel coordinates plot [@ocagne_coordonnees_1885], which shows observations by their quantile values for each variable with connected by lines to the quantile value in subsequent variables.

Parallel coordinates plot and other observations-based visuals such as pixel plots or Chernoff faces scale well with the number of dimensions but poorly with observations. These are perhaps best used when there are more variables than observations.

Observations-based visuals have a couple of issues. One is that they are asymmetric across variable ordering which can lead to different conclusions or features being focused on due to variable order. Another notable issue of observations-based visuals is the graphical channel used to convey information. Munzner suggests that position is the visual channel that is most perceptible by human perception [@munzner_visualization_2014]. In the case of parallel coordinates plots, the horizontal axes span variables rather than the values of one variable. That is the loss of using an axis, our most discerning visual channel.


### Principal component analysis

PCA is a good baseline of comparison for linear projections because of its frequent and broad use across disciplines. Principal component analysis [@pearson_liii._1901] finds new components, a linear combinations of the original variables, ordered by decreasing variation. While the full dimensionality is intact, the benefit comes from the ordered nature of the components. The data is said to be approximated but the first several components, the exact number typically being subjectively selected given the variance contained by each dimension.

```{r ch4fig1, fig.cap = "Scatterplot matrix of the first four principal components simulated data in six dimension. An analyst would have to view PC1 by PC2 and PC1 by PC4 to have a thorough take on which variables attribute to the separation between clusters."}
nsSafeIncGraphic("./figures/ch4_fig1_pca_splom.pdf")
```

### Animated linear projections, tours

<!-- tours intro -->
A data visualization _tour_ animates many linear projections over small changes in the projection basis. One of the insightful features of the tour is the object permanence of the data points; one can track the relative changes of observations as the basis moves, as opposed to discretely jumping to an orthogonal view with no intermediate information. Types of tours are distinguished by the selection of their basis paths [@lee_state_2021; @cook_grand_2008]. To contrast with the discrete orientations of PCA, we compare with continuous changes of linear projection with _grand_ and _radial_ tours.

#### Grand tours

<!-- Grand tour -->
In a grand tour [@asimov_grand_1985], the target bases are selected randomly. The grand tour is the first and most widely known tour. The random selection of target bases makes it a general unguided exploratory tool. It will make a good comparison that has continuity of data points in nearby frames along with the radial tour but lacks the user control enjoyed by PCA and radial tours. 

#### Manual and radial tours

<!-- Segue, highlighting lack of control -->
Whether an analyst uses a component space or the grand tour they have no way of influencing the basis. They cannot explore the structure identified or change the contribution of the variables. This means of user-control-steering is a key aspect of _manual_ tours that should facilitate testing variable attribution.

<!-- Manual tour -->
The manual tour [@cook_manual_1997] defines its basis path by manipulating the basis contribution of a selected variable. A manipulation dimension is appended onto the projection plane, with a full contribution given to the selected variable. The target bases are then chosen to rotate this newly created manipulation space. The target bases are then similarly orthogonally restrained, the data is projected through interpolated frames and rendered into an animation. For the variables to remain independent of each other, the contributions of the other variables must also change, _ie._ dimension space must maintain its orthonormal structure. A key feature of the manual tour is that it allows users to control the variable contributions to the basis. This means that such manipulations can be selected and queued in advance or selected on the spot for human-in-the-loop analysis [@karwowski_international_2006]. However, this navigation is relatively time-consuming due to the vast volume of $p$-space (an aspect of the curse of dimensionality, @bellman_dynamic_1957) and the abstract method of steering the projection basis. It is advisable first to identify a basis of particular interest and then use a manual tour as more directed, local exploration tool to observe how the contribution of a variable is sensitive to the feature of interest.

<!-- Radial tour variant -->
To simplify the task and keep its duration realistic, we consider a variant of the manual tour, called a _radial_ tour. In a radial tour, the selected variable is allowed to change its magnitude of contribution but not its angle; it must move along the direction of its original contribution radius. The radial tour benefits from both continuity of the data alongside grand tours, but also allows the user to steer via choosing the variable to rotate.

<!-- spinifex -->
The recent implementation of manual tours us the R package __spinifex__ [@spyrison_spinifex_2020], which facilitates manual tours (and radial variant). It is also compatible with tours made with __tourr__ [@wickham_tourr:_2011] and facilitates exporting to .gif or .html widget, with recent graphic packages. Now that we have a readily available means to produce various tours, we want to see how they fare against traditional discrete displays commonly used with PCA.


## User study {#sec:userstudy}

<!-- Overview of visual -->
An experiment was constructed to assess the performance of the radial tour relative to the grand tour and PCA for interpreting the variable attribution contributing to separation between two clusters. <!-- Introduce blocks --> These three methods were applied to simulations across three experimental factors: cluster shape, location of the cluster separation, and data dimensionality. Data was collected using a specially constructed web app, through crowdsourced with prolific.co [@palan_prolific_2018].


### Objective {#sec:objective}

<!-- Rational for factor levels -->
PCA will be used as a baseline for comparison as it is the most common linear embedding. The grand tour will act as a secondary control that will help evaluate the benefit of animation but without influencing its path. Lastly, the radial tour should perform best as it benefits both from animation and being user-control of the contribution of individual variables.

<!-- Prior expectations -->
Then for some subset of tasks, we expect to find that the radial tour performs most accurately, as it enjoys the persistence of points and input control to explore specific variables. Secondly, it may be the case that grand performs faster than the alternatives as its absence of inputs will allow to focus all of their attention on interpreting the fixed path. Conversely, we are less sure about the accuracy of such limited grand tours as there is no objective function in the selection of the bases; it is possible that, by chance, the planes altogether avoid bases showing cluster separation. However, given that the data dimensionality will be modest, it seems likely that grand tour will regularly crosses frames with the correct information to perform the task.

<!-- Explicit hypothesis tests -->
We measure the accuracy and response time over the support of the discussed experimental factors. The null hypotheses can be stated as:

$~~~~H_0: y_1, \text{task accuracy does not vary with the visualization method} \\$
$~~~~~H_\alpha: y_1, \text{task accuracy does vary with the visualization method} \\$
$~~~~~H_0: y_2, \text{task response time does not vary with the visualization method} \\$
$~~~~~H_\alpha: y_2, \text{task response time does vary with the visualization method} \\$


### Experimental factors {#sec:blocks}

<!-- Introduction to blocks -->
In addition to visual factor, we vary the data across three aspects: 1) The _location_ of the difference between clusters, by mixing a signal and a noise variable at different ratios, we vary the number of variables and their magnitude of cluster separation, 2) the _shape_ of the clusters, to reflect varying distributions of the data, and 3) the _dimension_-ality of the data. Below we describe the levels within each factor, while Figure \@ref(fig:ch4fig2) gives a visual representation.

```{r ch4fig2, out.width = '100%', fig.cap = "Illustration of the experimental factors, the parameter space of the independent variables, the support of our study."}
nsSafeIncGraphic("./figures/ch4_fig2_exp_factors.pdf")
```

<!-- Location mixing -->
The _location_ of the separation of the clusters is a crucial aspect of analysis, it is the variables or combination their of that is important to the explanation of the structure. To test the sensitivity to this, we mix a noise variable with the signal-containing variable such that the difference in the clusters is mixed at the following percentages: 0/100% (not mixed), 33/66%, 50/50% (evenly mixed).

<!-- Shape, vc matrix -->
In selecting the _shape_ of the clusters, we follow the convention given by @scrucca_mclust_2016, where 14 variants of model families containing three clusters are defined. The name of the model family is the abbreviation of its respective volume, shape, and orientation of the cluster, which are either equal or vary. We use the models EEE, EEV, and EVV. The latter is further modified by moving four-fifths of the data out in a "V" or banana-like shape.

<!-- Dimensionality -->
_Dimension_-ality is tested at two modest levels, namely, in four dimensions containing three clusters and six dimensions with four clusters. We must do so to bound the difficulty and search space to keep the task realistic for crowdsourcing.


### Task and evaluation {#sec:task}

<!-- segue to task and evaluation -->
With our hypothesis formulated, let us turn our attention to the task and how to evaluate it. Regardless of the visual method, the elements of the display are held constant a 2D scatterplot with axis biplot to its left. Observations were supervised with the cluster level mapped to color and shape.

<!-- Geom, clusters, explicit task -->
Participants were asked to 'check any/all variables that contribute more than average to the cluster separation green circles and orange triangles', which was further explained in the explanatory video as 'mark and all variable that carries more than their fair share of the weight, or one quarter in the case of four variables'.

<!-- Instruction and video -->
The instructions iterated several times in the video was: 1) use the input controls to find a frame that contains separation between the clusters of green circles and orange triangles, 2) look at the orientation of the variable contributions in the gray circle, a visual depiction of basis, and 3) select all variables that contribute more than average in the direction of the separation in the scatterplot. Independent with factor and block values,  participants were limited to 60 seconds for each evaluation of this task. This restriction did not impact many participants as the 25th, 50th, 75th quantiles of response time were about 7, 21, and 30 seconds respectively.

<!-- Evaluating measure -->
The evaluation measure of this task was designed with a couple of features in mind: 1) the sum of squares of the individual variable weights should be one, and 2) symmetric about zero, that is, without preference to under- or over-guessing. With these in mind, we define the following measure for evaluating the task.

Let a data $\textbf{X}_{n\*p\*k}$ be a simulation containing clusters of observations of different distributions. Where $n$ is the number of observations, $p$ is the number of variables, and $k$ is the number of clusters. Cluster membership is exclusive; an observation cannot belong to more than one cluster.

<!-- W, weights -->
We define weights, $W$ to be a vector explaining the variable-wise difference between two clusters. Namely, the difference of each variable between clusters, as a proportion of the total difference, less $1/p$, the amount of difference each variable would hold if it were uniformly distributed. <!-- R, participant responses -->Participant responses are a logical value for each variable, whether or not the participant thinks each variable separates the two clusters more than uniformly distributed separation. Then $Y_1$ is a vector of variable accuracy.

\begin{align*}
W_{j} &=\frac
{(\overline{X_{j=1, k=1}} - \overline{X_{1, 2}}, ~...~
(\overline{X_{p, 1}} - \overline{X_{p, 2}})}
{\sum_{j=1}^{p}(|\overline{X_{j, k=1}} - \overline{X_{j, k=2}}|)}
- \frac{1}{p} \\
\\
\text{Accuracy}, Y &= \sum_{j=1}^{p}I(r_j) * sign(w_j) * \sqrt{|w_j|} \\
\end{align*}

Where $I$ is the indicator function. Then the task accuracy is the sum of this vector. We use the time till the last response as a secondary dependent variable $Y_2$.

```{r ch4fig3, fig.cap = "(L), PCA biplot of the components showing the most cluster separation with (R) illustration of the magnitude of cluster separation is for each variable (bars) and the weight of the variable accuracy if selected (red/green lines). The horizontal dashed line is $1 / p$, the amount of separation each variable would have if evenly distributed. The weights equal the signed square of the difference between each variable value and the dashed line."}
nsSafeIncGraphic("./figures/ch4_fig3_accuracy_measure.pdf")
```


### Visual design standardization {#sec:standardization}

<!-- Background for methodology, application here -->
Section \@ref(sec:background) gives the sources and descriptions of the visual factors PCA, grand tours, and radial manual tours. The factors are tested within-participant, with each factor being evaluated by each participant. The order that factors are experienced is controlled with the block assignment as illustrated below in Figure \@ref(fig:ch4fig4). Below we cover the visual design standardization, as well the input and display within each factor.

<!-- Aesthetic standardization -->
The visualization methods were standardized wherever possible. Each factor was shown as a biplot, display variable contributions on a unit circle. All aesthetic values (colors, shapes, sizes, absence of legend, and absence of axis titles) were held constant. Variable contributions were always shown left of the scatterplot embeddings with their aesthetic values consistent. What did vary between factors were their inputs which caused a discrete jump to another pair or principal components. They were absent for the grand tour with target bases to animate through selected at random, or for the radial tour, which variable should have its contribution animated.

<!-- PCA -->
PCA inputs allowed for users to select between the top four principal components for both the x and y-axis regardless of the data dimensionality (either four or six). <!-- Grand tours -->There was no user input for the grand tour; users were instead shown a 15-second animation of the same randomly selected path. Participant were able to view the same clip up to four times within the time limit. <!-- Radial tours -->Radial tours were also displayed at five frames per second with an interpolation step size of 0.1 radians. Users were able to swap between variables. The display would change the start of radially increasing the contribution of the selected variable until it was full, zeroed, and then back to its initial contribution. The complete animation of any variable takes about 20 seconds and is almost entirely in the projection frame at around six seconds. The starting basis of each is initialized to a half-clock design, where the variables were evenly distributed in half of the circle which is then orthonormalized. This design was created to be variable agnostic while maximizing the independence of the variables.


### Data simulation

<!-- Clusters and correlation -->
Each dimension is originally distributed as $\mathcal{N}(2 * I(signal), 1)~|~\text{covariance}~\Sigma$, a function of the shape factor. Signal variables had a correlation of 0.9 when they have equal orientation and -0.9 when their orientations vary. Noise variables were restricted to zero correlation. Each cluster is simulated with 140 observations and is offset in a variable that did not distinguish previous variables. 
 
<!-- Apply shape and location transformations -->
Clusters of the EVV shape are transformed to the banana-chevron shape. Then location mixing is applied by post-multiplying a (2x2) rotation matrix to the signal variable and a noise variable for the clusters in question.<!-- Preprocess and replicate and save --> All variables are then standardized by standard deviation. The rows and columns are then shuffled randomly. The observation's cluster and order of shuffling are attached to the data and saved.

<!-- Iterating over factor -->
Each of these replications is then iterated with each level of the factor. For PCA, every pair of the top four principal components and saved as 12 plots. For the grand tour, we first save two basis paths of differing dimension before each replication is projected through the common basis path. At the same time, each repetition's variable order was previously shuffled. The resulting animations were saved as .gif files. The radial tour starts at either the four or six variable "half-clock" basis, where each variable has a uniform contribution and no variable contributing in the opposite direction (to minimize variable dependence), a radial tour is then produced for each variable and saved as a .gif.


### Randomized factor assignment

<!-- Introduction -->
Now, with simulation and their artifacts in hand. We explain how the experimental factors are assignment, and illustrate how this is experienced from a participant's perspective.

<!-- Periods, block assignment -->
We section the study into three periods. Each period is linked to a randomized level of factor visualization and the location. The order of dimension and shape are of secondary interest and are held constant in increasing order of difficultly; four then six dimensions and EEE, EEV, then EVV-banana, respectively.

<!-- Training and evaluation -->
Each period starts with an untimed training task at the simplest remaining block levels; location = 0/100%, shape = EEE, and four dimensions with three clusters. This serves to introduce and familiarize participants with input and visual differences. After the training, the participant is evaluated on two tasks with the same factor\*location level across the increasing difficulty of dimension\*shape. The plot was removed after 60 seconds, though participants rarely reached this limit.

<!-- Factor*location nested latin square -->
The order of the levels of the factor and location is randomized with a nested Latin square where all levels of the visual factor are exhausted before advancing to the next level of location. That means we need $3!^2 = 36$ participants to evaluate all permutations of the experimental factors once. This randomization is essential to control for any potential learning effects the participant may receive. Figure \@ref(fig:ch4fig4) illustrates how an arbitrary participant experiences the experimental factors.

<!-- Nested latin square assignment -->
```{r ch4fig4, echo = F, out.width = '100%', fig.cap = "Illustration of how a hypothetical participant 63 is assigned factor and block parameterizations. Each of the 6-factor order permutations is exhausted before iterating to the next permutation of location order."}
## This is a manual .pttx screen cap, .png ok.
nsSafeIncGraphic("./figures/ch4_fig4_randomization_MANUAL.PNG")
```

<!-- Pilot study; 3 even evaluations of each -->
Through pilot studies sampled by convenience (information technology and statistics Ph.D. students attending Monash University), we predict that we need three complete evaluations to power our study properly; we set out to crowdsource $N = 3 * 3!^2 = 108$ participants.


### Participants {#sec:articipants}

We recruited $N = 108$ participants via prolific.co [@palan_prolific_2018]. We filtered participants based on their claimed education requiring that they have completed at least an undergraduate degree (some 58,700 of the 150,400 users at the time); we apply this filter under the premise that linear projections and biplot displays used will not be regularly used for consumption by general audiences. There is also the implicit filter that Prolific participants must be at least 18 years of age and implicit biases  of timezone, location, and language. Participants were compensated for their time at \pounds 7.50 per hour, whereas the mean duration of the survey was about 16 minutes. We can't preclude previous knowledge or experience with the factors but validate this assumption in the follow-up survey asking about familiarity with the factors. The appendix contains a heatmap distribution of age and education paneled across preferred pronouns of the participants that completed the survey, who are relatively young, well educated, and slightly more likely to identify as males.


### Data collection

<!-- App, data collection, network issues -->
Data were recorded by a __shiny__ application and were written to a Google Sheet after each third of the study. Especially at the start of the study, participants experienced adverse network conditions due to the volume of participants hitting the application with modest allocated resources. In addition to this, API read/write limitations further hindered data collection. To mitigate this, we throttled the volume of participants and over-collect survey trials until we had received our target three evaluations of all permutation levels.

<!-- Preprocessing steps -->
The processing steps were minimal. First, we format to an analysis-ready form, decoding values to a more human-readable state, and add a flag to indicate if the survey had complete data. We filter to only the latest three complete studies of each block parameterization, those which should have experienced the least adverse network conditions. The bulk of the studies removed were partial data and a few over-sampled permutations. This brings us to the 108 studies described in the paper, from which models and aggregation tables were built. The post-study surveys were similarly decoded to human-readable format and then filtered to include only those 84 surveys that were associated with the final 108 studies.

The code, response files, their analyses, and the study application are publicly available at on [GitHub](https://github.com/nspyrison/spinifex_study).


## Results {#sec:results}

To recap, the primary response variable is task accuracy, as defined in section \@ref(sec:task), and the log of response time will be used as a secondary response variable. We have two primary data sets; the user study evaluations and the post-study survey. The former is the 108 participants with the explanatory variables: visual factor, location of the cluster separation signal, the shape of variance-covariance matrix, and the dimensionality of the data. Block parameterization and randomization were discussed in section \@ref(sec:blocks). The survey was completed by 84 of these 108 people. It collected demographic information (preferred pronoun, age, and education), and subjective measures for each of the factors (preference, familiarity, ease of use, and confidence).

Below we look at the marginal performance of the block parameters and survey responses. After that, we build a battery of regression models to explore the variables and their interactions. Lastly, we look at the subjective measures between the factors.


### Accuracy regression

<!-- Introduce regression model, explaining accuracy, and random effect term -->
To more thoroughly examine explanatory variables, we regress against accuracy. All models have a random effect term on the participant, which captures the effect of the individual participant. After we look at models of the block parameters, we extend to compare against survey variables. Last, we compare adding a random effect for data and regressing against time till last response fares against benchmark models. The matrices for models with more than a few terms quickly become rank deficient; there is not enough data to explain all of the effect terms. <!-- In which case, the least impactful terms are dropped. -->

<!-- Building a battery of models -->
In building a set of models to test, we include all single term models those with all independent terms. We also include an interaction term of factor by location, allowing for the slope of each location to change across each level of the factor. For comparison, an overly complex model with all interaction terms is included.

<!-- Y1 accuracy regression -->
$$
\begin{array}{ll}
\textbf{Fixed effects}           &\textbf{Full model} \\
\alpha                           &\widehat{Y_1} = \mu + \alpha_i + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha + \beta + \gamma + \delta &\widehat{Y_1} = \mu + \alpha_i + \beta_j + \gamma_k + \delta_l + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha * \beta + \gamma + \delta &\widehat{Y_1} = \mu + \alpha_i * \beta_j + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha * \beta * \gamma + \delta &\widehat{Y_1} = \mu + \alpha_i * \beta_j * \gamma_k + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha * \beta * \gamma * \delta &\widehat{Y_1} = \mu + \alpha_i * \beta_j * \gamma_k * \delta_l + \textbf{Z} + \textbf{W} + \epsilon
\end{array}
$$
$$
\begin{array}{ll}
\text{where } &\mu \text{ is the intercept of the model including the mean of random effect} \\
&\epsilon   \sim \mathcal{N}(0,~\sigma), \text{ the error of the model} \\
&\textbf{Z} \sim \mathcal{N}(0,~\tau), \text{ the random effect of participant} \\
&\textbf{W} \sim \mathcal{N}(0,~\upsilon), \text{ the random effect of simulation} \\
&\alpha_i \text{, fixed term for factor}~|~i\in (\text{pca, grand, radial}) \\
&\beta_j  \text{, fixed term for location}~|~j\in (\text{0/100\%, 33/66\%, 50/50\%}) \text{ \% noise/signal mixing} \\
&\gamma_k \text{, fixed term for shape}~|~k\in (\text{EEE, EEV, EVV banana}) \text{ model shapes} \\
&\delta_l \text{, fixed term for dimension}~|~l\in (\text{4 variables \& 3 cluster, 6 variables \& 4 clusters}) \\
\end{array}
$$

<!-- Y1 model comparisons -->
```{r ch4tab1, fig.cap = "<TABLE FIGURES DFEFINED IN THE KABLE>"}
## Set format and fonts for pdf vs html:
if(knitr::is_html_output()) fmt <- "html" else fmt <- "latex"
if(knitr::is_html_output()) fnt <- 12L    else fnt <- 10L
## Tried escape = FALSE, but that doesn't seem to be the way to go.


mod_comp <- readRDS("./figures/ch4_tab1_model_comp_ls.rds") 
## accuracy [[1]], log time [[2]]
if(knitr::is_html_output()){
  library(stringr)
  str_replace_all(string, pattern, replacement)
  mod_comp$modelComp_MarksByEval$`Fixed effects` <-
    str_replace_all(mod_comp$modelComp_MarksByEval$`Fixed effects`, "*", "\*")
  mod_comp$modelComp_TimeByEval$`Fixed effects` <-
    str_replace_all(mod_comp$modelComp_MarksByEval$`Fixed effects`, "*", "\*")
}

## return
kableExtra::kbl(
  x = mod_comp[[1]], format = fmt, align = c("l", rep("l", 2), rep("c", 5)),
  booktabs = TRUE, linesep = "", escape = TRUE,
  caption = "Model performance of random effect models regressing accuracy. Each model includes a random effect term of the participant explaining the individual's influence on accuracy. Complex models perform better in terms of $R^2$ and RMSE, yet AIC and BIC penalize their large number of fixed effects in favor of the much simpler model containing only the visual factor. Conditional $R^2$ includes the random effects, while marginal does not.") %>%
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = fnt)
```

<!-- Y1 coefficients of ABcd -->
```{r ch4tab2, fig.cap = "<TABLE FIGURES DFEFINED IN THE KABLE>"}
mod_coef <- readRDS("./figures/ch4_tab2_model_coef_ls.rds") 
## accuracy [[1]], log time [[2]]

## return
kableExtra::kbl(
  x = mod_coef[[1]], format = fmt, booktabs = TRUE, linesep = "", escape = TRUE,
  caption = "The task accuracy model coefficients for $\\widehat{Y_1} = \\alpha * \\beta + \\gamma + \\delta$, with factor=pca, location=0/100\\%, and shape=EEE held as baselines. Factor being radial is the fixed term with the strongest evidence supporting the hypothesis. When crossing factor with location radial performs worse with 33/66\\% mixing relative to the PCA with no mixing. The model fit is based on the 648 evaluations by the 108 participants.") %>%
  kableExtra::pack_rows("factor", 2, 3) %>%
  kableExtra::pack_rows("fixed effects", 4, 8) %>%
  kableExtra::pack_rows("interactions", 9, 12) %>%
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = fnt)
```

<!-- Conditional effects of variables -->
We also want to visually explore the conditional variables in the model. Figure \@ref(fig:ch4fig5) explores violin plots of accuracy by factor while faceting on the location (vertical) and shape (horizontal). Radial tends to increase the accuracy received, and especially so when there is no signal/noise mixing.

<!-- Violin plots and test overlay for Y1 factors -->
```{r ch4fig5, fig.cap = "Violin plots of terms of the model $\\widehat{Y_1} = \\alpha * \\beta + \\gamma + \\delta$. Overlaid with global significance from the Kruskal-Wallis test, and pairwise significance from the Wilcoxon test, both are non-parametric, ranked sum tests suitable for handling discrete data. Participants are more confident and find radial tour easier to use than the grand tour. Participants claim low familiarity as we expect from crowdsourced participants. Radial is more preferred compared with either alternative for this task."}
nsSafeIncGraphic("./figures/ch4_fig5_ABcd_violins.pdf")
```

## Response time regression

<!-- Time as secondary interest, Y2 -->
As a secondary explanatory variable, we also want to look at time. First, we take the log transformation of time as it is right-skewed. We repeat the same modeling procedure: 1) build a battery of all additive and multiplicative models. 2) Compare their performance, reporting some top performers. 3) Select a model to examine its coefficients.

<!-- Y2 model comparisons, continue to use ABcd -->
```{r timeCompTbl, fig.cap = "Use the caption arg in kable(), not this."}
kableExtra::kbl(
  mod_comp[[2]], format = fmt, align = c("l", rep("l", 2), rep("c", 5)),
  booktabs = TRUE, linesep = "", row.names = FALSE, escape = TRUE,
  caption = "Model performance regressing on log response time [seconds], $\\widehat{Y_2}$ random effect models, where each model includes random effect terms for participants and simulations. We see the same trade-off where the simplest factor model is preferred by AIC/BIC, while $R^2$ and RMSE is larest in the full multiplicative model. We again select the model $\\alpha * \\beta + \\gamma + \\delta$ to explore further as it has relatively high marginal $R^2$ while having much less complexity than the complete interaction model. Conditional $R^2$ includes the random effects, while marginal does not.") %>%
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = fnt)
```

<!-- Y2 coeffiecients -->
```{r timeCoefTbl, fig.cap = "Use the caption arg in kable(), not this."}
kableExtra::kbl(
  mod_coef[[2]], booktabs = TRUE, linesep = "", format = fmt, escape = TRUE,
  caption = "Model coefficients for log response time [seconds] $\\widehat{Y_2} = \\alpha * \\beta + \\gamma + \\delta$, with factor=PCA, location=0/100\\%, and shape=EEE held as baselines. Location=50/50\\% is the fixed term with the strongest evidence and takes less time. In contrast, the interaction term location=50/50\\%:shape=EEV has the most evidence and takes much longer on average.") %>%
  kableExtra::pack_rows("Factor", 2, 3) %>%
  kableExtra::pack_rows("Fixed effects", 4, 8) %>%
  kableExtra::pack_rows("Interactions", 9, 12) %>%
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = fnt)
```

<!-- No Y2 violin plots -->


### Subjective measures

<!-- Introduce subjective measures from n=84 survey responses -->
The 84 evaluations of the post-study survey also collect four subjective measures for each factor. Figure \@ref(fig:ch4fig6) shows the Likert plots, or stacked percentage bar plots, alongside violin plots with the same non-parametric, ranked sum tests previously used. Participants preferred to use radial for this task. Participants were also more confident of their answers and found radial tours easier than grand tours. All factors have reportedly low familiarity, as expected from crowdsourced participants.

```{r ch4fig6, fig.show = "asis", fig.cap = "The subjective measures of the 84 responses of the post-study survey, five discrete Likert scale levels of agreement (L) Likert plots (stacked percent bar plots) with (R) violin plots of the same measures. Violin plots are overlaid with global significance from the Kruskal-Wallis test, and pairwise significance from the Wilcoxon test, both are non-parametric, ranked sum tests."}
nsSafeIncGraphic("./figures/ch4_fig6_subjective_measures.pdf")
```


## Conclusion {#sec:conclusion}

<!-- context -->
Data visualization is an integral part of EDA. Yet through exploration of data in high dimensions become difficult. Previous methods offer no means for an analyst to impact the projection basis. The manual tour provides a mechanism for changing the contribution of a selected variable to the basis. Giving analysts such control should facilitate the exploration of variable-level sensitivity to the identified structure. We find strong evidence that using the radial tour improves the accuracy relative to PCA or the grand tour on the supervised cluster task assigning variable attribution to the separation of the two clusters.

<!-- Recap study -->
This paper discussed an $n=108$, with-in participant user study comparing the efficacy of three linear projection techniques. The participants performed a supervised cluster task, specifically identifying which variables contribute to the separation between two target clusters. This was evaluated evenly over four experimental factors. In summary, we find strong evidence that using the radial tour leads to a sizable increase in accuracy. There is also evidence for a small change response time, with an increasing order of PCA, grand, and radial. The effect sizes on accuracy are large relative to the change from the other experimental factors, though smaller than the random effect of the participant. The radial tour was subjectively most preferred, leading to more confidence in answers, and increased ease of use than the alternatives.

<!-- Discussion and going further -->
There are several ways that this study could be extended. In addition to expanding the support of the experimental factors, more exciting directions include: changing the type of the task, visualizations used, and experience level of the target population. It is difficult to achieve good coverage given the number of possible permutations. Keep in mind the volume of traffic and low effort of responses from participants when crowdsourcing.


## Accompanying tool: radial tour application {#sec:spinifex}

To accompany this study we have produced an application to illustrate the radial tour. The __R__ package, __spinifex__, [@spyrison_spinifex_2020] is free, open-source and now contains an __shiny__ [@chang_shiny_2020] application that allows users to apply various preprocessing tasks and interactively explore their data via interactive radial tour. Example datasets are provided with the ability to upload data. The .html widget produced is a more interactive variant relative to the one used in the user study. Screen captures and more details are provided in the appendix. Run the following __R__ code will run the application locally.

```{r getting_started, eval=FALSE, echo=TRUE}
## Download:
install.packages("spinifex", dependencies = TRUE)
## Run shiny app:
spinifex::run_app()
```


## Acknowledgments {#sec:acknowledgments}

This research was supported by an Australian government Research Training Program (RTP) scholarship. This article was created in __R__ [@r_core_team_r:_2020] and __rmarkdown__ [@xie_r_2018]. Visuals were prepared with __spinifex__. All packages used are available from the Comprehensive __R__ Archive Network [CRAN](https://CRAN.R-project.org/). The source files for this article, application, data, and analysis can be found on [GitHub](https://github.com/nspyrison/spinifex_study/). The source code for the __spinifex__ package and accompanying shiny application can be found [here](https://github.com/nspyrison/spinifex/).


## Appendix


### Survey participant demographics {-}

The target population is relatively well educated people, as linear projections may prove difficult for generalized consumption. Hence we restrict Prolific.co participants to those with an undergraduate degree (58,700 of the 150,400 users at the time of the study). From this cohort 108 performed a complete study. Of these participants, 84 submitted the post-study survey, who are represented in the following heatmap. All participants were compensated for their time at \pounds 7.50 per hour, with a mean time of about 16 minutes.

```{r ch4zappfig1, echo = F, out.width = '100%', fig.cap = "Heatmaps of survey participant demographics; counts of age group by completed education as faceted across preferred pronoun. Our sample tended to be between 18 and 35 years of age with an undergraduate or graduate degree."}
nsSafeIncGraphic("./figures/ch4_zapp_fig1_survey_demograpics.pdf")
```


### Random effect ranges {-}

<!-- Random effects vs Mean Mark CI by participant and sim -->
Residual plots have no noticeable non-linear trends and contain striped patterns as an artifact from regressing on discrete variables. Figure \@ref(fig:ch4zappfig2) illustrates (T) the effect size of the random terms participant and simulation, or more accurately, the 95\% CI from Gelman simulation of their posterior distribution. The effect size of the participant is much larger than simulation. The most extreme participants are statistically significant at $\alpha = .95$, while none of the simulation effects significantly deviate from the null of having no effect size on the marks. In comparison, (B) 95\% confidence intervals participation and simulation mean accuracy, respectively.

```{r ch4zappfig2, out.width="100%", fig.show="asis", fig.cap="(T) Estimated effect ranges of the random effect terms participant and data simulation of the accuracy model, $\\widehat{Y_1} = \\alpha * \\beta + \\gamma + \\delta$. Confidence intervals are created with Gelman simulation on the effect posterior distributions. The effect size of the participant is relatively large, with several significant extrema. None of the simulations deviate significantly. (B) The ordered distributions of the CI of mean marks follow the same general pattern and give the additional context of how much variation is in the data, an upper limit to the effect range. The effect ranges capture about two-thirds of the range of the data without the model. All intervals for $\\alpha = .95$ confidence."}
nsSafeIncGraphic("./figures/ch4_zapp_fig2_effect_range.pdf")
```

```{r ch4zappfig3, out.width="100%", fig.show="asis", fig.cap="(T) The effect ranges of Gelman resimulation on posterior distributions for the time model, $\\widehat{Y_2} = \\alpha * \\beta + \\gamma + \\delta$. These show the magnitude and distributions of particular participants and simulations. Simulation has a relatively small effect on response time. (B) Confidence intervals for mean log time by participant and simulation. The marginal density shows that the response times are left-skewed after log transformation. Interpreting back to linear time there is quite the spread of response times: $e^{1} = 2.7$, $e^{2.75} = 15.6$, $e^{3.75} = 42.5$ seconds. Of the simulations on the right, the bottom has a large variation in response time, relative to the effect ranges which means that the variation is explained in the terms of the model and not by the simulation itself."}
nsSafeIncGraphic("./figures/ch4_zapp_fig3_T_effect_range.pdf")
```


### Radial tour application details {-}

Below we describe a locally ran __shiny__ app available in the __spinifex__ package. It streamlines creating and interacting with radial tours more interactive than the application used in the user study.

```{r ch4zappfig4, out.width="100%", fig.show="asis", fig.cap="Process data tab, interactively loads or select data, check which variables to project, and optionally scale columns by standard deviation."}
nsSafeIncGraphic("./figures/ch4_zapp_fig4_app_pg1.PNG")
```

In the initial tab, users upload their own (.csv, .rds, or .rda) data or select from predefined data sets. The numeric columns appear as a list of variables to include in the projection. Below that, a line displays whether or not NA rows were removed. Scaling by standard deviation is included by default, as this is a common transformation used to explore linear projections of spaces. Summaries of the raw data and processed numeric data are displayed to illustrate how the data was read and its transformation.

```{r ch4zappfig5, out.width="100%", fig.show="asis", fig.cap="Radial tour tab, interactively create radial tours, changing the manipulation variable, and color or shape of the resulting manual tour. Here, the palmer penguins data is being explored, bill length was selected to manipulate as it is the only variable separating the green cluster from the orange. By mapping shape to island of observation, we also notice that the species in green live on all three islands, while the other species live only on one of the islands."}
nsSafeIncGraphic("./figures/ch4_zapp_fig5_app_pg2.PNG")
```

The second tab contains interaction for selecting the manipulation variable and non-numeric columns can be used to change the color and shape of the data points in the projection. The radial tour is created in real-time animating as an interactive __plotly__ .html widget. The application offers users a fast, intuitive introduction elucidating what the radial tour does and some of the features offered.


<!-- Adds a bib section at the end of every chapter -->

<!--chapter:end:03-efficacy_radial_tour.Rmd-->

---
chapter: 5
knit: "bookdown::render_book"
---

# Scrutinizing the linear variable importance of local explanations of non-linear models with animated linear projections {#ch-cheem}

<!-- WARNING, XXX:TO ADRESS -->
_THE CHAPTER IS NOW SLIGHTLY OUTDATED, Bring content over again after Intro changes._

<!-- Segue -->
Now we can be confident that the radial tour leads to better analysis of variable-level attribution to features identified in a projection. We want to increase the interpretability of complex models. Specifically, I suggest a using the radial tour to explore variable sensitivity to the structure identified in linear local explanations of non-linear models.

<!-- Abstract -->
Artificial Intelligence (AI) has seen a revitalization in recent years from the use of increasingly hard-to-interpret black-box models. In such models, increased predictive power comes at the cost of opaque factor analysis, which has led to the field of eXplainable AI (XAI). XAI attempts to shed light on these models. One such approach is _local explanations_. A local explanation of a model gives a point-estimate of linear variable importance in the vicinity of one observation. We extract explanations for each observation and approximate data, and this attribution space side-by-side with linked brushing. After identifying an observation of interest, its local explanation is used as a 1D projection basis. We then manipulate the magnitude of the variable contributions with the _tour_ technique. The tour animates many projections over small changes in the projection basis as it changes the contribution of one variable. Doing so allows a user to visually explore the data space through the lens of this local explanation and test its support. The implementation of our framework is available as an __R__ package __cheem__ available on CRAN.

## Introduction {#sec:intro}
<!-- Higher-level topics -->

<!-- History of regression and classification -->
Mathematically rigorous approaches to predictive modeling are attributed to the least-squares method, over two centuries ago by Legendre and Gauss in 1805 and 1809, respectively. In 1886 Francis Galton coined the term _regression_ to refer to continuous, quantitative predictions. While _classification_ refers to discrete predictions as introduced by Fisher in 1936.

<!-- Introduce explanatory vs predictive modeling -->
Breiman and Shmueli [@breiman_statistical_2001; @shmueli_explain_2010] introduce the idea of distinguishing modeling based on its purpose; _explanatory_ modeling is done for some inferential purpose such as hypothesis testing, while _predictive_ modeling predicts new, out-of-sample, observations. This distinction draws attention to the divide between interpretable models and black-box models. The intended use has important implications for model selection and development. In explanatory modeling, interpretability is vital for drawing inferential conclusions. While predictive modeling may opt for more accurate non-linear models, the additional hindrance to interpretation should not be taken lightly.

<!-- XAI & interpretability crisis -->
Black-box models are becoming increasingly common, but not without their share of controversy [@oneil_weapons_2016; @kodiyan_overview_2019]. Black-box models have been known to reflect common biases, including sex [@dastin_amazon_2018; @duffy_apple_2019], race [@larson_how_2016], and age [@diaz_addressing_2018]. Such issues occur when biases existent in the training data, the model picks up on this influence on the response variable, which is then built into the model. Another issue is data drift when new data is outside the support of latent or exogenous explanatory variables. Data drift can lead to worse predictions [@lazer_parable_2014; @salzberg_why_2014]. While typically more resistant to data drift, linear models will similarly find biases in the data. However, their transparency goes a long way into exploring and mitigating the biases. Such cases highlight the need to make models fair, accountable, ethical, and transparent, which has led to the movement of XAI [@adadi_peeking_2018; @arrieta_explainable_2020].

<!-- Local explanations -->
One branch of XAI is local explanations, which take a variable attribution approach to bring transparency to a model. Local explanations attempt to approximate linear variable importance at the location of one observation. There are many such local explanations.


<!-- Data visualization tours -->
In multivariate data visualization, a _tour_ [@asimov_grand_1985; @buja_grand_1986; @lee_state_2021] is a sequence of linear projections of data onto a lower-dimensional space, typically 1-3D. Tours are viewed as an animation over minor changes to the projection basis. Structure in a projection can then be explored visually to see which variables contribute to the formation of that structure. The intuition is similar to watching the shadow of a hidden 3D object change as the object is rotated; watching the shape of the shadow change conveys information of the structure and features of the object.

<!-- Manual tours --> 
There are various types of tours distinguished by the generation of projection bases. In a _manual_ tour [@cook_manual_1997; @spyrison_spinifex_2020], the path is defined by changing the contribution of a selected variable. <!--tours and models --> Applying tours to models has been done in a couple of contexts. Specifically for exploring various statistical model fits and classification boundaries [@wickham_visualizing_2015], and using tree- and forest-based approaches as a projection pursuit index to generate a tour basis path [@lee_pptree_2013; @da_silva_projection_2021].

<!-- Purposed approach -->
The proposed approach uses the radial, manual tour to scrutinize a local explanation. After identifying an observation of interest, its explanation can be evaluated by testing the support of the structure identified by the explanation as the contributions of the variables are varied with the radial tour. We provide a free and open-source R package __cheem__ with an interactive application to facilitate analysis. We provide toy and modern datasets case studies for both classification and regression tasks.

<!-- What-if & counterfactual analysis -->
The change in the projection basis might feel similar to counterfactual, what-if analysis, such as _ceteris paribus_ [@biecek_ceterisparibus_2020]. This phrase, Latin for “other things held constant” or “all else unchanged”, shows how an observation's prediction would change from a marginal change in one explanatory variable given that other variables are held constant. It ignores correlations of the variables and imagines a case that was not observed. In contrast, our approach is a geometric explanation of the factual; it varies contributions of the variables by rotating the basis, a reorientation of the data object. Another difference is that the basis must remain orthonormal. That is to say, when the contribution of one variable decreases, the contributions of others necessarily increase such that there is a complete component in that direction.

<!-- Paper structure -->
The remainder of this paper is organized as follows. The following section, [Local explanation statistics](#sec:explanations), covers the background of the local explanation, SHAP, and the traditional visuals produced from it. [Tours and the radial tour](#sec:tour) digs deeper into these animations of continuous linear projections. The section [Application Design](#sec:applicationdesign) discusses the visual layouts, how they facilitates analysis, data preprocessing, and package infrastructure. The section [Case Studies](#sec:casestudies) illustrates several applications of this method. We conclude with a [Discussion](#sec:cheemdiscussion) of the insights we draw from classification and regression tasks.


## Local explanation statistics {#sec:explanations}

<!-- Reminder of local explanation -->
Consider a highly non-linear model. At face value, it is hard to say which variable(s) are sensitive to  crossing a classification boundary or identify which variables caused an observation to have a relatively extreme residual. Local explanations shed light on these cases by approximating linear variable importance in the vicinity of one observation.

<!-- Taxonomy of local explanations -->
Figure six of @arrieta_explainable_2020 gives comprehensive summarization of the taxonomy and literature of explanation techniques. The figure includes a large number of model-specific explanations such as deepLIFT, [@shrikumar_not_2016; @shrikumar_learning_2017] a popular recursive method for estimating importance in neural networks. There are fewer model-agnostic explanations, of which LIME, [@ribeiro_why_2016] SHAP, [@lundberg_unified_2017] and their variants are popular.

<!-- Uses of local explanations -->
These instance-level explanations are used in various ways depending on the data. In images, saliency maps overlay or offset a heatmap indicating which pixels were necessary [@simonyan_deep_2014]. For instance, snow may be highlighted when distinguishing if a picture contains a wolf or husky. In text analysis, word-level contextual sentiment analysis can be used to highlight the sentiment and magnitude of influential words [@vanni_textual_2018]. In the case of numeric regression, they are used to explain variable additive contributions from the model intercept to the observation's prediction [@ribeiro_why_2016].


### SHAP and tree SHAP

<!-- SHAP and history -->
SHaply Additive exPlanations (SHAP)[@strumbelj_efficient_2010; @strumbelj_explaining_2014] approximates the variable importance in the vicinity of one observation by taking the median importance of a subset of permutations in the explanatory variables. This idea stems from the field of game theory, where @shapley_value_1953 devised a method to evaluate an individual's contribution to cooperative games by permuting the players that contribute to the score.


<!-- Fifa example -->
To illustrate SHAP and its original use, explaining the difference between the intercept and an observation's prediction, we use soccer data from FIFA 2020 season [@leone_fifa_2020]. We have 5000 observations of nine skill measures (after aggregating highly correlated variables). A random forest model is fit to regress the log wages, in 2020 Euros, from the skill measures. We then extract the SHAP values of a star offensive player (L. Messi) and defensive player (V. van Dijk). We expect to see a difference in the attribution of the variable importance across the two positions of the players.

```{r ch5fig1, echo=FALSE, out.width = "100%", fig.align='center', fig.cap = "Illustration of the distribution of SHAP attributions and a break-down plot. From FIFA 2020 data, a random forest model regresses wages from nine skill attributes for a star offensive and defensive player. The players have very different salaries, but a) shows the distributions of 25 permutations in the explanatory variables. The medians of these distributions are the final SHAP values. The variable importance differs across the exogenous information of player position. These explanations make sense; the variable importances seem realistic given the player's positions. b) Break-down plots of the observations using their explanations to additively cover the difference between the model intercept and the observation predictions."}
nsSafeIncGraphic("./figures/ch5_fig1_shap_distr_bd.png")
```

<!-- Illustration -->
Figure \@ref(fig:ch5fig1) shows the SHAP values of these players. Panel a) shows these players receive a sizable difference in wages. Panel b) shows the underlying distribution of the SHAP attributions while permuting the explanatory variables, with the medians being the SHAP values. In the light of the player position, the difference in the variable importance makes sense; offense and movement skills are more important for the offensive player, while defensive and power skills are more informative to the model for explaining the prediction of the defensive player. We would likewise expect the profile of variable importance to be unique for star players of other positions, such as goalkeepers or middle fielders. Panel b) shows a simplified break-down plot [@gosiewska_ibreakdown_2019], where a local explanation is used to additively explain the difference from the intercept to the observation's prediction. Such additive approaches will show asymmetry in their variable ordering, so we opt to fix the order to panel a; by the decreasing sum of the SHAP values.

<!-- Segue -->
In summary, this figure highlights how local explanations bring interpretability to a model, at least in the vicinity of their observations. In this instance, two players with different positions receive different profiles of variable importance to explain the prediction of their wages. In application we apply _tree SHAP_, a variant of SHAP that enjoys a lower computational complexity [@lundberg_consistent_2018]. Tree SHAP only compatible with tree-based models; we illustrate on random forests. In the following section, we will use normalized explanations as the starting projection basis to further scrutinize the explanation.

## Tours and the radial tour

<!-- Tours intro -->
A data visualization _tour_ animates many linear projections over small changes in the basis. One of the critical features of the tour is the object permanence of the data points; one can track the relative changes of observations as the basis moves, as opposed to discretely jumping to an orthogonal view with no intermediate information. There are various types of tours that are distinguished by selecting their basis paths [@lee_state_2021; @cook_grand_2008].


### Manual tours and its radial case

<!-- Manual tour -->
The manual tour [@cook_manual_1997] defines its basis path by manipulating a selected variable's basis contribution. A manipulation dimension is appended onto the projection plane, giving a full contribution to the chosen variable. The bases are then selected based on rotating this newly created manipulation space. A crucial feature of the manual tour is that it allows users to control the variable contributions of the basis. Such manipulations can be selected and queued in advance or selected on the spot for human-in-the-loop analysis [@karwowski_international_2006]. However, this navigation is relatively time-consuming due to the vast volume of the display-space. It is advisable to use this method to explore the sensitivity of the variable contribution to a previously identified feature of interest. In this case, the projection of the normalized explanations is the feature of interest.

<!-- Radial tour -->
More generally, the manual tour can change the contribution of a variable to the display dimensions. We will apply a more directed interaction, namely, a _radial_ tour. In a radial tour, the selected variable is allowed to change its magnitude of contribution but not its angle; it must move along the direction of its original contribution.

## Cheem viewer {#sec:applicationdesign}

Below we illustrate the two primary displays of the cheem viewer application: the global view and the tour view. Then we cover what we take away from the classification and regression tasks. Lastly, we discuss the preprocessing the data before application runtime.


### Global view

<!-- Purpose -->
The global view provides an essential context of all observations and facilitates the exploration of the separability of the data- and attribution-spaces. While comparison of these spaces is interesting, the global view's purpose is to facilitate selecting observations. The local explanation of these points will be explored in more detail.

<!-- Approximations of the spaces, PC1:2 -->
An approximation of these spaces is given as the first two principal components of their respective spaces. Model information, observed response by its prediction, is also provided. The orientation and magnitude of the variables are inscribed on a unit circle. <!-- Interactions -->Misclassified observations are circled in red if applicable. Linked brushing between the plots and tabular display of select points facilitates exploration of the spaces and the model. A single 2D projection will not encompass all of the structure of higher-dimensional space. However, it is a reasonable summarization given the real task at hand; the selection of observations to explore further.


### Radial cheem tour

<!-- A function of the obs selected -->
The global view facilitated the selection of a primary and optional comparison observation. The variable-level attribution of the primary observation is normalized and used as the initial 1D basis in a radial tour. This is an approximation of the contributions of the linear variables that best explain the difference between the model intercept and an observation's prediction, not the local shape of the model surface.

<!-- Tour start frame -->
The initial frame is the normalized SHAP values of the primary observation. The current projection basis is depicted as the width of a bar, the variable's contribution to the horizontal axis. The normalized values of all observations are shown as vertical parallel coordinate plots.

<!-- Tour animation -->
The radial tour creates a basis path by varying the contribution of a selected variable, fully into and out of a projection frame. Doing so tests an individual variable's sensitivity to the structure identified by the local explanation.The default variable selected has the largest discrepancy between the attribution of primary and comparison observations. The following sections elaborate on the takeaways we draw from applying this approach in classification and regression tasks, respectively. <!-- Segue to classification -->Now that we have introduced the global view and corresponding cheem radial tour let us discuss the differences between the classification and regression cases.


### Classification task

What information do we glean from using this method on a classification task? Typically we select a misclassified observation compared to a correctly classified point nearby in data space. The initial frame is the linear attribution of that observation's local explanation. By default, the manual tour varies the contribution of the variable with the largest difference between the primary and comparison observation. That is, we can test the sensitivity of each variable to structure identified by the local explanation; we are exploring the support of the explanation, evaluating the support or robustness of the prediction.

```{r ch5fig2, echo=F, out.width="100%", fig.align="center",  fig.cap = "Display illustrating the classification case. Plots are  colored on predicted class and red circles indicate misclassified observations. The radial tour is a 1D projection starting at the normalized tree SHAP values of the primary point. The first frame is the linear-variable importances that best describe the difference from model intercept to this observation's prediction. We probe the support of the variable contributions by selecting a variable to vary the contribution."}
nsSafeIncGraphic("./figures/ch5_fig2_app_classification.PNG")
```


### Regression task

In the regression case, the global view can be colored on a statistic to highlight the structure in the explanation space, including residuals, log Mahalanobis distance of data space (a measure of outlyingness), and the correlation of the attribution projection with the observed response. In the radial tour, the horizontal positions are the same, the basis projection of the radial tour. The vertical position is fixed to the observed response variable and residuals in the middle and right panels, respectively. Correspondingly, the display changes from univariate density to 2D scatterplot. The basis is still one component (horizontal) independent of the vertical position.

```{r ch5fig3, echo=FALSE, out.width="100%", fig.align='center', fig.cap="Display of the regression task. The global view can be colored on the correlation of the attribution projection and observed response. In the tour, the horizontal values are the same as the classification case; the projection through the basis. The vertical position is now maped to the observed y and residuals."}
nsSafeIncGraphic("./figures/ch5_fig3_app_regression.PNG")
```

### Interactive features

<!-- Reactive vs exploration interactions -->
The application has several reactive inputs that affect the data used, aesthetic display, and tour manipulation. These reactive inputs make the software flexible and extensible. The application also has It also has more exploratory interactions to help link points in the data and extract other information not on plot axes.

<!-- Exploratory interactions -->
A tooltip displays observation number/name and classification information while the cursor hovers over a point. Linked brushing allows the selection of points (left click and drag) where those points will be highlighted across plots. The information corresponding to the selected points is populated on a dynamic table. These interactions aid exploration of the spaces and, finally, identification of a primary and comparison observation.

```{r ch5fig4, echo=F, out.width="100%", fig.cap = "Illustration of data explorations interactions in the global view. This view has linked brushing of the points where observations selected in one facet are highlighted in the other facets and populate an interactive tabular display below. Tooltips display when hovering over an obersvation."}
nsSafeIncGraphic("./figures/ch5_fig4_app_interactions.PNG")
```


### Preprocessing

It is vital to mitigate the render time of visuals, especially when users may want to iterate many times. In order to keep render time low all of the computational operations should be prepared before runtime. The work remaining at runtime is solely reacting to inputs and rendering of visuals and tables. Below we discuss the steps and details of the reprocessing.

(ref:citeRf) @liaw_classification_2002
(ref:citeTs) @kominsarczyk_treeshap_2021
\begin{itemize}
	\item \textbf{Data:} a complete numerical matrix; explanatory and response variable. An optional categorical variable can be mapped to the color and shape of observations. Explanatory variables are scaled in visualization after modeling or creating local explanations. 
	\item \textbf{Model:} any model can be used with this method. Currently, we apply random forest models via the package \textbf{randomForest} [(ref:citeRf)] for compatibility with the local explanation, which requires tree-based models.
	\item \textbf{Local explanation:} any model-compatible linear explanation could be used. We apply tree SHAP, a more computationally efficient variant of SHAP, compatible with tree-based models. Tree SHAP was calculated with the package \textbf{treeshap} [(ref:citeTs), hosted on GitHub only]. The global view shows all observations in attribution space, requiring the variable importance from \emph{all} observations rather than just one.
\end{itemize}

<!-- Note on time of execution -->
The time to preprocess the data will vary significantly with the model and local explanation. For reference, the FIFA data, 5000 observations of nine explanatory variables, took 2.9 seconds to fit a random forest model of modest hyperparameters. Extracting the tree SHAP values of each observation took 254 seconds combined. PCA and statistics of the variables and attributions took 0.6 seconds. These runtimes were from a non-parallelized R session on a modern laptop, but suffice to say that the bulk of the time will be spent on the local attribution. An increased of model complexity or data dimensionality will quickly become an obstacle. This makes tree SHAP, with its reduced computational complexity, a good candidate to start with. Alternatively, the package __fastshap__ [@greenwell_fastshap_2020] claims extremely low runtimes, which are attributed to fewer calls to the prediction function, partial implementation in C++, and efficient use of logical subsetting.


### Package infrastructure {#sec:infrastructure}

The above-described method and application are implemented as an open-source __R__ package, __cheem__ available on [CRAN](https://CRAN.R-project.org/package=cheem). Preprocessing was facilitated with models created via __randomForest__ [@liaw_classification_2002], and explanations calculated with __treeshap__ [@kominsarczyk_treeshap_2021]. The application was made with __shiny__ [@chang_shiny_2021]. The tour visual is built with __spinifex__ [@spyrison_spinifex_2020]. Both views are created first with first with __ggplot2__ [@wickham_ggplot2_2016] and then rendered as interactive HTML widgets with __plotly__ [@sievert_interactive_2020]. __DALEX__ [@biecek_dalex_2018] and the free ebook, _Explanatory Model Analysis_ [@biecek_explanatory_2021] were a huge boon to understanding local explanations and how to apply them.

### Installation and getting started

The following __R__ code will help getting up and running:

```{r eval=FALSE, echo=TRUE}
## Download the package
install.packages("cheem", dependencies = TRUE)
## Restart the R session so the IDE has the correct directory structure
restartSession()
## Load cheem into session
library("cheem")
## Try the app
run_app()

# Processing your data
## Install treeshap from github, to use as a local explainer
remotes::install_github('ModelOriented/treeshap') ## Local 
## Follow the examples in cheem_ls()
?cheem_ls
```


## Case studies {#sec:casestudies}

To illustrate the use of this analysis, we apply it to modern datasets, two classification examples and then two of regression. 

### 1) Penguin, species classification

Palmer penguins data [@gorman_ecological_2014; @horst_palmerpenguins_2020] consist of 330 observations across four physical measurements of three species of penguins foraging near Palmer Station, Antarctica. A random forest model was fit, classifying the species of the penguin given the physical measurements.

```{r casepenguins, echo=F, out.width = "100%", fig.cap = "Species classification of Palmer penguin data."}
nsSafeIncGraphic("./figures/ch5_fig5_case1_penguins.png")
```

In figure \@ref(fig:casepenguins), a misclassified point is contrasted with a correctly classified point of its observed class nearby in data-space. The attribution space from the tree SHAP local explanations is a more separable space, where the comparison is squarely in the middle of the orange distribution. The primary observation is between the predicted and observed clusters, a sign of uncertainty in the prediction. The tour varies the contribution of bill length (b_l) as this variable differs most from the contribution of the comparison observation. Downplaying the contribution of bill length is crucial to the linear explanation of this observation being misclassified.

### 2) Chocolates, milk/dark chocolate classification

The chocolates dataset consists of 88 observations of 10 nutritional measurements from their labels. Each of which was labeled as being either milk or dark chocolates. With this data, we can see if a manufacturer accurately portrays the chocolate. We are curious to see if there are chocolates that nutritionally look like milk chocolates labeled as dark chocolates, which may hold a higher market value. We should note that not all chocolates consist wholly of chocolate. The addition of other ingredients will decrease the predictive power of the model nutritional explanatory variable. A random forest model is fit classifying the type of chocolate. We selected a chocolate labeled dark, through predicted to be milk chocolate compared with a chocolate labeled 85% cocoa.

```{r casechocolates, echo=F, out.width = "100%", fig.cap = "Chocolates data type classification (milk or dark)."}
nsSafeIncGraphic("./figures/ch5_fig6_case2_chocolates.png")
```

In figure \@ref(fig:casechocolates), we similarly see that attribution space is more separable than data-space. Interestingly, the class imbalance that we suspected was not observed; there are only six chocolates labeled as dark and predicted as milk, while eight of the inverse case. Calories from fat is the variable with the largest divide in treeshap attribution between these points.


### 3) FIFA, wage regression

The 2020 season FIFA data [@leone_fifa_2020; @biecek_dalex_2018] contains many skill measurements of soccer/football players and wage information. After aggregation of the skill measurements, we regress the log wages [2020 euros] given just the skill aggregates. A model was fit from 5000 observations of the nine skill aggregates before being thinned to 500 players to mitigate occlusion and render time. We compare a leading offensive fielder (L. Messi) with that of a top defensive fielder (V. van Dijk), the same observations were used in figure \@ref(fig:ch5fig1).

```{r casefifa, echo=F, out.width = "100%", fig.cap = "FIFA 2020, regressing log wages [2020 Euros] from aggregations of skill measurements. The primary observation is a star offensive player (L. Messi) compared with a top defensive player (V. van Dijk)."}
nsSafeIncGraphic("./figures/ch5_fig7_case3_fifa.png")
```

With figure \@ref(fig:casefifa), we will test the premise of the local explanation. If we remove reaction and movement skills from the basis, then offense skills have almost singular importance for explaining the offensive player. We vary the contribution of offensive skills. In the tour (panel b, frame 3), offensive skills are removed, and Messi is no longer separated from the group. We also notice that accuracy has rotated into the frame, maintaining some separability.


### 4) Ames housing 2018, sales price regression

Ames 2018, housing data was subset to North Ames (the neighborhood with the most house sales). The remaining are 338 house sales across nine variables. Using interaction from the global view, we select a house with an extreme negative residual and an accurate observation close to it in the data.

```{r caseames, echo=F, out.width = "100%", fig.cap = "Ames housing 2018 regressing log sales price [2018 USD]."}
nsSafeIncGraphic("./figures/ch5_fig8_case4_ames2018.png")
```

Figure \@ref(fig:caseames) shows the global view and extrema of the tour. The horizontal distance in the tour didn't show a significant disparity between our selected points. This is not particularly surprising as most variables have a sizable contributions. Rotating any one variable out of the frame will rotate other vital variables into the frame, preserving most of the distance from intercept to prediction. However, the tour has revealed an interesting feature worth discussing. Notice that the observations pivot about the origin, the basis roughly halfway between bases in frames one and two of panel b) the data is near a singular profile. This means that there is a basis orthogonal to this point that describes sizable variation. Knowing these singular bases can
point toward others with meaningful data variation.


## Discussion {#sec:cheemdiscussion}

The need to maintain the interpretability of black-box models is evident. One aspect uses local explanations of the model in the vicinity of an observation. Local explanations approximate the linear variable importance to the model. Our contribution is to assess explanations by examining the support by varying the contributions with a radial tour. First, a global view visualizes approximations of the data and explanation spaces side-by-side, using dynamic interaction to compare and contrast and, identify primary and comparison observations of interest. The normalized linear importance from the explanation of the primary observation becomes the feature of interest to further explore with radial tour. The variable sensitivity to the structure identified in the explanation is explored by the tours.

We have illustrated this method on random forest models using the tree SHAP local explanation, while it could be generally used with any compatible model-explanation pairing. We apply it to the classification and regression tasks. We have created an open-source __R__ package __cheem__, available on [CRAN](https://CRAN.R-project.org/package=cheem), to facilitate preprocessing and exploration with the described interactive application. Toy and real data are provided, or upload your data after preprocessing.


## Acknowledgments

We would like to thank Professor Przemyslaw Biecek for his input early in the project and to the broader $\text{MI}^\text2$ lab group for the __DALEX__ ecosystem of __R__ and __Python__ packages. This research was supported by Australian Government Research Training Program (RTP) scholarships. Thanks to Jieyang Chong for helping proofread this piece.

The namesake, Cheem, refers to a fictional race of humanoid trees from Doctor Who lore. __DALEX__ pulls on from that universe, and we initially apply tree SHAP explanations specific to tree-based models.


<!-- Adds a bib section at the end of every chapter -->

<!--chapter:end:04-cheem.Rmd-->

---
chapter: 6
knit: "bookdown::render_book"
---

# Conclusion {#ch-conclusion}

<!-- Context -->
We know that visualizing data is more robust than numerical summarization alone. It provides a means to rapidly get a feel for the data, check for erroneous values, and confirm model assumptions. But visualizing data spaces becomes increasingly complex as dimensionality increases. Linear dimension reduction and especially dynamic animations of linear projections, tours, seems to be a fruitful way to extend data visualization as dimensionality increases. Previously we haven't had a means of testing the sensitivity of individual variables' contribution to the structure in one embedding. In Chapter \@ref(ch-spinifex) introduced the __spinifex__ package, which facilitates user-controlled manual tours and extends the exporting of any tour, interoperability with __tourr__. Use of the radial, manual tour does sizably improve the accuracy on variable-level supervised cluster separation task compared with PCA and the grand tour. Chapter \@ref(ch-efficacy_radial_tour) covers the with-in participants user study that led to this conclusion. Finally, we extended the use of the manual tour to increase the interpretability of non-linear models by using it to explore their local explanations in the package __cheem__ is discussed in \@ref(ch-cheem).

<!-- user study -->
<!-- Chapter \ref{ch-efficacy_radial_tour} discussed a user study comparing the radial tour against PCA and the grand tour. Participants performed a variable attribution task of the supervised cluster separation. We define an accuracy measure for this task and the additional experimental factors location, shape, and dimensionality. I find strong evidence that use of the radial tour leads to high accuracy for this task. Using mixed model regression, I partition the error term of the model into the variation from participant's skill, and variation in difficult of the simulations due to random draws. -->


## Software development

<!-- spinifex -->
The __spinifex__ package facilitates the preprocess transformations with `scale_*` functions, identifies features with `basis_*` functions, and allows user-control over variable contributions to the basis with manual tours. It creates a framework for layered creation of tour visuals with `proto_*` functions will feel at home to __ggplot2__ users. Manual tours and those created from __tourr__ can be rendered and exported as interactive HTML widgets, .gif, or .mp4 files. Vignettes and interactive an application help users get started. It has been downloaded over 14,400 times from CRAN between 09 April 2019 and 28 November 2021. The package is available on [CRAN](https://CRAN.R-project.org/package=spinifex) with vignettes and version notes on its [pkgdown](https://nspyrison.github.io/spinifex/) site. My contributions __spinifex__ and __tourr__ won the ACEMS Impact and Engagement Award, 2018.

<!-- cheem -->
In the __cheem__ package, I suggest using manual tours to extend the interpretability of black-box models by exploring their local explanations. A workflow is developed to facilitate the creation of a random forest model, calculating tree SHAP local explanations for each observation. An interactive application illustrates the purposed visual and analysis from several datasets or upload your data after processing. Local explanations are evaluated through testing the sensitivity of the variables to the structure identified in the explanation. __cheem__ was recently uploaded to [CRAN](https://CRAN.R-project.org/package=cheem) and has a corresponding [pkgdown site](https://nspyrison.github.io/cheem/).


## Further extensions

<!-- General tour extension -->
In addition to controlling the contribution of a single variable, it may be insightful to able to change contributions of several at once (manipulating on a linear combination). A sort of dimension reduction tour that would append several manual tours together, zeroing the contributions of variables one at a time may prove interesting in the vicinity of the a features intrinsic dimensionality.

<!-- spinifex --> 
There are direct natural extensions to __spinifex__, such as extending the type of protos available, such as adding a text table of the basis, convex hulls, alpha hulls, and a 'high density region' display where the bulk of the data is shown as density contour, while the outer most observations are displayed as points [@hyndman_computing_1996]. An additional interpolation of the manual tour could correlate the frame number with the magnitude of a radial tour; the first frame would contain zero magnitude, and the last contains a full contribution, while the default starting frame would  initial value.

<!-- 2.5d & XR -->
Experiencing tours as 3D scatterplots in extended reality with stereoscopically true head tracking may be fruitful. [@nelson_xgobi_1998] explore 2D tour is in virtual reality. Other works view 3D scatterplot tours on 2D displays [@yang_3d_1999; @yang_interactive_2000]. It would be interesting to see modern implementations using WebGL, Mozilla A-frame, or Unity. One concern would be trying to keep hardware and software as generalized as possible.

<!-- cheem -->
Similarly, __cheem__ analysis could be generalized to a broader range models and local explanations. The models and local explanations facilitated by `DALEX::explain()` seems to be a good starting point [@biecek_dalex_2018; @biecek_explanatory_2021]. Alternatively, there may be other statistics that better show the structure identified by explanations that could be added.

<!-- tour apps --> 
<!-- In addition to application that involve one tour variant such as the applications we have discussed or __liminal__ [@lee_casting_2020], a general more consolidated application with multiple linked brushing would be nice to have. There have been many takes on applications to facilitate the exploration of multivariate data and the audience that know of and uses tours remains small. Tour-based applications [@fisherkeller_prim-9:_1974; @swayne_xgobi:_1991; @hardle_xplore:_1995; @carr_explorn:_1996; @nelson_xgobi_1998; @sutherland_orca:_2000; @swayne_ggobi:_2003] and other multi-variate data visualization applications [@carr_explor4:_1988; @tierney_lisp-stat:_1990; @huh_davis:_2002; @wegman_visual_2003; @jeong_ipca:_2009, hadjar_webvr_2018; cordeil_iatk_2019]. -->


## Other contributions

In addition to the research discussed around the thesis of the manual and radial, manual tour other notable contributions during my candidature include:

- _The state-of-the-art on tours for dynamic visualization of high-dimensional data_ [@lee_state_2021], WIREs Computational Statistics. Review of current tour methods. I contributed writing and manual tour frame discussing manual tours.
- _"Is IEEE VIS that good?" On key factors in the initial assessment of manuscript and venue quality_ [@spyrison_is_2021]. A survey IEEE VIS authors, how they source articles, decide which to read, and evaluate venue quality. We find low evidence that sentiment changes across academic positions for these topics and provide commentary and discussion on the effects of "publish or perish" environment, standard author and journal metrics, and the need to publish "null" findings and replication studies.
- _Intraday effect of COVID-19 restrictions on Melbourne electricity consumption_ [@barrow_changes_2020]. We corroborate that the Victorian interday effect on energy consumption did not change, and novelly find that the intraday distribution of energy consumption does change. Namely, we find a statistically significant change in the height of the morning and evening peak, energy consumption that we posit is due to less strict schedules associated with working from home, absence of commute time, and other employment changes. We were awarded 1st place of hundreds of entries in the insights category of the Melbourne 2020 Datathon.
- Student volunteering at three conferences: UseR!2021 - Online, CHI Down Under 2020 - Online, and UseR!2018 - Brisbane, Australia.

<!--chapter:end:05-conclusion.Rmd-->

<!---

# Appendix {#ch-appendix}

\appendix

ADD SPINIFEX VIGNETTE GGPROTO API HERE? after first paper?


<!-- at the end, create bib for html version ->
`r if (knitr::is_html_output()) '# Bibliography {-}'`

--->



<!--
# Glossary {#ch-glossary}

## Notation {#sec:notation}

Tour notation varies across articles and authors. In my work, I use the following:

* $n$, number of observations in the data.
* $p$, number of numeric variables, the dimensionality of data space.
* $d$, the dimensionality of projection space.
* $\textbf{X}_{[n,~p]}$, a data matrix in variable-space, $\textbf{X} \in \mathbb{R}^{p}$. Typically centered, scaled, and optionally sphered.
* $\textbf{A}_{[p,~d]}$, an orthonormal matrix (columns are independant, at right angles with eachother and each normalized to a length of 1). This is the linear combination of variables from the orignal space to a lower embedding space. That is, the orientation of the variables, mapping from   $p-$ to $d-$space.
* $\textbf{Y}_{[n,~d]}$, projected data matrix in projection-space, $\textbf{Y} \in \mathbb{R}^{d}$.
    * In chapter 
* Reference axes, a display showing how each variables coefficient(s) contribute to a projection. Either on its own axis (1D) or relative to a unit circle (2D).
* Geometric objects are referred to in generalized dimensions; the use of the term plane is not necessarily a 2D surface, but a hyperplane in the arbitrary dimensions of the projection space.

<!-- 
## Data visualization terminology {#sec:terminology}

* 2D - representation of data in 2 dimensions, without the use of depth perception cues and minimal aesthetic mapping (such as color, size, and height) to data points.
* 2.5D - following the definition given in @ware_designing_2000: visualizations that are essentially 2D but select depth cues are used to provide some suggestion of 3D. However, the term 2.5D is commonly used for several meanings *due to the ambiguous use of 2.5D, this document errs on the side stating 3D with descriptions of depth cues used*.
* 3D - visualizations of 3 dimensions with liberal use of depth cues unless otherwise qualified.
* Depth perception cues - an indication that indicates the depth to an observer, including:
    * linear perspective - the property of parallel lines converging on a vanishing point.
    * aerial perspective - objects that far away have lower contrast and color saturation due to light scattering in the atmosphere.
    * occultation (or interposition) - where closer objects partially block the view of further objects.
    * motion perspective/parallax - closer objects, move across the field of view faster than further objects.
    * accommodation - the change of focal length due to change in the shape of the eye. Effective for distances of less than 2 meters.
    * binocular stereopsis/disparity - the use of 2 images of slightly varied angles from the horizontal distance of the eyes. The disparity for distant objects is small, but it is significant for nearby objects.
    * binocular convergence - The ocular-motor cue due to stereopsis focusing on the same objects. Convergence is effective for distances up to 10 meters.
* Virtual reality (VR) - an immersive experience of computer-generated sensory input.
* Augmented reality (AR) - view of physical spaces with augmenting/ supplementing sensory input of information.
* Mixed reality (MR) - a mix of physical and virtual realities with objects from both interacting in real time. This differs from AR by the flow of interaction; AR augments physical reality while MR has reciprocating interactions.
* Extended reality (XR) - any degree of virtual, augmented, or mixed reality.
* Scatterplot matrices (SPLOMs) - matrix display of pair-wise 2D scatterplots, sometimes with 1D density on the diagonal.
* Human-in-the-loop - any model that requires human interaction [@karwowski_international_2006]. -->

<!--
### Notation {#sec:terminology_tours}
Notation moved to appendix A in 90-appA.Rmd.

### History

The first tour was introduced was the *grand tour* in @asimov_grand_1985 at the Stanford Linear Accelerator, Stanford University. Asimov suggested three types of grand tours: torus, at-random, and random-walk. The original application of tours was performed on high energy physics on the PRIM-9 system.

Before choosing projection paths randomly, an exhaustive search of $p-$space was suggested by @mcdonald_interactive_1982, also at the Stanford Linear Accelerator. This was later coined *little tour*.
<!-- Buja and Asimov were in the acknowledgments.

@friedman_projection_1974 and later @huber_projection_1985 recommended projection pursuit (also referred to as PP). Projection pursuit optimizes an objective function before it removes a single component/variable and then iterates in this newly embedded subspace. Within each subspace, the projection seeks for a local extremum via a hill climbing algorithm on an objective function. This formed the basis for *guided tours* suggested by @hurley_analyzing_1990.

The grand and little tours have no input from the user aside from the starting basis. Guided tours allow for an index to be selected. The bulk of tour development since has largely been around the dynamic display, user interaction, geometric representation, and application.
<!--Note c2 paper, pointing to: Buja & Asimov 1986, Hurley & Buja 1990, Wegman 1991, Cook, Buja, Cabrera, & Hurley 1995, Buja, Cook Asimov & Hurley 1997,Cook & Buja 1997.


### Path generation {#sec:path_generation}

A fundamental aspect of tours is the path of rotation. There are four primary distinctions of tour path generation [@buja_computational_2005]: random choice, data-driven, precomputed choice, and manual control.

* Random choice, *grand tour*, constrained random walks $p$-space. Paths are constrained for changes in direction small enough to maintain continuity and aid in user comprehension
    + torus-surface [@asimov_grand_1985]
    + at-random [@asimov_grand_1985]
    + random-walk [@asimov_grand_1985]
    + *local tour* [@wickham_tourr_2011], a sort of grand tour on a leash, such that it goes to a nearby random projection before returning to the original position and iterating to a new nearby projection.
* Data-driven, *guided tour*, optimizing some objective function/index within the projection-space, called projection pursuit (PP) [@hurley_analyzing_1990], including the following indexes:
    + holes [@cook_projection_1993] - moves points away from the center.
    + cmass [@cook_projection_1993] - moves points toward the center.
    + lda [@lee_projection_2005] - linear discriminant analysis, seeks a projection where 2 or more classes are most separated.
    + pda [@lee_projection_2010] - penalized discriminant analysis for use in highly correlated variables when classification is needed.
    + convex [@laa_using_2019] - the ratio of the area of convex and alpha hulls.
    + skinny [@laa_using_2019] - the ratio of the perimeter distance to the area of the alpha hull.
    + stringy [@laa_using_2019] - based on the minimum spanning tree (MST), the diameter of the MST over the length of the MST.
    + dcor2D [@grimm_mbgraphic:_2017; @laa_using_2019] - distance correlation that finds linear and non-linear dependencies between variables.
    + splines2D [@grimm_mbgraphic:_2017; @laa_using_2019] - measure of non-linear dependence by fitting spline models.
    + other user-defined objective indices can be applied to the framework provided in the *tourr* package @wickham_tourr_2011.
    + Another data-drive tour is the *dependence tour*, a combination of $n$ independent 1D tours. A vector describes the axis each variable will be displayed on. for example $c(1, 1, 2, 2)$ is a 4- to 2D tour with the first 2 variables on the first axis, and the remaining on the second.
        - *correlation tour* [@buja_data_1987], a special case of the dependence tour, analogous to canonical correlation analysis.
* Precomputed choice, *planned tour*, in which the path has already been generated or defined.
    + *little tour* [@mcdonald_interactive_1982], where every permutation of variables is stepped through in order, analogous to brute-force or exhaustive search.
    + a saved path of any other tour, typically an array of basis targets to interpolate between.
* Manual control, *manual tour*, a constrained rotation on selected manipulation variable and magnitude [@cook_manual_1997]. Typically used to explore the local area after identifying an interesting feature, perhaps via guided tour.


### Interpolation

NOTE: Search 'Interpolator' in Wickham’s tourr paper
After target bases are identified, the frames in-between need to be filled in. There are several methods to do so:

* Geodesic - via Gram-Schmidt process
* Givens rotations
* Householder reflections

### Path evaluation

Consider projection down to 2D, then each projection is called a 2-frame (each spanning a 2-plane). Mathematically, a Grassmannian is the set of all possible unoriented 2-frames in $p$-space, $\textbf{Gr}(2,~p)$. @asimov_grand_1985 pointed out that the unique 2-frames of the grand tour approaches $\textbf{Gr}(2,~p)$ as time goes to infinity. The *density* of a tour is defined as the fraction of the Grassmannian explored. Ideally, an exploring tour will be dense, but the time taken to become dense vastly increases as variable space increases dimensionality. *Rapidity* is then defined as how quickly a tour encompasses the Grassmannian. Due to the random selection of a grand tour, it will end up visiting homomorphisms of previous 2-frames before all unique values are visited, leading sub-optimal rapidity.

The little tour introduced in @mcdonald_interactive_1982, on the other hand, is necessarily both dense and rapid, performing essentially an exhaustive search on the Grassmannian. However, this path uninteresting and with long periods of similar projections strung together. The Grassmannian is necessarily large and increases exponentially at the rate of $p$. Viewing of the whole Grassmannian is time-consuming, and interesting projections are sparse, there was a clear space for computers to narrow the search space.

Guided tours [@hurley_analyzing_1990] optimize an objective function generating path will be a relatively small subset of the Grassmannian. As they are not used for exploration, density and rapidity are poor measures. On the other hand, they excel at finding interesting projections quickly. Recently, @laa_using_2019, compared projection pursuit indices with the metrics: *smoothness, squintability, flexibility, rotation invariance* and *speed*.

### Tour software

Tours have yet to be widely adopted, due in part, to the fact that print and static .pdf output does not accommodate dynamic viewing. Conceptual abstraction and technically density have also hampered user growth. Due to low levels of adoption and the rapid advancement of technology support and maintenance of such implementations give them a particularly short life span. Despite the small user base, there have been a fair number of tour implementations, including:
<!-- See Wickham’s thesis and C2 paper for partial lists. 

* spinifex [github.com/nspyrison/spinifex](https://github.com/nspyrison/spinifex) -- R package, all platforms.
* tourr [@wickham_tourr_2011] -- R package, all platforms.
* CyrstalVision [@wegman_visual_2003] -- for Windows.
* GGobi [@swayne_ggobi:_2003] -- for Linux and Windows.
* DAVIS [@huh_davis:_2002] -- Java based, with GUI.
* ORCA [@sutherland_orca:_2000] -- Extensible toolkit built in Java.
* VRGobi [@nelson_xgobi_1998] -- for use with the C2, tours in stereoscopic 3D displays.
* ExplorN [@carr_explorn:_1996] -- for SGI Unix.
* ExploRe [@hardle_xplore:_1995]
* XGobi [@swayne_xgobi:_1991] -- for Linux, Unix, and Windows (via emulation).
* XLispStat [@tierney_lisp-stat:_1990] -- for Unix and Windows.
* Explor4 [@carr_explor4:_1988] -- Four-dimensional data using stereo-ray glyphs.
* Prim-9 [@asimov_grand_1985;@fisherkeller_prim-9:_1974] -- on an internal operating system.
-->


<!--chapter:end:90-appA.Rmd-->

