---
chapter: 4
knit: "bookdown::render_book"
---

# A Study on the Benefit of a User-Controlled Radial Tour for Variable Importance for Structure in High-Dimensional Data {#ch-userstudy}

<!-- Segue -->
The previous chapter introduced the package __spinifex__ which gave us the means to perform radial tours. There is no evidence to support that the user-controlled steering of the radial tour leads to better perception than traditional methods. Therefore, this chapter discusses the user study to elucidate the efficacy of the radial tour.

<!-- Abstract -->
In Chapters \@ref(ch-introduction) and \@ref(ch-background) have introduced PCA, the grand tour, and radial tour. This chapter describes a within-participants user study evaluating efficacy of these techniques. A supervised classification task is devised where participants evaluate variable attribution of the separation between two classes. An accuracy measure is defined to use as the response variable. Data were collected from 108 crowdsourced participants, who performed two trials of each visual for 648 trials in total.

<!-- thesis_ns introduction -->
The user influence over a basis, uniquely available in the radial tour, is crucial to testing variable sensitivity to the structure visible in projection. If the contribution of a variable is reduced and the feature disappears, then it is said that the variable is sensitive to that structure. For example, Figure \@ref(fig:figClSep) shows two frames of simulated data. Panel (a) has identified separation between the two clusters. The contributions in panel (b) show no such cluster separation. The former has a large contribution to V2 in the direction of separation, while it is negligible in the right frame. Because of this it is said that V2 is sensitive to the separation of the clusters.

```{r figClSep, echo = F, out.width = "100%", fig.cap = "Illustration of cluster separation. Panel (a) shows clear separation in V2 and no separation in the direction of V3. While V1 and v4 have relatively small contributions to the frame. Panel (b) has a random basis with a minimal contribution from V2, and no separation between the cluster means is resolved."}
knitr::include_graphics("./figures/ch4_fig1_cl_sep.pdf")
```

<!--- black-box models-->
Knowing which variables to use is also important for statistical modeling and their interpretations. Models are becoming increasingly complex, and the nonlinear interactions of the terms cause an opaqueness to model interpretability. Exploratory Artificial Intelligence [XAI, @adadi_peeking_2018; @arrieta_explainable_2020] is an emerging field that extends the interpretability of such black-box models. Multivariate data visualization is essential for exploring features spaces and communicating interpretations of models [@biecek_dalex_2018; @biecek_explanatory_2021; @wickham_visualizing_2015].

<!-- Structure of the paper -->
The chapter is structured as follows. Section \@ref(sec:userstudy) describes the experimental factors, task, and accuracy measure used. The results of the study are discussed in Section \@ref(sec:results). Conclusions and potential future directions are discussed in Section \@ref(sec:conclusion). The software used for the study is described in Section \@ref(sec:spinifex).


## User study {#sec:userstudy}

<!-- Overview of visual -->
An experiment was constructed to assess the performance of the radial tour relative to the grand tour and PCA for interpreting the variable attribution contributing to separation between two clusters. <!-- Introduce experimental factors --> Data were simulated across three experimental factors: cluster shape, location of the cluster separation, and data dimensionality. Participant responses were collected using a web application and crowdsourced through prolific.co, [@palan_prolific_2018] an alternative to MTurk.


### Objective {#sec:objective}

<!-- Rational for factor levels -->
PCA will be used as a baseline for comparison as it is the most commonly used linear embedding. The grand tour will act as a secondary control that will help evaluate the benefit of animation without influencing its path. Lastly, the radial tour should perform best as it benefits from animation and user control.

<!-- Prior expectations -->
Then for some subset of tasks, we expect to find that the radial tour performs most accurately. In the appendix, section \@ref(sec:appendix), regresses on the log response time. Due to the absence of inputs, the grand tour may lead to faster response time than the alternatives since users can focus all of their attention on interpreting the fixed path. Conversely, we are less sure about the accuracy of such limited grand tours as there is no objective function in selecting the bases; it is possible that the random selection of the target bases altogether avoids bases showing cluster separation. However, given that the data dimensionality was modest, it seems plausible that the grand tour coincidentally regularly crossed frames with the correct information for the task.

<!-- Explicit hypothesis tests -->
An experimental factors and definition of an accuracy measure are given below. The null hypothesis can be stated as:

$~~~~H_0: \text{accuracy does not change across visualization method} \\$
$~~~~~H_\alpha: \text{accuracy does change across visualization method} \\$


### Experimental factors {#sec:expfactors}

<!-- Introduction to experimental factors -->
In addition to visual factor, data are simulated across three aspects. First, the _location_ of the separation between clusters by mixing a signal and a noise variable at different ratios. Secondly, the _shape_ of the clusters reflects varying distributions of the data. And third, the _dimension_-ality of the data. The levels within each factors are described below and Figure \@ref(fig:figExpFactors) gives a visual representation.

<!-- Illustration of experimental factors -->
```{r figExpFactors, out.width='100%', fig.cap = "Illustration of the experimental factors, the parameter space of the independent variables, the support of our study."}
nsSafeIncGraphic("./figures/ch4_fig3_exp_factors.pdf")
```

<!-- Location mixing -->
The _location_ of the separation of the clusters is at the heart of the measure. It would be good to test a few varying levels. To test the sensitivity, noise and signal-containing variables are mixed. The separation between clusters are mixed at the following percentages: 0/100% (not mixed), 33/66%, 50/50% (evenly mixed).

<!-- Shape, vc matrix -->
In selecting the _shape_ of the clusters, the convention given by @scrucca_mclust_2016 is followed. They describe 14 variants of model families containing three clusters. The name of the model family is the abbreviation of its respective volume, shape, and orientation of the clusters, the levels of which are either _E_qual or _V_ary. The models EEE, EEV, and EVV are used. For Instance, in the EEV model, the volume and shape of clusters are constant, while the shape's orientation varies. The EVV model is further modified by moving four-fifths of the data out in a "V" or banana-like shape.

<!-- Dimensionality -->
_Dimension_-ality is tested at two modest levels, namely, in four dimensions containing three clusters and six dimensions with four clusters. Such modest dimensionality is required to bound the difficulty and search space to keep the task realistic for crowdsourcing.


### Task and evaluation {#sec:task}

<!-- segue to task and evaluation -->
With our hypothesis formulated and data at hand, let us turn our attention to the task and how to evaluate it. Regardless of the visual method, the elements of the display are held constant, shown as a 2D scatterplot with an axis biplot to its left. Observations were supervised with the cluster level mapped to color and shape.

<!-- Geom, clusters, explicit task -->
Participants were asked to 'check any/all variables that contribute more than average to the cluster separation green circles and orange triangles,' which was further explained in the explanatory video as 'mark any and all variable that carries more than their fair share of the weight, or one quarter in the case of four variables'.

<!-- Instruction and video -->
The instructions iterated several times in the video was: 1) use the input controls to find a frame that contains separation between the clusters of green circles and orange triangles, 2) look at the orientation of the variable contributions in the gray circle (biplot axes orientation), and 3) select all variables that contribute more than uniformed distributed cluster separation in the scatterplot. Independent with experimental level, participants were limited to 60 seconds for each evaluation of this task. This restriction did not impact many participants as the 25th, 50th, 75th quantiles of the response time were about 7, 21, and 30 seconds respectively.

<!-- Evaluating measure -->
The evaluation measure of this task was designed with a few features in mind: 1) the sum of squares of the individual variable weights should be one, 2) symmetric about zero, that is, without preference to under- or over-guessing 3) heavier than linear weight with increasing distance from a uniform height. The following measure is defined for evaluating the task with these in mind.

Let a data $\textbf{X}_{n,~p,~k}$ be a simulation containing clusters of observations of different distributions. Where $n$ is the number of observations, $p$ is the number of variables, and $k$ indicates the cluster an observation belongs. Cluster membership is exclusive; an observation cannot belong to more than one cluster.

<!-- W, weights -->
The weights, $w$ as a vector explaining the variable-wise difference between two clusters. Namely, the difference of each variable between clusters, as a proportion of the total difference, less $1/p$, the expected cluster separation if it were uniformly distributed. <!-- R, participant responses -->Participant responses are a logical value for each variable - whether or not the participant thinks each variable separates the two clusters more than uniformly distributed separation.

<!-- __v1 measure sqrt__ -->
<!-- \begin{align*} -->
<!-- W_{j} &=\frac -->
<!-- {(\overline{X_{j=1, k=1}} - \overline{X_{1, 2}}, ~...~ -->
<!-- (\overline{X_{p, 1}} - \overline{X_{p, 2}})} -->
<!-- {\sum_{j=1}^{p}(|\overline{X_{j, k=1}} - \overline{X_{j, k=2}}|)} -->
<!-- - \frac{1}{p} \\ -->
<!-- \\ -->
<!-- \text{Accuracy}, Y &= \sum_{j=1}^{p}I(r_j) \cdot sign(w_j) \cdot \sqrt{|w_j|} \\ -->
<!-- \end{align*} -->

<!-- __v2 measure 01 scaled__ -->
<!--\left\{ \begin{aligned}
    \frac{w_+^2}{\sum w_+^2}  &&\Bigg\vert w_+ \text{ the positive elements of } w \\
    \frac{w_-^2}{\sum w_-^2}  &&\Bigg\vert w_- \text{ the negative elements of } w \\ \end{aligned} \right. -->

<!-- __v2 measure sq__ -->
\begin{align*}
  w_{j} &=\frac{(\overline{X}_{\cdot, j=1, k=1} - \overline{X}_{\cdot, 1, 2}, ~...~ 
    (\overline{X}_{\cdot, p, 1} - \overline{X}_{\cdot, p, 2})}
    {\sum_{j=1}^{p}(|\overline{X}_{\cdot, j, k=1} - \overline{X}_{\cdot, j, 2}|)} - \frac{1}{p} \shortintertext{Where accuracy, A, is defined as:}
  A &= \sum_{j=1}^{p}I(j) \cdot sign(w_j) \cdot w^2
\end{align*}

Where $I(j)$ is the indicator function, the binary response for variable $j$. Figure \@ref(fig:figBiplotScoring) shows one frame of a simulation with its observed variable separation (wide bars), expected uniform separation (dashed line), and accuracy if selected (thin lines).

```{r, figBiplotScoring, out.width="100%", fig.cap = "A pair of principal components of simulated data and the cluster separtion contained in the variables. (L), Scatterplot and biplot of PC1 by PC4 of a simulated data set (R) illustration of cluster separation between the means of the green circles and orange triangles. Bars indicate observed cluster separation and (red/green) lines show the accuracy weight of the variable if selected. The horizontal dashed line is $1 / p$, the expected value. The weights equal the signed square of the difference between each variable value and the dashed line."}
nsSafeIncGraphic("./figures/ch4_fig4_accuracy_measure.pdf")
```


### Visual design standardization {#sec:standardization}

<!-- Background for methodology, application here -->
The factors are tested within-participant, with each visual being evaluated twice by each participant. The order that experimental factors are experienced is controlled with the assignment, as illustrated in Figure \@ref(fig:figParmeterizationExample). Below discussed the visual design standardization and the input and display within each factor.

<!-- Aesthetic standardization -->
The visualization methods were standardized wherever possible. Data were displayed as 2D scatterplots with biplots [@gabriel_biplot_1971], a visual with variable contributions inscribed on a unit circle. All aesthetic values (colors, shapes, sizes, absence of legend, and axis titles) were constant. Variable contributions were always shown left of the scatterplot embeddings with their aesthetic values consistent. What did vary between factors were their inputs.

<!-- PCA -->
PCA inputs allowed users to select between the top four principal components for both axes regardless of the data dimensionality (four or six). Data were simulated to have cluster separation within the 2nd to 4th components. Cluster separation was sampled to not bury signal in 5th and 6th components (not selectable in PCA input) in the interest of simplicity and time. <!-- Grand tours -->There was no user input for the grand tour; users were instead shown a 15-second animation of the same randomly selected path. Participants could view the same clip up to four times within the time limit. <!-- Radial tours -->Radial tours were also displayed at five frames per second with a step size of 0.1 radians between interpolated frames. Users were able to swap between variables. Selecting a new variable resets the animation where the new variable is manipulated to a full, zero, and then back to its initial contribution. The complete animation of any variable takes about 20 seconds and is almost entirely in the projection frame at around six seconds. The starting basis was initialized to a half-clock design, where the variables were evenly distributed in half of the circle. This design was created to be variable agnostic while maximizing the independence of the variables.


### Data simulation

<!-- Clusters and correlation -->
Each dimension is originally distributed as $\mathcal{N}(0, 1)$, given the covariance set by the shape factor. Clusters were originally separated by a distance of two before location mixing. Signal variables had a correlation of 0.9 when they had equal orientation and -0.9 when their orientations varies. Noise variables were restricted to zero correlation. Each cluster is simulated with 140 observations and is offset in a variable that did not distinguish previous variables.
 
<!-- Apply shape and location transformations -->
Clusters of the EVV shape are transformed to the banana-chevron shape (illustrated in figure \@ref(fig:figExpFactors), shape row). Then location mixing is applied by post-multiplying a (2x2) rotation matrix to the signal variable and a noise variable for the clusters in question.<!-- Preprocess and replicate and save --> All variables are then standardized by standard deviation. The rows and columns are then shuffled randomly. The observation's cluster and order of shuffling are attached to the data and saved.

<!-- Iterating over factor -->
Each of these replications is then iterated with each level of the factor. For PCA, projections were saved for each of the 12 pairs of the top four principal components. A basis path is saved for a 4- and 6D grand tour. The data from each simulation is then projected through its corresponding bases path. Each simulation's variable order was previously shuffled, effectually randomizing cluster separation shown. The resulting animations were saved as `gif` files. The radial tour starts at either the four or six variable "half-clock" basis, where each variable has a uniform contribution in the right half with no variable contributing in the opposite direction. This acts to minimize the dependence between variable contributions. A radial tour is then produced for each variable and saved as a `gif`.


### Randomized factor assignment

<!-- Introduction -->
Now, with simulation and their artifacts in hand, we explain how the experimental factors are assigned and illustrate how this is experienced from a participant's perspective.

<!-- Periods, exp factor assignment -->
The study is sectioned into three periods. Each period is linked to a randomized level of factor visualization and the location. The order of dimension and shape are of secondary interest and are held constant in increasing order of difficulty; four then six dimensions and EEE, EEV, then EVV-banana, respectively.

<!-- Training and evaluation -->
Each period starts with an untimed training task at the simplest remaining experimental levels; location = 0/100%, shape = EEE, and four dimensions with three clusters. This serves to introduce and familiarize participants with input and visual differences. After the training, the participant performs on two trials with the same factor and location level across the increasing difficulty of dimension and shape. The plot was removed after 60 seconds, though participants rarely reached this limit.

<!-- Factor*location nested latin square -->
The order of the factor and location levels is randomized with a nested Latin square where all levels of the visual factor are exhausted before advancing to the next level of location. That requires $3!^2 = 36$ participants to evaluate all permutations of the experimental factors once. This randomization controls for potential learning effects the participant may receive. Figure \@ref(fig:figParmeterizationExample) illustrates how an arbitrary participant experiences the experimental factors.

<!-- Nested latin square assignment -->
```{r figParmeterizationExample, out.width="100%", fig.cap = "Illustration of how a hypothetical participant 63 is assigned experimental factors. Each of the six factor order permutations is exhausted before iterating to the next permutation of location order."}
## This is a manual .pttx screen cap, .png ok.
nsSafeIncGraphic("./figures/ch4_fig5_randomization_MANUAL.PNG")
```

<!-- Pilot study; 3 even evaluations of each -->
Through pilot studies sampled by convenience (information technology and statistics Ph.D. students attending Monash University), it is estimated that three full evaluations are needed to properly power the study, for a total of $N = 3 \cdot 3!^2 = 108$ participants.


### Participants {#sec:articipants}

$N = 108$ participants were recruited via prolific.co [@palan_prolific_2018]. Participants are restricted based on their claimed education requiring that they have completed at least an undergraduate degree (some 58,700 of the 150,400 users at the time). This restriction is used on the premise that linear projections and biplot displays will not be regularly used for consumption by general audiences. There is also the implicit filter that Prolific participants must be at least 18 years of age and implicit biases of timezone, location, and language. Participants were compensated for their time at \pounds 7.50 per hour, whereas the mean duration of the survey was about 16 minutes. Previous knowledge or familiarity was minimal as validated in the follow-up survey. The appendix contains a heatmap distribution of age and education paneled across preferred pronouns of the participants that completed the survey, who are relatively young, well educated, and slightly more likely to identify as males.


### Data collection

<!-- App, data collection, network issues -->
Data were recorded in __shiny__ application and written to a Google Sheet after each third of the study. Especially at the start of the study, participants experienced adverse network conditions due to the volume of participants hitting the application with modest allocated resources. In addition to this, API read/write limitations further hindered data collection. To mitigate this, the number of participants were throttled and over-collect survey trials until three evaluations are collected for all permutation levels.

<!-- Preprocessing steps -->
The processing steps were minimal. The data were formatted and then filtered to only the latest three complete studies of each experimental factor, which should have experienced the least adverse network conditions. The bulk of the studies removed were partial data and a few over-sampled permutations. This brings us to the 108 studies described in the paper, from which models and aggregation tables were built. The post-study surveys were similarly decoded to human-readable format and then filtered to include only those 84 associated with the final 108 studies.

The code, response files, their analyses, and the study application are publicly available at; \url{https://github.com/nspyrison/spinifex_study}.


## Results {#sec:results}

To recap, the primary response variable is task accuracy, as defined in section \@ref(sec:task). The parallel analysis of the log response time is provided in the appendix. Two primary data sets were collected; the user study evaluations and the post-study survey. The former is the 108 participants with the explanatory variables: visual factor, location of the cluster separation signal, the shape of variance-covariance matrix, and the dimensionality of the data. Experimental factors and randomization were discussed in section \@ref(sec:expfactors). The survey was completed by 84 of these 108 people. It collected demographic information (preferred pronoun, age, and education), and subjective measures for each factor (preference, familiarity, ease of use, and confidence).

Below a battery of mixed regression models is built to examine the degree of the evidence and the size of the effects from the experimental factors. Then, Likert plots and rank-sum tests to compare the subjective measures between the visual factors.


### Accuracy regression

<!-- Introduce regression model, explaining accuracy, and random effect term -->
To more thoroughly examine explanatory variables a mixed models regress accuracy. All models have a random effect term on the participant and the simulation. These terms explain the error attributed to the individual participant's effect and variation due to the random sampling data.

<!-- Building a battery of models -->
In building a set of models to test, a base model with only visual factor is compared with the full linear model and several models progressively crossing an additional term. The models with three and four interacting variables are rank deficient; there is not enough varying information in the data to explain all interacting terms.

<!-- Y1 accuracy regression -->
$$
\begin{array}{ll}
\textbf{Fixed effects}           &\textbf{Full model} \\
\alpha                           &\widehat{Y} = \mu + \alpha_i + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha + \beta + \gamma + \delta &\widehat{Y} = \mu + \alpha_i + \beta_j + \gamma_k + \delta_l + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha \cdot \beta + \gamma + \delta &\widehat{Y} = \mu + \alpha_i \cdot \beta_j + \gamma_k + \delta_l + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha \cdot \beta \cdot \gamma + \delta &\widehat{Y} = \mu + \alpha_i \cdot \beta_j \cdot \gamma_k + \delta_l + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha \cdot \beta \cdot \gamma \cdot \delta &\widehat{Y} = \mu + \alpha_i \cdot \beta_j \cdot \gamma_k \cdot \delta_l + \textbf{Z} + \textbf{W} + \epsilon
\end{array}
$$
$$
\begin{array}{ll}
\text{where }
&\alpha_i \text{, fixed term for factor}~|~i\in (\text{pca, grand, radial}) \\
&\beta_j  \text{, fixed term for location}~|~j\in (\text{0/100\%, 33/66\%, 50/50\%}) \text{ \% noise/signal mixing} \\
&\gamma_k \text{, fixed term for shape}~|~k\in (\text{EEE, EEV, EVV banana}) \text{ model shapes} \\
&\delta_l \text{, fixed term for dimension}~|~l\in (\text{4 variables \& 3 cluster, 6 variables \& 4 clusters}) \\
&\mu \text{ is the intercept of the model including the mean of random effect} \\
&\textbf{Z} \sim \mathcal{N}(0,~\tau), \text{ the error of the random effect of participant} \\
&\textbf{W} \sim \mathcal{N}(0,~\upsilon), \text{ the error of the random effect of simulation} \\
&\epsilon   \sim \mathcal{N}(0,~\sigma), \text{ the remaining error in the model} \\
\end{array}
$$


<!-- Y1 model comparisons -->
```{r marksCompTbl, fig.cap = "Use the caption arg in kable(), not this."}
## Set format and fonts for pdf vs html:
if(knitr::is_html_output()){
  fmt      <- "html"
  fnt      <- 12L
  mod_comp <- readRDS("./figures/ch4_tab1_model_comp_ls_html.rds")
  ## Escape HTML formating of * multiplication
  mod_comp$modelComp_MarksByEval$`Fixed effects` <-
      gsub("[*]", "\\\\*", mod_comp$modelComp_MarksByEval$`Fixed effects`)
  mod_comp$modelComp_TimeByEval$`Fixed effects`  <-
      gsub("[*]", "\\\\*", mod_comp$modelComp_TimeByEval$`Fixed effects`)
}else{
  fmt      <- "latex"
  fnt      <- 10L
  mod_comp <- readRDS("./figures/ch4_tab1_model_comp_ls_latex.rds")
}
  esc_comp <- FALSE
  esc_coef <- TRUE

## return
kableExtra::kbl(
  x = mod_comp[[1]], format = fmt, align = c("l", rep("l", 2), rep("c", 5)),
  booktabs = TRUE, linesep = "", escape = esc_comp,
  caption = "Model performance of random effect models regressing accuracy. Each model includes a random effect term of the participant explaining the individual's influence on accuracy. Complex models perform better in terms of $R^2$ and RMSE, yet AIC and BIC penalize their large number of fixed effects in favor of the much simpler model containing only the visual factor. Conditional $R^2$ includes the random effects, while marginal does not.") %>%
  kableExtra::kable_classic(font_size = fnt)
```


<!-- Y1 coefficients of ABcd -->
```{r marksCoefTbl, fig.cap = "Use the caption arg in kable(), not this."}
mod_coef <- readRDS("./figures/ch4_tab2_model_coef_ls.rds")
## accuracy [[1]], log time [[2]]

## return
kableExtra::kbl(
  x = mod_coef[[1]], format = fmt, 
  booktabs = TRUE, linesep = "", escape = esc_coef,
  caption = "The task accuracy model coefficients for $\\widehat{Y} = \\alpha \\cdot \\beta + \\gamma + \\delta$, with factor = pca, location = 0/100\\%, shape = EEE, and dim = 4 held as baselines. Factor being radial is the fixed term with the strongest evidence supporting the hypothesis. Interacting with the location term there is evidence suggesting radial performs worse with 33/66\\% mixing.") %>%
  kableExtra::pack_rows("factor", 2, 3) %>%
  kableExtra::pack_rows("fixed effects", 4, 8) %>%
  kableExtra::pack_rows("interactions", 9, 12) %>%
  kableExtra::kable_classic(font_size = fnt)
```

<!-- Model selection and coefficients -->
Table \@ref(tab:marksCompTbl) compares the model summaries across increasing complexity. The $\alpha \cdot \beta + \gamma + \delta$ model to is selected to examine in more detail. Table \@ref(tab:marksCoefTbl) looks at the coefficients for this model.

<!-- Conditional effects of variables -->
We also want to visually examine the conditional variables in the model. Figure \@ref(fig:figMarksABcd) examines violin plots of accuracy by factor while faceting on the location (vertical) and shape (horizontal). Use of the radial visual, on average, increases the accuracy, and especially so when the location of signal mixing is not 33/66%.

<!-- Violin plots and test overlay for Y1 factors -->
```{r, figMarksABcd, out.width="100%", fig.cap = "Violin plots of terms of the model $\\widehat{Y} = \\alpha \\cdot \\beta + \\gamma + \\delta$. Overlaid with global significance from the Kruskal-Wallis test and pairwise significance from the Wilcoxon test, both are non-parametric, ranked-sum tests suitable for handling discrete data. Participants are more confident and find the radial tour easier to use than the grand tour. Participants claim low familiarity, as expect from crowdsourced participants. Radial is more preferred compared with either alternative for this task."}
nsSafeIncGraphic("./figures/ch4_fig6_ABcd_violins.pdf")
```


### Subjective measures

<!-- Introduce subjective measures from n=84 survey responses -->
The 84 evaluations of the post-study survey also collect four subjective measures for each factor. Figure \@ref(fig:figSubjectiveMeasures) shows the Likert plots, or stacked percentage bar plots, alongside violin plots with the same non-parametric, ranked sum tests previously used. Participants preferred to use radial for this task. Participants were also more confident of their answers and found radial tours easier than grand tours. All factors have reportedly low familiarity, as expected from crowdsourced participants.

```{r figSubjectiveMeasures, out.width="100%", fig.show="asis", fig.cap = "The subjective measures of the 84 responses of the post-study survey, five discrete Likert scale levels of agreement (L) Likert plots (stacked percent bar plots) with (R) violin plots of the same measures. Violin plots are overlaid with global significance from the Kruskal-Wallis test and pairwise significance from the Wilcoxon test, both are non-parametric, ranked sum tests."}
nsSafeIncGraphic("./figures/ch4_fig7_subjective_measures.pdf")
```


## Conclusion {#sec:conclusion}

<!-- Context -->
Data visualization is an integral part understanding relationships in data, and how models are fitted. However, thorough exploration of data in high dimensions becomes difficult. Previous methods offer no means for an analyst to impact the projection basis. The manual tour provides a mechanism for changing the contribution of a selected variable to the basis. Giving analysts such control should facilitate the exploration of variable-level sensitivity to the identified structure.

<!-- Recap study -->
This paper discussed a with-in participant user study ($n=108$) comparing the efficacy of three linear projection techniques. The participants performed a supervised cluster task, explicitly identifying which variables contribute to separating two target clusters. This was evaluated evenly over four experimental factors. In summary, mixed model regression finds strong evidence that using the radial tour leads to a sizable accuracy increase. In the extended analysis, there is significant evidence for a change in response time, with PCA being fastest, then the grand tour, followed by the radial tour. The effect sizes on accuracy are large relative to the change from the other experimental factors, though smaller than the random effect of the participant. The radial tour was subjectively preferred, leading to more confidence in answers, and increased ease of use than the alternatives.

<!-- Discussion and going further -->
There are several ways that this study could be extended. In addition to expanding the support of the experimental factors, more exciting directions include: changing the task, visualizations used, and experience level of the target population. It is difficult to achieve good coverage given the number of possible permutations. Keep in mind the traffic volume and low effort of responses from participants when crowdsourcing.



<!-- ## References -->

<!-- <div id="refs"></div> -->

<!-- Adds a bib section at the end of every chapter -->
