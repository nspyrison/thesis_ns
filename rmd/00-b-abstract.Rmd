---
knit: "bookdown::render_book"
---

# Abstract {-}

Visualizing data space is crucial to exploratory data analysis, checking model assumptions and validating model performance. Yet visualization quickly becomes difficult as the dimensionality of the data or features increases. Traditionally, static, low-dimensional linear embeddings are used to mitigate this complexity. Such embedded space is a lossy approximation of the full space. However, by viewing many embeddings with interactive supporting views, maximizes the information conveied. Data visualization _tours_ are a class of dynamic linear projections that animate many linear projections over small changes to the projection basis.

Tours are distinguished the path of their projection bases. _Manual tours_ allow for User-Controlled Steering (UCS) of bases path, where the contributions of individual variables can be controlled. First I create a free, open-source __R__ package, __spinifex__ that facilitates the creation of such tours. These manipulations of the variables can be precomputed or made in real-time. The animations are interoperable with the existing package __tourr__ and can be rendered with recent graphics interfaces __plotly__ and __gganimate__. The former offers interactive use with HTML widgets while the latter can render to static formats such as .gif or .mp4 files. An interactive application is inclused to illustration use of the manual tour.

Theoretically, UCS should enable an analyst to better explore explore the variable attribution to the structure identified in an embedding. To test this I conduct a within-participant, crowd-sourced user study. The primary variable of interest is the visualization method where manual tours are compared against the benchmarks of Principal Component Analysis (PCA) and an alternative tour variant, the grand tour. Each of the $n=108$ participants performs two trials with each visual for 648 total trials. The task is a supervised classification problem asking participants which variables attribution to the separation of 2 clusters. I find strong evidence to support that use of the radial tour leads to a large improvement in the accuracy of the variable attribution for this task.

Modern modeling techniques are sometimes refered to as black-box models due the un-interpretable nature of model terms which are often many and non-linear. Recent research in Explainable Artificial Intelligence (XAI) tries to bring interpretability to these models through the use of _local explanations_. Local explanations are a class of techniques that approximate the linear-variable importance at one point in the data. I purpose interactively exploring data- and explanation-spaces side-by-side to identify two observation to compare. Then the explanations can be evaluated to see how well they hold up under the varying contributions with the manual tour. I have created the free open-sourced __R__ package __cheem__ provides preprocessing defaults and in interactive application to explore.

<!-- 
The following line is required to re-set page numbering after preliminary material. Do not remove
-->
\clearpage\pagenumbering{arabic}\setcounter{page}{0}
