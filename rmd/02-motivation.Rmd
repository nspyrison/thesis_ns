---
chapter: 2
knit: "bookdown::render_book"
---

<!-- motivation or background?? -->
# Motivation {#ch-motivation}


First I motivation the early are frequent us of data visualization in general. Then we discuss some of the alternative data visualizations, and their scalability. The we discuss dimension reduction before cover tours in more detail.

<!-- note on data and context -->
For our purposes I will be focusing the case where data $X_{nxp}$ contains $n$ observations of $p$ variables, is complete with no missing values, variables are numeric (ideally not ordinal levels), and $n>p$ typically many more observations than variables. While I write as though always operating on the original variable space these methods could similarly be applied to feature decomposition of data not fitting this description. In the case of the linear projection let, $Y_{nxd} = X_{nxp} * A_{pxd}$ be the embedding of the data mapped by the basis $A$, where $d<p$. When $p$ is large, say over 10 or 20 variables, the viewing space is quite large. In these cases a PCA initialization step is commonly used where the variables are approximated as fewer principal components and this reduced space can be view, albeit with the disadvantage of having another linear mapping back to the original space. @wickham_visualizing_2015 also views model spaces and features while urging to have a preference for visualizing data-space directly. 

<!-- better than numerical summarization  -->
Visualization is much more robust than numerical summarization alone [@anscombe_graphs_1973; @matejka_same_2017]. In these studies the data have the same summary statistics, yet contains obvious visual trends and shapes that could go completely unheeded if plotting is foregone. Data visualization is fundamental to EDA and quickly evaluating data supports ensuring that models are suitable.

(ref:matejka17fig-cap) Starting with the datasarus plot, observations are allows to drift (by iterated simulated annealing) toward 12 patterns provided that they stay close to the originally listed statistics [@matejka_same_2017].

```{r matejka17fig, echo=F, out.width='70%', fig.cap = "(ref:matejka17fig-cap)"}
knitr::include_graphics("./figures/matejka17fig.png")
```

## Visualizing multivariate data.

<!-- penguins data -->
The work in @grinstein_high-dimensional_2002 gives a good taxonomy of high-dimensional visualization. We will follow a few examples building up to why we conclude with tour methods. Broadly speaking, we are concerned with the question "How can an analyst visualize arbitrary $p-$ dimensions?". To illustrate some of the options for data I use the penguins data[@gorman_ecological_2014, horst_palmerpenguins_2020]. It contains 333 observations of 4 physical measurements for 3 species of penguins observed near Palm Station, Antarctica.

<!-- SPLOM -->
Viewing as many univariate histograms or density curves is one method. Similarly, one could look at all variable pairing as scatter plots. This forms the crux of scatterplot matrices, also known as SPLOM [@chambers_graphical_1983]. In a scatterplot matrix variables are displayed across the columns and row, the diagonal elements show the univariate densities while one or both sides of the off diagonal show scatterplot pairs. This is useful for getting a handle on the support and shapes of the variables, but is not going to scale well with dimension and is not a suitable audience-ready display as it is very busy and doesn't draw attention to any one spot. Munzner reminds us to abstract all of the cognitive work out of the visual allowing the audience to focus on seeing the evidence supporting the claim [@munzner_visualization_2014].

(ref:penguinsplom-cap) Scatterplot matrix of penguins data. This is a good initial step, but will not scale well at $p$ increases.

```{r penguinsplom, echo=F, out.width='100%', fig.cap = "(ref:penguinsmplom-cap)"}
knitr::include_graphics("./figures_from_script/ch2_fig1_penguin_splom.pdf")
```

<!-- PCP & observation based visuals -->
Alternatively, we could consider a class of observation-based visuals. In parallel coordinate plots [@ocagne_coordonnees_1885] variables are arranged horizontally and observations are connected by lines with the height mapping to the quantile or z-value for each variable. This scales much better with dimensions, but poorly with observations. It also suffers from an asymmetry with the variable order, that is, changing the order of the variable will lead people to very different conclusions. The x-axis is also used to display variables rather than the values of the observations. This restricts the amount of information that can be gleaned between variables. Munzner asserts that position is the more human-perceptible channel for encoding information; we should like to reserve it for the values of the observations. The same issues persist across other observation-based displays such as radial variants, pixel-oriented visuals, and Chernoff faces [@keim_designing_2000; chernoff_use_1973]. These visuals are better suited for the $n<p$ case where there are more variables than observations.

(ref:penguinpcp-cap) Parallel coordinate plots of penguins data. Doesn't scale well with observations, suffers from asymmetry with the variable ordering, and horizontal position is used for variable rather than observation levels.

```{r penguinpcp, echo=F, out.width='100%', fig.cap = "(ref:penguinpcp-cap)"}
knitr::include_graphics("./figures_from_script/ch2_fig2_penguin_pcp.pdf")
```


## Dimension reduction

Ultimately, we will need to turn to dimension reduction to create a compelling visual allowing audiences to focus on features with contributions from multiple variables. Dimension reduction is separated into two categories, linear and non-linear. The linear case spans all affine mathematical transformations, essentially any mapping where parallel lines stay parallel. Non-linear transformations are the complement of that, think transformations with exponents or interacting terms. Examples in low dimension are relatable. For instance shadows are examples of linear projections where a 3-dimensional object casts a 2D projection, the shadow. Our vision at one instance in time is also a 2D projection-perspective. An example of a non-linear transformation is that of 2D representation of the globe. There are many different ways (and features to optimize) to distort the surface to display as a map. Most common may be rectangular display where area is distorted more and more with distance away from the equator. Other distortions are created when the surface is unwrapped as a long ellipse. Yet others create non-continuous gaps in oceans to minimize the distortion of countries.

Non-linear techniques often have hyperparameters that affect how the spaces are distorted to fit into a lower-space. To quote Anastasios Panagiotelis "All non-linear projections are wrong, but some are useful", a play on George Box quote about models. Non-linear techniques distort the space in unclear ways, and what is more, they can introduce features not in the data depending on the selection of hyperparameters. It feels like they are necessary, but not sufficient to conclude the existence of a feature, that is, if a non-linear embedding resolves a feature, we shouldn't be certain that it really exists in the data. 

Unfortunately, there is no free lunch here. An increase in data space will lead a $p-d$-dimensional viewing space in the linear case and an increasingly perturbed and distorted space in non-linear techniques. The intrinsic dimensionality of data is the number of variables needed to minimally represent the data [@grinstein_high-dimensional_2002]. This is an important aspect of dimension reduction that does not have to end in visual, but is also a common part of factor analysis and preprocessing data. Consider a Psychology survey consisting of 100 questions about the Big Five personality traits. The data consists of 100 variables, while the theory would suggest the intrinsic dimensionality is five. The data likely picks up on other aspects and may better be summarized in with 8 or 10 dimensions. If this were the case, reducing the data to this space will be necessary to gate the exponentially increasing view time.


## Tours, animated linear projections

A single linear projection is a resulting space from the data multiplied by the basis where the basis is an orthonormal matrix (the orientation of the unit origin) mapping the data space to a lower dimension. A data visualization tour animates many such projections through small changes in the basis. Originally in a _grand_ tour [@asimov_grand_1985] several target frames are randomly selected and then interpolated along their geodesic path.

(ref:buja05fig-cap) Illustration of the grand tour selecting random target frames (grey) connected via geodesically interpolated frame (white), figure from @buja_computational_2005.

```{r buja05fig, echo=F, out.width='100%', fig.cap = "(ref:buja05fig-cap)"}
knitr::include_graphics("./figures/buja05fig.PNG")
```

There are various types of tours that are classified by the generation of their basis paths. A _guided_ tour uses simulated annealing to move progressively closer to an objective function in the embedded space [@hurley_analyzing_1990]. A more comprehensive discussion and review of tours can be found in the works of @cook_grand_2008 and @lee_review_2021.

Tours are used for a couple of salient features: maintains transparency back to the original variable space and the persistence data points from frame to frame convey more information than looking at discrete jumps to other bases. Because of these features tours are a good method to extend the visualization of data space as dimensionality increases.

The work below covers manual tours [@cook_manual_1997; @spyrison_spinifex_2020] in Chapter \ref{ch-spinifex}. Radial tours are one type of manual tour that where the contribution of one variable is extended radially to a full contribution, removed completely, then restored to its original contribution. Chapter \ref{ch-efficacy_radial_tour} compares the efficacy of the radial tour as compared with PCA and the grand tour in a user study. Lastly, Chapter \ref{ch-cheem} extends the use of the radial tour to evaluate the local explanation of black-box models.


