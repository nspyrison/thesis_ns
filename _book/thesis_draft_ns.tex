% This is a LaTeX thesis template for Monash University.
% to be used with Rmarkdown
% This template was produced by Rob Hyndman
% Version: 6 September 2016

\documentclass{monashthesis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Add any LaTeX packages and other preamble here if required
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{thumbpdf,lmodern}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{booktabs}
%%%% NS
\usepackage{graphicx}
%\usepackage{pdflscape} %% Allow landscape layout, not used
%\usepackage{lmodern} %% Already loaded, allows GB Pound sign
\usepackage{animate} %% Needed for animations
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref} %% Trying to wrap and format urls correctly


\author{Nicholas S Spyrison}
\title{Dynamic visualization of high-dimensional data via low-dimension projections, their efficay, and their application to model-agnostic local explanations}
\degrees{B.Sc. Statistics, Iowa State University}
\def\degreetitle{Doctor of Philosophy}
% Add subject and keywords below
\hypersetup{
     %pdfsubject={The Subject},
     %pdfkeywords={Some Keywords},
     pdfauthor={Nicholas S Spyrison},
     pdftitle={Dynamic visualization of high-dimensional data via low-dimension projections, their efficay, and their application to model-agnostic local explanations},
     pdfproducer={Bookdown with LaTeX}
}


\bibliography{thesisrefs}

\begin{document}

\pagenumbering{roman}

\titlepage

{\setstretch{1.2}\sf\tighttoc\doublespacing}

\hypertarget{abstract}{%
\chapter*{Abstract}\label{abstract}}
\addcontentsline{toc}{chapter}{Abstract}

Visualizing data space is crucial to exploratory and general data analysis yet doing so quickly becomes difficult as the dimensionality of the data increases. Traditionally, static, low-dimensional linear embeddings are used to identify clustering, outliers, and structure. Observing one such embedding often misses a significant amount of variation, and hence, information held within the data. \emph{Tours} are a class of dynamic linear projections that animates many linear projections as the orientation in data space changes. User-controlled steering (UCS) of the original dimensionality offers fine control of the local structure of projections.

Data visualization has lagged behind in utilizing 3D and virtual spaces after the overhype of the 1980s and '90s gave way to some unpromising results. Modern mixed reality hardware has significantly improved the quality and simultaneously reduced the barrier to entry. Contemporary studies have regularly shown increased accuracy of perception of visuals displayed in 3D over 2D, including in projected subspaces. It's time to further explore dynamic projections in virtual spaces.

Multivariate data is ubiquitous and viewing it in data-space is a crucial aspect of data analysis and consumption. This research is four-fold and allows for fine exploration of the data structure in embeddings of high dimensional spaces, contrasts UCS with traditional static techniques, extends UCS \& creates surface projections in 3D space, and quantifies the benefits of dynamic projections across display devices.

\clearpage\pagenumbering{arabic}\setcounter{page}{0}

\hypertarget{ch:introduction}{%
\chapter{Introduction}\label{ch:introduction}}

\hypertarget{exploratory-data-analysis}{%
\section{Exploratory data analysis}\label{exploratory-data-analysis}}

The term exploratory data analysis was coined in \textcite{tukey_exploratory_1977}, who leaves it as an intentionally broad term wich encompass the initial summarization and visualization of a data set. This is a critical first step of checking for realistic values and validating assumptions made by prospective methodology. Visualization is crucial to a clear understanding of the data. Things can go awry when data is summarized via numeric statistics alone \autocite{anscombe_graphs_1973} as demonstrated in figure \ref{fig:matejka17fig} \autocite{matejka_same_2017}. In these studies, bivariate data have the same summary statistics (such as mean and standard deviation), yet contains obvious visual trends and shapes that could go completely unheeded if plotting is foregone. Because there are inherent dangers to relying on statistics alone, this requirement for looking at visuals necessitates \emph{human-in-the-loop} analysis, defined as any model that requires human interaction \autocite{karwowski_international_2006}.



\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{./figures/matejka17fig} 

}

\caption{12 data sets created from the datasaurus by simulated annealing. Each is restrained to the same summary statistics but given shapes with visual peculiarity to mutate into \autocite{matejka_same_2017}.}\label{fig:matejka17fig}
\end{figure}

It is clear that data-space visualization is needed but it becomes complex as data dimensionality increases. Embedding (or projecting) \(p-\)dimensional data on to a lower, \(d\)-dimensional subspace is a common dimension reduction approach to visualize multivariate data spaces. Traditionally a single static projection is used to summarize a space, which necessarily shows a subset of the variation of the data. \textcite{asimov_grand_1985} suggested the use of viewing projections dynamically across a changing projection basis allows for more variation to be contained and viewed temporally. This dynamic view of many changing projections is known as \emph{tours}. While, there are different methods of generating tour paths, human-in-the-loop user-controlled steering (UCS) offers the finest control for navigating the local structure.

Tours are typically viewed from standard 2D monitors, and most commonly viewed as a projection down to 2D. A notable exception being \textcite{nelson_xgobi_1998}, where 3D embeddings were viewed in 3D head-tracked VR. Data visualization studies generally show benefits in 3D visuals over 2D, especially when adequate depth cues are provided. The state of modern hardware has made VR more affordable and available to wider audiences, at ever increasing resolutions of display than previously possible. It is therefore timely for research to be conducted to compare the structure and speed of comprehension of dynamic linear projections across 2- and 3D display devices.

\hypertarget{research-objectives}{%
\section{Research objectives}\label{research-objectives}}

Data and models are typically high-dimensional, with many variables and parameters. Developing new methods to visualize high dimensions has been a pursuit of statisticians, computer scientists and visualization researchers for decades. As technology evolves examining, extending, and assessing current techniques, in new environments, for new data challenges, is an important endeavor. The primary objectives of this Ph.D.~research can then be summarized as the following.

\textbf{Research objectives (RO):}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{How can UCS be generalized to work within graphic-specific environments for 2D projections?}\\
  (Work in progress, chapter \ref{ch:workinprogress}.)\\
  Building from the UCS algorithm in \textcite{cook_manual_1997}, the algorithm should be modified for generalized use with graphic-specific environments. This enables fine control to explore the sensitivity of structure to the variable contributing to the projection and sets the foundation to be used in the remaining objectives.
\item
  \textbf{Does 2D UCS tours provide benefits over alternatives?}\\
  (Future work, chapter \ref{ch:future_work}.)\\
  The quality and effectiveness of 2D UCS will be compared with alternatives of static, single, linear and non-linear projection techniques. They will be quantified by the measurement of structure, variation, and clustering across on benchmark datasets.
\item
  \textbf{How can UCS be extended to 3D?}\\
  (Future work, chapter \ref{ch:future_work}.)\\
  The addition of a 3rd dimension potentially allows for the improved perception of the structure of the data in dynamic UCS. To investigate this UCS algorithm needs to be extended to a third dimension. This would also allow for novel application multi-parameter function projection. This will involve the addition of a new angle and it controls to the projection space, reference axes, and manipulation space. In particular, the manipulation space, now in 4D, will be hard to visualize, but it should be able to stand as a mathematical construct facilitated through interaction with a point (the projection coefficients of the selected manipulation variable) on the now 3D reference axes volume.
\item
  \textbf{Does UCS in 3D displays provide benefits over 2D displays?}\\
  (Future work, chapter \ref{ch:future_work}.)\\
  The addition of a 3rd dimension has previously been shown to provide benefits. The extension of UCS into 3D should be used to explore the potential benefits of UCS projections as well. Interactive, time-varying tours theoretically allow for improved understanding and comprehension speed of the structure of the data. These metrics will be measured across the display device (including a 2D standard monitor, 3D head tracked monitor, and 3D head-mounted display).
\end{enumerate}

\textbf{Contributions:}

The intended contributions and scope of this research can be summarized as:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A modified UCS algorithm and new implementation applied to contemporary high energy physics and astrophysics applications in 2D animation frameworks.
\item
  A performance comparison of static and interactive UCS projection techniques assessed on benchmark data sets from the recent literature.
\item
  A new algorithm for UCS in 3D. With new applications to function visualization in 3D.
\item
  Quantitative understanding of the relative benefits of UCS across 2- and 3D display devices.
\end{enumerate}

\hypertarget{methodology}{%
\section{Methodology}\label{methodology}}

This research is interdisciplinary; it stems from a linear dimension reduction technique developed by statisticians and extended with information technology into 3D including VR technologies, with applications in high energy physics identified\autocite{cook_dynamical_2018}. Experts in these fields correspondingly supervise the research.

The research corresponding with RO \#1 entails a work in progress \textbf{algorithm design} following the work in \textcite{cook_manual_1997}. The proposed algorithm discusses the generalized application of UCS for use across animation-specific frameworks. The outcome of this is an \emph{R} package, \texttt{spinifex}, which will be submitted to CRAN and for hosting and distribution. This forms the foundation for future work in the remaining objectives.

The second objective is addressed with a benchmark dataset \textbf{performance comparison} between dynamic linear projections and alternatives (static linear and static non-linear projections such as principal component analysis, multi-dimensional scaling, and t-distributed neighbor embeddings, described in more detail in chapter \ref{ch:future_work}). Benchmark datasets will be compared across techniques, measurements will include variation explained, transparency to the original variable space, clustering identification, and outlier identification.

The research for RO \#3 involves \textbf{algorithm design}, where the work in RO \#1 will be extended to display with the use of a third spatial dimension. This will also be used to develop visualization of projected multi-dimension function surfaces. This forms the calculation base for the work. Several difficulties may arise when bringing dynamic projection into 3D spaces, especially when exploring 3D surfaces (discussed in more detail in chapter \ref{ch:future_work}).

The research resulting from RO \#4 is a controlled \textbf{usability study} to explore the efficacy of bringing UCS into 3D as compared across various display devices, in a standardized interface allowed by the work stemming from RO \# 3. In this design, the factors are user tasks (such as separation of clusters and ranking of manipulation variable) across the treatment of display device (including 2D standard monitor, 3D head-tracked monitor, and head-mounted display). Quantitative measurements include participant speed and accuracy of tasks, biometric readings, and subjective Likert surveys of participants. A lineup-type model as outlined in \textcite{hofmann_graphical_2012} may also be employed for assessing the quality of display types.

\hypertarget{workflow-and-reproducibility}{%
\section{Workflow and reproducibility}\label{workflow-and-reproducibility}}

Figure \ref{fig:dataanalysisworkflow} depicts the general data analysis workflow \autocite{wickham_r_2016}. Where data first must be imported into a tool, the structure of the data must be tidied and ordered neatly into the correct use format. After the data enters a repeating cycle, where values may be transformed, visualized, and modeled with communication going to the appropriate recipients. The research proposed in this document aids exploratory data analysis as well as the visualization aspect of this workflow. Mature analysis workflow is also made reproducible with the use of programmatic scripts.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figures/data_analysis_workflow} 

}

\caption{Data analysis workflow \autocite{wickham_r_2016}. This research aids visualization in exploratory data analysis and workflow.}\label{fig:dataanalysisworkflow}
\end{figure}

The programing language, \emph{R}, is used in the work described below to import, tidy, and transform data. It can be used directly to visualize 2D tours (RO \#1 \& 2) or be consumed into the game engine \emph{Unity} to visualize 3D tours (RO \#3 \& 4). Doing analysis and writeup in such programmatic ways allow work to be done reproducibly. Where data, analysis, and code are stored in the same directory. Reproducible work facilitates validation, maintains transparency and minimizes the chance for human error. Reproduction of work is a key feature to validate and defend the claims and methodology held within a work. Directories of current and planned work are/will be hosted publicly on GitHub, including this report. Accessing the source files for this report is discussed in section \ref{sec:source}.

\hypertarget{ch:projectoverview}{%
\section{Project overview}\label{ch:projectoverview}}

Figure \ref{fig:ProjectOverview} depicts a schematic flow chart that the research objectives will be executed in. The research stemming from RO \#1, the application of 2D user-controlled steering (UCS), sets the foundation for which the other objectives can be researched. RO \#3, the application of 3D UCS, must precede RO \#4, as it explores the efficacy of 3D UCS across display devices. RO \#2, the comparison of 2D UCS vs alternatives, must come after RO \#1, but is of lower priority to RO \#3 \& 4, and so will be conducted last, in the event of a time crunch.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figures/ProjectOverview} 

}

\caption{Flow chart of research objective dependencies, work order, and methodology.}\label{fig:ProjectOverview}
\end{figure}

In this report, the related literature is discussed in chapter \ref{ch:lit_review}. A brief overview of the research is given in chapter \ref{ch:projectoverview}, followed by ongoing work and future work in chapters \ref{ch:workinprogress} and \ref{ch:future_work} respectively. A prospective timeline is listed in chapter \ref{ch:timeline}. Notation for dynamic projections and VR data visualization can be found in appendix \ref{ch:glossary} and an excerpt of a paper to be submitted to the R Journal can be found in appendix \ref{ch:spinifex}.

\hypertarget{ch:lit_review}{%
\chapter{Literature review}\label{ch:lit_review}}

The following chapter discusses the current research in two primary areas: dynamic linear projections (collectively known as tours) and multivariate data visualization in stereoscopic 3D. After each section, research gaps are highlighted.

\hypertarget{sec:tour}{%
\section{Dynamic linear projections of multivariate data (tours)}\label{sec:tour}}

\hypertarget{overview}{%
\subsection{Overview}\label{overview}}

The introduction established that visualizing data-space is an important aspect of exploratory data analysis and data analysis in general. Yet, there is an inherent difficulty as the dimensionality of data increases. In univariate data sets histograms or smoothed density curves are employed to visualize data. In bivariate data, \(XY\) scatterplots and contour plots (2D density) can be employed. In three dimensions, the two most common techniques are 3D scatterplot\footnote{Graphs depicting three dimensions are typically viewed on a 2D display, in print or with a standard monitor. These are 2D images of monocular 3D spaces, sometimes referred to as 2.5D data visualization, more on this in section \ref{sec:3d-terminology}.} or 2D scatterplot with the 3rd variable as an aesthetic (such as color, size, or height). These aesthetic cues convey some information but are not a sufficient replacement for axes for use with continuous variables.

As dimensionality of the data, \(p\), increases the visualization of data-space quickly becomes complex. It is common that visualizing data-space is dropped in favor of graphing model-space (for example residuals), parameter-space (in fewer dimensions), or worse yet: long tables of statistics without visuals \autocite{wickham_visualizing_2015}. To preserve the visualization of data-space, a solution that scales with the dimensionality of data is needed; this is where dimensionality reduction comes in. This work will focus on a group of dynamic linear projection techniques collectively known as \emph{tours}. The scope of the project lies within the dynamic linear projections; a broader review of dimensionality reduction techniques is discussed in the review paper by \textcite{grinstein_high-dimensional_2002}. Tours are used for a couple of salient features: the use of linear projections maintains transparency back to the original variable space (which non-linear projections lose) and the use of many projections shows more variation than a single linear projection. Employing the breadth of tours extends the dimensionality of visualizations, and with it, the intrinsic understanding of the structure and distribution of data that is more succinct or beyond the reach of summary statistics alone.

Let \(p\) be the dimensionality of the data, and \(d\) be the dimension of the projection space. Tours perform linear dimensionality reduction, orthogonally projecting \(p\)-space down to \(d(\leq p)\) dimensions. Many such projections are interpolated, each making small rotations in \(p\)-space. These frames are then viewed in order as an animation of the lower dimensional embedding changing as the original variable space is manipulated. Shadow puppets offer a useful analogy to aid in conceptualizing tours. Imagine a fixed light source facing a wall. When an object is introduced it projects a 2D shadow onto the wall. This is a physical representation of a simple projection, that from \(p=3\) down to \(d=2\). If the object rotates then the shadow correspondingly changes. Observers watching only the shadow are functionally watching a 2D tour as the 3D object is manipulated. Some orientations explain more information about the shape of the object than others while watching an animation of the shadow changing gives a more robust understanding than looking at any one frame. More complex structures generally require more time to comprehend the nature of the geometry. These features hold for tours as well.

\emph{An extended tour notation is listed in the appendix section \ref{sec:tour_notation}.}

\hypertarget{history}{%
\subsection{History}\label{history}}

The first tour was introduced was the \emph{grand tour} in \textcite{asimov_grand_1985} at the Stanford Linear Accelerator, Stanford University. Asimov suggested three types of grand tours: torus, at-random, and random-walk. The original application of tours was performed on high energy physics on the PRIM-9 system.

Before choosing projection paths randomly, an exhaustive search of \(p-\)space was suggested by \textcite{mcdonald_interactive_1982}, also at the Stanford Linear Accelerator. This was later coined \emph{little tour}.

\textcite{friedman_projection_1974} and later \textcite{huber_projection_1985} recommended projection pursuit (also referred to as PP). Projection pursuit optimizes an objective function before it removes a single component/variable and then iterates in this newly embedded subspace. Within each subspace, the projection seeks for a local extremum via a hill climbing algorithm on an objective function. This formed the basis for \emph{guided tours} suggested by \textcite{hurley_analyzing_1990}.

The grand and little tours have no input from the user aside from the starting basis. Guided tours allow for an index to be selected. The bulk of tour development since has largely been around the dynamic display, user interaction, geometric representation, and application.

\hypertarget{sec:path_generation}{%
\subsection{Path generation}\label{sec:path_generation}}

A fundamental aspect of tours is the path of rotation. There are four primary distinctions of tour path generation \autocite{buja_computational_2005}: random choice, data-driven, precomputed choice, and manual control.

\begin{itemize}
\tightlist
\item
  Random choice, \emph{grand tour}, constrained random walks \(p\)-space. Paths are constrained for changes in direction small enough to maintain continuity and aid in user comprehension

  \begin{itemize}
  \tightlist
  \item
    torus-surface \autocite{asimov_grand_1985}
  \item
    at-random \autocite{asimov_grand_1985}
  \item
    random-walk \autocite{asimov_grand_1985}
  \item
    \emph{local tour} \autocite{wickham_tourr_2011}, a sort of grand tour on a leash, such that it goes to a nearby random projection before returning to the original position and iterating to a new nearby projection.
  \end{itemize}
\item
  Data-driven, \emph{guided tour}, optimizing some objective function/index within the projection-space, called projection pursuit (PP) \autocite{hurley_analyzing_1990}, including the following indexes:

  \begin{itemize}
  \tightlist
  \item
    holes \autocite{cook_projection_1993} - moves points away from the center.
  \item
    cmass \autocite{cook_projection_1993} - moves points toward the center.
  \item
    lda \autocite{lee_projection_2005} - linear discriminant analysis, seeks a projection where 2 or more classes are most separated.
  \item
    pda \autocite{lee_projection_2010} - penalized discriminant analysis for use in highly correlated variables when classification is needed.
  \item
    convex \autocite{laa_using_2019} - the ratio of the area of convex and alpha hulls.
  \item
    skinny \autocite{laa_using_2019} - the ratio of the perimeter distance to the area of the alpha hull.
  \item
    stringy \autocite{laa_using_2019} - based on the minimum spanning tree (MST), the diameter of the MST over the length of the MST.
  \item
    dcor2D \autocite{grimm_mbgraphic:_2017,laa_using_2019} - distance correlation that finds linear and non-linear dependencies between variables.
  \item
    splines2D \autocite{grimm_mbgraphic:_2017,laa_using_2019} - measure of non-linear dependence by fitting spline models.
  \item
    other user-defined objective indices can be applied to the framework provided in the \emph{tourr} package \textcite{wickham_tourr_2011}.
  \item
    Another data-drive tour is the \emph{dependence tour}, a combination of \(n\) independent 1D tours. A vector describes the axis each variable will be displayed on. for example \(c(1, 1, 2, 2)\) is a 4- to 2D tour with the first 2 variables on the first axis, and the remaining on the second.

    \begin{itemize}
    \tightlist
    \item
      \emph{correlation tour} \autocite{buja_data_1987}, a special case of the dependence tour, analogous to canonical correlation analysis.
    \end{itemize}
  \end{itemize}
\item
  Precomputed choice, \emph{planned tour}, in which the path has already been generated or defined.

  \begin{itemize}
  \tightlist
  \item
    \emph{little tour} \autocite{mcdonald_interactive_1982}, where every permutation of variables is stepped through in order, analogous to brute-force or exhaustive search.
  \item
    a saved path of any other tour, typically an array of basis targets to interpolate between.
  \end{itemize}
\item
  Manual control, \emph{manual tour}, a constrained rotation on selected manipulation variable and magnitude \autocite{cook_manual_1997}. Typically used to explore the local area after identifying an interesting feature, perhaps via guided tour.
\end{itemize}

\hypertarget{path-evaluation}{%
\subsection{Path evaluation}\label{path-evaluation}}

Consider projection down to 2D, then each projection is called a 2-frame (each spanning a 2-plane). Mathematically, a Grassmannian is the set of all possible unoriented 2-frames in \(p\)-space, \(\textbf{Gr}(2,~p)\). \textcite{asimov_grand_1985} pointed out that the unique 2-frames of the grand tour approaches \(\textbf{Gr}(2,~p)\) as time goes to infinity. The \emph{density} of a tour is defined as the fraction of the Grassmannian explored. Ideally, an exploring tour will be dense, but the time taken to become dense vastly increases as variable space increases dimensionality. \emph{Rapidity} is then defined as how quickly a tour encompasses the Grassmannian. Due to the random selection of a grand tour, it will end up visiting homomorphisms of previous 2-frames before all unique values are visited, leading sub-optimal rapidity.

The little tour introduced in \textcite{mcdonald_interactive_1982}, on the other hand, is necessarily both dense and rapid, performing essentially an exhaustive search on the Grassmannian. However, this path uninteresting and with long periods of similar projections strung together. The Grassmannian is necessarily large and increases exponentially at the rate of \(p\). Viewing of the whole Grassmannian is time-consuming, and interesting projections are sparse, there was a clear space for computers to narrow the search space.

Guided tours \autocite{hurley_analyzing_1990} optimize an objective function generating path will be a relatively small subset of the Grassmannian. As they are not used for exploration, density and rapidity are poor measures. On the other hand, they excel at finding interesting projections quickly. Recently, \textcite{laa_using_2019}, compared projection pursuit indices with the metrics: \emph{smoothness, squintability, flexibility, rotation invariance} and \emph{speed}.

\hypertarget{sec:geom_display}{%
\subsection{Geometric display by dimensionality}\label{sec:geom_display}}

Up to this point, 2D scatterplots have been the primary display discussed, they offer a logical display for viewing embeddings of high-dimensional point clouds. However, other geometrics offer perfectly valid projections as well.

\begin{itemize}
\tightlist
\item
  1D geometrics (geoms)

  \begin{itemize}
  \tightlist
  \item
    1D densities: such as histogram, average shifted histograms \autocite{scott_averaged_1985}, and kernel density \autocite{scott_incorporating_1995}.
  \item
    image (pixel): \autocite{wegman_pixel_2001}.
  \item
    time series: where multivariate values are independently lagged to view peak and trough alignment. Use case is discussed in \autocite{cook_manual_1997}.
  \end{itemize}
\item
  2D geoms

  \begin{itemize}
  \tightlist
  \item
    2D density (available on GitHub at \url{https://github.com/nspyrison/tourr})
  \item
    \(XY\) scatterplot
  \end{itemize}
\item
  3D geoms

  \begin{itemize}
  \tightlist
  \item
    Anaglyphs, sometimes called stereo, where red images are positioned for the left channel and cyan for the right when viewed with corresponding filter glasses give a perception of depth to the image.
  \item
    Depth, which gives depth cues via aesthetic mappings, most common size and/or color of data points.
  \end{itemize}
\item
  \(d\)-dimensional geoms

  \begin{itemize}
  \tightlist
  \item
    Andrews curves \autocite{andrews_plots_1972}, smoothed variant of parallel coordinate plots, discussed below.
  \item
    Chernoff faces \autocite{chernoff_use_1973}, variables linked to the size of facial features. The idea is to apply the human facial-perception for rapid, cursory, like-ness comparisons between observations.
  \item
    Parallel coordinate plots {[}\textcite{ocagne_coordonnees_1885}; wegman\_hyperdimensional\_1990{]}, where any number of variables are plotted in parallel with observations linked to their corresponding variable value by polylines.
  \item
    Scatterplot matrix (SPLOM) \autocite{chambers_graphical_1983}, showing a triangle matrix of bivariate scatterplots with 1D density on the diagonal.
  \item
    Radial glyphs, radial variants of parallel coordinates including radar, spider \autocite{duffin_spiders:_1994}, and star glyphs \autocite{siegel_surgical_1972}.
  \end{itemize}
\end{itemize}

\hypertarget{tour-software}{%
\subsection{Tour software}\label{tour-software}}

Tours have yet to be widely adopted, due in part, to the fact that print and static .pdf output does not accommodate dynamic viewing. Conceptual abstraction and technically density have also hampered user growth. Due to low levels of adoption and the rapid advancement of technology support and maintenance of such implementations give them a particularly short life span. Despite the small user base, there have been a fair number of tour implementations, including:

\begin{itemize}
\tightlist
\item
  spinifex \href{https://github.com/nspyrison/spinifex}{github.com/nspyrison/spinifex} -- R package, all platforms.
\item
  tourr \autocite{wickham_tourr_2011} -- R package, all platforms.
\item
  CyrstalVision \autocite{wegman_visual_2003} -- for Windows.
\item
  GGobi \autocite{swayne_ggobi:_2003} -- for Linux and Windows.
\item
  DAVIS \autocite{huh_davis:_2002} -- Java based, with GUI.
\item
  ORCA \autocite{sutherland_orca:_2000} -- Extensible toolkit built in Java.
\item
  VRGobi \autocite{nelson_xgobi_1998} -- for use with the C2, tours in stereoscopic 3D displays.
\item
  ExplorN \autocite{carr_explorn:_1996} -- for SGI Unix.
\item
  ExploRe \autocite{hardle_xplore:_1995}
\item
  XGobi \autocite{swayne_xgobi:_1991} -- for Linux, Unix, and Windows (via emulation).
\item
  XLispStat \autocite{tierney_lisp-stat:_1990} -- for Unix and Windows.
\item
  Explor4 \autocite{carr_explor4:_1988} -- Four-dimensional data using stereo-ray glyphs.
\item
  Prim-9 \autocite{asimov_grand_1985,fisherkeller_prim-9:_1974} -- on an internal operating system.
\end{itemize}

\hypertarget{research-gaps}{%
\subsection{Research gaps}\label{research-gaps}}

Dynamic projections offering UCS have not incorporated recent animation frameworks (\textbf{RO \#1}). This leaves the class of dynamic linear projections without the most precise, fine-scale control of rotating \(p\)-space. This should be implemented with an eye on extensibility and maintainability.

A comparative study outlining the benefits of UCS vs alternatives is also absent from the literature (\textbf{RO \#2}). The benefits of dynamic linear projections hold in theory, but a direct comparison with popular alternatives should be made. Barriers to adoption and sharing should also be kept in mind as the dynamic display is not easy to display on print and in .pdf documents.

\hypertarget{sec:3d}{%
\section{Multivariate data visualization in 3D}\label{sec:3d}}

The scope of this research pertains to numeric multivariate data, a wider overview of 3D data visualization is discussed in chapter 2 of \textcite{marriott_immersive_2018}. Terminology for 3D visuals is in the glossary section \ref{sec:3d-terminology}.

\hypertarget{a-rocky-start}{%
\subsection{A rocky start}\label{a-rocky-start}}

Scientific visualization has readily adopted mixed realities as a large amount of the science exists in three spatial dimensions, lending itself well to virtual immersion \autocite{marriott_immersive_2018}. Data visualization, on the other hand, has been slow to utilize graphics above 2.5D, (and haptic interaction) primarily due to the mixed results of over-hyped of 3D visuals from the 1980s and '90s \autocite{munzner_visualization_2014}. Since then, however, there have been several promising studies suggesting that it is time for data visualization to revisit and adopt the use of 3D visuals for specific combinations of displays and depth cues.

\hypertarget{d-rotated-projections-vs-3-2d-orthogonal-projections}{%
\subsection{3D rotated projections vs 3 2D orthogonal projections}\label{d-rotated-projections-vs-3-2d-orthogonal-projections}}

Three orthogonal 2D views can represent the three face-on views of 3D shapes. When 3D representations are used with binocular cues, they are found to have more accurate perception than 2D counterparts \autocite{lee_effects_1986}.

Between 3D and split view 2D of three-dimensional economics, data \textcite{wickens_implications_1994} asked participants questions integrating several variables, finding that 3D visuals resulted in faster answers when questions involved three dimensions, while the speed was similar when questions involved fewer dimensions.

Using 3D rotated projections gives more precise (relative to 2D) estimates of the height a ball is suspended above complex box shapes, while combinations of 2D and 3D give the most precise orientation and positioning information \autocite[depicted in figure \ref{fig:tory06fig}]{tory_visualization_2006}.



\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{./figures/tory06fig} 

}

\caption{Screen capture from \textcite{tory_visualization_2006}: ``fig.~1 (a) 2D, (b) 3D Rotated, (c) 3D Shadow, (d) Orientation Icon, and (e) ExoVis displays used in Experiment 1 (position estimation). Participants estimated the height of the ball relative to the block shape. In this example, the ball is at height 1.5 diameters above the block shape.''}\label{fig:tory06fig}
\end{figure}

\textcite{sedlmair_empirical_2013}, depicted in figure \ref{fig:sedlmair13fig}, asked users about cluster separation across 2D scatterplots, 2D scatterplot matrices (SPLOMs) and interactive 3D scatterplots, PCA (linear projection), and t-SNE (non-linear projection) as viewed in monocular 3D from a standard monitor. They conclude that interactive 3D scatterplots perform worse for class separation. This result is surprising as the extra dimension theoretically allows for the clustering structure to be seen and explored more clearly.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figures/sedlmair13fig} 

}

\caption{Screen capture of ``figure 5. example of a mesh display'' from \textcite{sedlmair_empirical_2013}: ``fig.~5. (a)-(d): Screenshots of the entangled dataset \texttt{entangled1-3d-3cl-separate} designed to show the most possible benefits for i3D. (a),(b) two viewpoints of the same i3D PCA scatterplot. An accompanying video shows the full 3D rotation. (c) 2D PCA projection. (d) t-SNE untangles this class structure in 2D. (e)-(f): 2D scatterplots of the reduced \texttt{entangled2-15d-adjacent} dataset which we designed to have a ground truth entangled class structure in 15D. (e) Glimmer MDS cannot untangle the classes, neither can PCA and robPCA (see supplemental material). (f) t-SNE nicely untangles and separates the ground truth classes in 2D.''}\label{fig:sedlmair13fig}
\end{figure}

\hypertarget{comparing-3d-and-2d-embeddings-of-multivariate-data}{%
\subsection{Comparing 3D and 2D embeddings of multivariate data}\label{comparing-3d-and-2d-embeddings-of-multivariate-data}}

\textcite{nelson_xgobi_1998}, depicted in figure \ref{fig:nelson98fig}, had \(n=15\) participants perform brushing and identification tasks (of clusters, structure, and data dimensionality) in 3D with head-tracked binocular VR. 3D proved to have a substantial advantage for cluster identification and some advantage in identifying the shape. Brushing did take longer in VR, perhaps due to the lower familiarity of manipulating 3D spaces.



\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{./figures/nelson98fig} 

}

\caption{Screen capture from \textcite{nelson_xgobi_1998}: ``figure 4: This is a picture of a 3-D room, running VRGobi. Data is plotted in the center, with painting tools to the right and variable spheres to the left. In the viewing box, the data can be seen to contain three clusters, and one is being brushed.''}\label{fig:nelson98fig}
\end{figure}

Another study, \textcite{gracia_new_2016} performed dimensionality reduction down to 2- and 3D scatterplots, both displayed in monocular 3D on a standard monitor. Users were found to more accurately compare distances between points and identify outliers on 3D scatterplots. However, both tasks were performed slower with the use of the 3D scatterplots and statistical significance was not reported.

\textcite{wagner_filho_immersive_2018}, depicted in figure \ref{fig:wagner18fig}, performed an \(n=30\) empirical study on PCA embedded projections, measuring perception error across 4 tasks and 3 display types: 2D, static 3D, and immersive 3D. Overall task error was less in static and immersive 3D relative to 2D. According to the user Likert-scale survey, 2D is slightly easier to navigate and slightly more comfortable, while, static and immersive 3D displays are slightly easier to interact with and moderately easier to find information on.



\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{./figures/wagner18fig} 

}

\caption{Screen capture from \textcite{wagner_filho_immersive_2018}, original captions contained in the capture.}\label{fig:wagner18fig}
\end{figure}

\hypertarget{immersive-analytics-platform-in-vr}{%
\subsection{Immersive analytics platform in VR}\label{immersive-analytics-platform-in-vr}}

Immersive analytics is an emerging field, where data visualization and analysis is facilitated in an intuitive, immersive virtual reality environment \autocite{chandler_immersive_2015,cordeil_immersive_2017}. An example of which is shown in \textcite{cordeil_imaxes:_2017} introduces a collaborative space for immersive data analysis. Where axes are displayed and intuitively interacted with while responding to proximity to other variable axes and popping into place changing the resulting geometric display. For example, three variables can be placed as the \(x,~y,~z-\) axes for a 3D scatterplot or stood up right next to each other for a parallel coordinate plot. The subsequent work in \textcite{cordeil_iatk:_2019} builds from the previous reference and refines it for the next iteration in interactive, scalable data visualization in virtual spaces.



\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{./figures/cordeil2017fig} 

}

\caption{Screen capture of figure 15 from \textcite{cordeil_imaxes:_2017}.}\label{fig:cordeil2017fig}
\end{figure}

\hypertarget{research-gaps-1}{%
\subsection{Research gaps}\label{research-gaps-1}}

When comparing between 2- and 3D orthogonal views, in general, studies show that perception accuracy is better in 3D, though manipulation speed is generally slower. The speed discrepancy is confounded by the difference in users familiar with manipulating 2D vs 3D spaces \autocites{lee_effects_1986,wickens_implications_1994,tory_visualization_2006}[counterexample][]{sedlmair_empirical_2013}.

Similar results have been shown in static, 3D projected spaces \autocite{gracia_new_2016,wagner_filho_immersive_2018} and in dynamic 2D embedded spaces depicted in immersive 3D \autocite{nelson_xgobi_1998}. Modern VR hardware brings about a steady improvements to quality, resolution while driving down cost, making VR more easily accessible than ever. It's timely to review dynamic 3D projections and do so in immersive spaces to quantify the corresponding benefits (\textbf{RO \#3 \& 4}).

\hypertarget{ch:spinifex}{%
\chapter{\texorpdfstring{Manual tours and the R package \texttt{spinifex}}{Manual tours and the R package spinifex}}\label{ch:spinifex}}

\begin{figure}

{\centering \includegraphics[width=1\linewidth,]{./figures_from_script/ch3_fig1_biplot} 

}

\caption{<a figure caption> XXX: TODO}\label{fig:ch3fig1}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth,]{./figures_from_script/ch3_fig2_manip_sp} 

}

\caption{<a figure caption> XXX: TODO}\label{fig:ch3fig2}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth,]{./figures_from_script/ch3_fig3_filmstrip} 

}

\caption{<a figure caption> XXX: TODO}\label{fig:ch3fig3}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth,]{./figures_from_script/ch3_fig4_jet_better_pc4} 

}

\caption{<a figure caption> XXX: TODO}\label{fig:ch3fig4}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth,]{./figures_from_script/ch3_fig5_jet_worse_pc3} 

}

\caption{<a figure caption> XXX: TODO}\label{fig:ch3fig5}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth,]{./figures_from_script/ch3_fig6_DIS_better_pc6} 

}

\caption{<a figure caption> XXX: TODO}\label{fig:ch3fig6}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth,]{./figures_from_script/ch3_fig7_DIS_worse_pc2} 

}

\caption{<a figure caption> XXX: TODO}\label{fig:ch3fig7}
\end{figure}

\hypertarget{ch:efficacy_radial_tour}{%
\chapter{Efficacy of the radial tour}\label{ch:efficacy_radial_tour}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Multivariate data underlies most classification problems. Yet exploratory data analysis \autocite[EDA,][]{tukey_exploratory_1977} of such spaces is difficult, increasingly so as dimension increases, and often leads to the consideration of models for these problems being considered to be black-boxes. There is increasing emphasis on the need to provide explainers to improve the interpretability for black-box models \autocite{biecek_dalex_2018,biecek_explanatory_2021,lundberg_unified_2017,ribeiro_why_2016,wickham_visualizing_2015}. Visualization is an important part of providing interpretations \autocite{anscombe_graphs_1973,coleman_geometric_1986,goodman_dirty_2008,matejka_same_2017}. Tour methods \autocite{lee_review_2021,cook_grand_2008} provide ways to visualize linear projections of high-dimensional spaces, to obtain an overview of shape (distributions and associations) and anomalies (outliers, clusters). The recently introduced radial tour \autocite{spyrison_spinifex_2020} provides a user-controlled manual rotation of variables into and out of a projection, which might especially be useful for studying variable importance.

Dimension reduction is commonly used in conjunction with visualization to provide informative low-dimensional summaries of high-dimensional data. There have been several user studies
for dimension reduction comparing across embeddings and display dimensionality \autocite{gracia_new_2016,wagner_filho_immersive_2018}. There are also empirical metrics and comparisons used to describe non-linear reduction and how well and faithfully they embed the data \autocite{bertini_quality_2011,liu_visualizing_2017,sedlmair_empirical_2013,van_der_maaten_visualizing_2008}. There is an absence of studies comparing techniques for assessing variable importance, particularly, how best to convey information to the viewer.

This paper describes a user study conducted to assess the benefit of the radial tour, in comparison with principal component analysis and a grand tour for understanding variable importance. The experiment is a within-participant user study. The type of visualization is the primary factor of the study, corresponding to a null hypothesis that all techniques provide a similar ability for the user to determine variable importance. The techniques are compared by having subjects complete several tasks, where accuracy and speed are recorded.

The paper is structured as follows. Section \ref{sec:background} provides the background on the visualization methods being compared. Section \ref{sec:userstudy} describes the user study, the tasks, evaluation, and measures used. The results of the study are in Section \ref{sec:results}. Conclusions and potential future directions are discussed in Section \ref{sec:conclusion}. The software used for the study is described in Section \ref{sec:spinifex}.

\hypertarget{sec:background}{%
\chapter{Background}\label{sec:background}}

\hypertarget{principal-component-analysis}{%
\section{Principal component analysis}\label{principal-component-analysis}}

Principal component analysis is a good baseline of comparison for linear projections because of its frequent and broad use across disciplines. Principal component analysis \autocite[PCA,][]{pearson_liii._1901} creates new components that are linear combinations of the original variables. The creation of these variables is ordered by decreasing variation which is orthogonally constrained to all previous components. While the full dimensionality is intact, the benefit comes from the ordered nature of the components. The first 2 or 3 components are typically used to approximate the variation multivariate data set, while the rest are discarded.

\begin{figure}

{\centering \includegraphics[width=1\linewidth,]{./figures_from_script/ch4_fig1_pca_splom} 

}

\caption{Scatterplot matrix of the first 3 principal components for simulated data. Traditional multivariate visualization of *data*-space involes static display of the first several principal components.}\label{fig:ch4fig1}
\end{figure}

\hypertarget{scatterplot-matrix}{%
\section{Scatterplot matrix}\label{scatterplot-matrix}}

An extension to showing only a few components is to show pairs of components as a scatterplot matrix \autocite{chambers_graphical_1983}, where all pairs of the components are displayed on the upper or lower triangles of the matrix. This is a convenient way to fit many plots onto a page, but when there are many variables there is insufficient space on a page to display them all. Figure \ref{fig:ch4fig1} shows the first three components of simulated data as a scatterplot matrix.

\hypertarget{data-visualization-tours}{%
\section{Data visualization tours}\label{data-visualization-tours}}

A data visualization \emph{tour} uses time to animate local changes in the projection basis. One of the key features of the tour is the object permanence of the data points; that is to say by watching nearby frames one can track the relative changes of observations as the basis moves toward the next target basis. There are various types of tours that are distinguished by the selection or generation of their basis paths \autocite{lee_review_2021,cook_grand_2008}. To contrast with the discrete orientations of PCA, we compare with continuous changes of linear projection with \emph{grand} and \emph{radial} tours.

\hypertarget{grand-tours}{%
\subsection{Grand tours}\label{grand-tours}}

In a grand tour \autocite{asimov_grand_1985} the target bases are selected randomly. The grand tour is the first and most widely known tour. It will serve as an intermediate unit of comparison which has continuity of data points in nearby frames along with the radial tour but lacks the user control enjoyed by PCA and radial tours. This lack of control makes grand tours more of a generalist exploratory tool.

\hypertarget{radial-manual-tours}{%
\subsection{Radial (manual) tours}\label{radial-manual-tours}}

The \emph{manual} tour \autocite{cook_manual_1997} defines its basis path by manipulating the basis contribution of a selected variable. A manipulation dimension is appended onto the projection plane, with a full contribution given to the selected variable. The target bases are then selected based on rotating this newly created manipulation space. The target bases are then similarly orthogonally restrained, data projected, and rendered into an animation. For the variables to remain independent of each other, the contributions of the other variables must also change, \emph{ie.} dimension space should maintain its orthonormal structure. A key feature of the manual tour is that it affords users a way to control the variable contributions of the next target basis. This means that such manipulations can be selected and queued in advance or select on the spot for human-in-the-loop analysis \autocite{karwowski_international_2006}. However, this navigation is relatively time-consuming due to the huge volume of \(p\)-space (an aspect of the curse of dimensionality \autocite{bellman_dynamic_1957}) and the abstract method of steering the projection basis. It is advisable to first identify a basis of particular interest and then use a manual tour as a finer, local exploration tool to observe how the contributions of the selected variable do or do not contribute to the feature of interest.

To simplify the task and keep its duration realistic, we consider a variant of the manual tour, called a \emph{radial} tour. In a radial tour, the selected variable is allowed to change its magnitude of contribution, but not its angle; it must move along the direction of its original contribution radius. The radial tour benefits from both continuity of the data alongside grand tours, but also allows the user to steer via choosing the variable to rotate.

The recent implementation of manual tours us the R package \{spinifex\} \autocite{spyrison_spinifex_2020}, which facilitates manual tours (and radial variant). It is also compatible with tours made with \{tourr\} \autocite{wickham_tourr:_2011} and facilitates exporting to .gif or .html widget, with recent graphic packages. Now that we have a readily available means to produce various tours, we want to see how they fare against traditional discrete displays commonly used with PCA.

\hypertarget{sec:userstudy}{%
\chapter{User study}\label{sec:userstudy}}

An experiment was constructed to assess the performance of the radial tour relative to the tour and scatterplots of principal components for interpreting the importance of variables to class separations.

The three methods were examined for three different cluster shapes, using different combinations of contributing variables, and data dimensionality. Data was collected using a specially constructed web app, through crowd-sourced with prolific.co \autocite{palan_prolific_2018}.

\hypertarget{sec:blocks}{%
\section{Experimental factors}\label{sec:blocks}}

In addition to visual factor, we vary the data across 3 aspects: 1) The \emph{location} of the difference between clusters, by mixing a signal and a noise variable at different ratios, we vary the number of variables and their magnitude of cluster separation, 2) the \emph{shape} of the clusters, to reflect different distributions of the data, and 3) the \emph{dimension}-ality of the data.

\begin{figure}

{\centering \includegraphics[width=1\linewidth,]{./figures_from_script/ch4_fig2_exp_factors} 

}

\caption{Illustration of the experimental factors: visualization factor, the variable location of the cluster separationm, the shape of the clusters, and the dimensionality (and clusters) of the data.}\label{fig:ch4fig2}
\end{figure}

The \emph{location} of the separation of the clusters is a crucial aspect of analysis, it is the variables or combination their of that is important to the explanation of the structure. To test the sensitivity to this we mix a noise-variable with the signal-containing variable such that the difference in the clusters is mixed at the following percentages: 0/100\% (not mixed), 33/66\%, 50/50\% (evenly mixed).

In selecting the \emph{shape} of the clusters we follow the convention given by \textcite{scrucca_mclust_2016}, where 14 variants of model families containing 3 clusters are defined. The name of the model family is the abbreviation of its respective volume, shape, and orientation of the cluster, which are either equal or vary. We use the models EEE, EEV, and EVV, the latter is further modified by moving 4 fifths of the data out in a ``V'' or banana-like shape. Figure \ref{fig:ch4fig2} shows the principal component isodensity of the 3 model variants applied here.

\emph{Dimension}-ality is tested at 2 modest levels, namely, in 4 dimensions containing 3 clusters and 6 dimensions with 4 clusters. We must do so to bound the difficulty and search space to keep the task realistic for crowdsourcing.

\hypertarget{sec:objective}{%
\section{Objective}\label{sec:objective}}

PCA will be used as a baseline for comparison as it is the most common linear embedding. The grand tour will act as a secondary control that will help evaluate the benefit of animation, with the persistence of the data points across changes in basis, but without the ability to influence its path. Lastly, the radial tour should perform best as it benefits both from animation and being able to select an individual variable to change the contribution. Next we cover how we expect them to perform and state the hypothesis to test.

Then for some subset of tasks, we expect to find that the radial tour performs most accurately, as it enjoys both the persistence of points and input control to explore specific variables. Secondly, it may be the case that grand performs faster than the alternatives with its absence of inputs, users can focus all of their attention on interpreting the fixed path. Conversely, we are less certain about the accuracy of such a limited grand tours as there is no objective function in the target bases; it is possible that, by chance, the planes completely avoid the information needed. However, given that the data dimensionality will be modest, it seems likely that grand tour regularly crosses frames with the correct information to perform the task quickly.

We measure the accuracy and speed over the support of the discussed experimental factors. The null hypotheses can be stated as:

\(~~~~~H_0: \text{visualization factor does not impact task } \textit{accuracy}, Y_1. \\\)
\(~~~~~~H_0: \text{visualization factor does not impact task } \textit{speed}, Y_2. \\\)

\hypertarget{sec:task}{%
\section{Task and evaluation}\label{sec:task}}

With our hypothesis formulated let's turn our attention to the task and how to evaluate it.
Recall that the display was a 2D scatterplot with axis biplot to its left. Observations were supervised with the cluster level coded by color and shape.

Participants were asked to `check any/all variables that contribute more than average to the cluster separation green circles and orange triangles', which was further explained in the explanatory video as `mark and all variable that carries more than their fair share of the weight, or 1 quarter in the case of 4 variables'.

The instructions iterated several times in the video was: 1) Use the input controls to find a frame that contains separation between the clusters of green circles and orange triangles, 2) look at the orientation of the variable contributions in the gray circle, a visual depiction of basis, and 3) select all variables that contribute more than average in the direction of the separation in the scatterplot. Regardless of factor and block values participants were limited to 60 seconds for each evaluation of this task.

The evaluation measure of this task was designed with a couple of features in mind: 1) the sum of squares of the individual variable marks should be 1, and 2) symmetric about 0, without preference to under- or over-guessing. With these in mind, we define the following measure for evaluating the task.

Let a dataset \(\textbf{X}\) be a simulation containing clusters of observations of different distributions. Let \(\textbf{X}_k\) be the subset of observations in cluster \(k\) containing the \(p\) variables.

\begin{align*}
W_{j} &=\frac
{(\overline{X_{j=1, k=1}} - \overline{X_{1, k=2}}, ~...~
(\overline{X_{j=p, k=1}} - \overline{X_{j=p, k=2}})}
{\sum_{j=1}^{p}(|\overline{X_{j,k=1}} - \overline{X_{j,k=2}}|)}
- \frac{1}{p} \\
\\
\text{Accuracy}, Y &= \sum_{j=1}^{p}I(r_j) * sign(w_j) * \sqrt{|w_j|}  \\
\end{align*}

where \(I\) is the indicator function. Then the total marks for this task is the sum of this marks vector. We use the time till the last response as a secondary dependent variable \(Y_2\).

\begin{figure}

{\centering \includegraphics[width=1\linewidth,]{./figures_from_script/ch4_fig3_accuracy_measure} 

}

\caption{(L), PCA biplot of the components showing the most cluster separation with (R) illustration of the magnitude of cluster separation is for each variable (wide bar) and the weight of the marks given if a variable is selected (red/green line). The horizontal dashed line is 1 / dimensionality, the amount of separation each variable would have if evenly distributed. The weights equal the signed square of the difference between each variable value and the dashed line.}\label{fig:ch4fig3}
\end{figure}

\hypertarget{sec:standardization}{%
\section{Visual design standardization}\label{sec:standardization}}

Section \ref{sec:background} gives the sources and a description of the visual factors PCA, grand tours, and radial manual tours. The factors are tested within-participant, with each factor being evaluated by each participant. The order that factors are experienced is controlled with the block assignment as illustrated below in Figure \ref{fig:ch4fig4}. Below we cover the visual design standardization, as well the input and display within each factor.

The visualization methods were standardized wherever possible. each factor was shown as a biplot, with variable contributions displayed on a unit circle. All aesthetic values (colors, shapes, sizes, absence of legend, and absence of axis titles) were held constant. Variable contributions were always shown left of the scatterplot embeddings with their aesthetic values consistent as well. What did vary between factors were their inputs which caused a discrete jump to another pair or principal components, were absent for the grand tour with target bases to animate through selected at random, or for the radial tour which variable should have its contribution animated.

PCA inputs allowed for users to select between the top 4 principal components for both the x and y-axis regardless of the data dimensionality (either 4 or 6). There was no user input for the grand tour, users were instead shown a 15-second animation of the same randomly selected path. Users were able to view the same clip up to 4 times within the time limit. Radial tours were also displayed at 5 frames per second within the interpolation step size of 0.1 radians. Users were able to swap between variables, upon which the display would change the start of radially increasing the contribution of the selected variable till it was full, zeroed, and then back to its initial contribution. The complete animation of any variable takes about 20 seconds and is almost fully in the projection frame at around 6 seconds. The starting basis of each is initialized to a half-clock design, where the variables were evenly distributed in half of the circle which is then orthonormalized. This design was created to be variable agnostic while maximizing the independence of the variables.

\hypertarget{data-simulation-task}{%
\section{Data simulation, task}\label{data-simulation-task}}

Each dimension is originally distributed as \(\mathcal{N}(2 * I(signal), 1)~|~\text{covariance}~\Sigma\), a function of the shape. Signal variables have a correlation of 0.9 when they have equal orientation and -0.9 when their orientations vary. Noise variables were restricted to 0 correlation. Each cluster is simulated with 140 observations and is offset in a variable that does not separate previous variables.

Clusters of the EVV shape are transformed to the banana-chevron shape. Then location mixing is applied by post-multiplying a (2x2) rotation matrix to the signal variable and a noise variable for the clusters in question.

All variables are then standardized by standard deviation. The rows and columns are then shuffled randomly. The observation's cluster and order of shuffling are attached to the data and saved.

Each of these replications are then iterated with each level of the factor. For PCA, every pair of the top 4 principal components and saved as 12 plots. For the grand tour, we first save 2 basis paths (for 4 and 6 dimension), each replication is then projected through the common basis path as the variable(s) containing the were previously shuffled. The resulting animations were saved as .gif files. The radial tour starts at either the 4 or 6-variable ``half-clock'' basis, where each variable has a uniform contribution, and no variable contributing in the opposite direction (to minimize variable dependence), a radial tour is then produced for each variable and saved as a .gif.

\hypertarget{data-collection-and-factor-assignment}{%
\section{Data collection, and factor assignment}\label{data-collection-and-factor-assignment}}

Now, with simulation and their artifacts in hand. We explain how the experimental factors are assignment, and illustrate how this is experienced from a participant's perspective.

We section the study into 3 periods, each period is linked to a randomized level of both the factor visualization and the location. The order of dimension and shape are of secondary interest and are held constant in increasing order of difficultly; 4 then 6 dimensions and EEE, EEV, then EVV-banana respectively.

The period starts with an untimed training task at the simplest remaining block parameterization; location = 0/100\%, shape = EEE, and 4 dimensions with 3 clusters. This serves to introduce and familiarize participants with input and visual differences. After the training, the participant is evaluated on 2 tasks with the same factor * location level, across the increasing difficulty of dimension * shape. These evaluations removed the plot after 60 seconds, though this limit was rarely reached by participants.

The order of the levels of the factor and location is randomized with a nested Latin square where all levels of factor are exhausted before advancing to the next level of location. That means we need \(3!^2 = 36\) participants to perform a full block evaluation. This randomization is important to control for any potential learning effects the participant may receive. Figure \ref{fig:ch4fig4} illustrates how an arbitrary participant experiences the experimental factors.

\begin{figure}

{\centering \includegraphics[width=1\linewidth,]{./figures_from_script/ch4_fig4_randomization_MANUAL} 

}

\caption{Illustration of the nested latin square and how a hypothetical participant 63 is assigned factor and block parameterizations. Each of the 6-factor permutations is exhausted before iterating to the next permutation of location.}\label{fig:ch4fig4}
\end{figure}

Through pilot studies sampled by convenience (information technology and statistics Ph.D.~students attending Monash University), we predict that we need 3 full evaluations to properly power our study; we set out to crowdsource \(N = 3 * 3!^2 = 108\) participants.

\hypertarget{sec:subjects}{%
\section{Recruiting subjects}\label{sec:subjects}}

We recruited \(N = 108\) participants via prolific.co \autocite{palan_prolific_2018}. We filtered participants based on their claimed education requiring that they have completed at least an undergraduate degree (some 58,700 of the 150,400 users at the time); we apply this filter under the premise that linear projections and biplot displays used will not be regularly used for consumption by general audiences. There is also the implicit filter that Prolific participants must be at least 18 years of age and location/language bias associated with. Participants were compensated for their time at \pounds 7.50 per hour, whereas the mean duration of the survey was about 16 minutes. We can't preclude previous knowledge or experience with the factors, but validate this assumption in the follow-up survey where we ask about familiarity with the factors (see Figure \ref{fig:ch4fig6}). The appendix contains a heatmap distribution of age and education paneled across preferred pronouns of the participants that completed the survey, who are relatively young and well educated.

\hypertarget{collecting-participant-data}{%
\section{Collecting participant data}\label{collecting-participant-data}}

Data were recorded by a \{shiny\} application and were written to a Google Sheet after each third of the study. Especially at the start of the study, participants experienced adverse network conditions due to the volume of participants hitting the application with modest allocated resources. In addition to this, API read/write limitations further hindered data collection. To mitigate this we throttled the volume of participant and over-collect survey trials until we had received our target 3 evaluations of our 36 permutation levels.

The processing steps were minimal. First, we format to an analysis ready form, decoding values to a more human-readable state, and add a flag to indicate if the survey had complete data. We filter to only the latest 3 complete studies of each block parameterization, those which should have experienced the least adverse network conditions. Of the studies removed the bulk were partial data and a few of over sampled permutations. This brings us to the 108 studies described in the paper, from which models and aggregation tables were built. The post-study surveys were similarly decoded to human-readable format and then filtered to include only those 84 surveys that were associated with the final 108 studies.

The code, response files, their analyses, and the study application are publicly available at on GitHub \url{https://github.com/nspyrison/spinifex_study}.

\hypertarget{sec:results}{%
\chapter{Results}\label{sec:results}}

To recap, the primary response variable is task marks as defined in section \ref{sec:task}, and the log of response time will be used as a secondary response variable. We have 2 primary data sets; the user study evaluations and post-study survey. The former is contains the 108 trials with explanatory variables: visual factor, location of the cluster separation signal, the shape of variance-covariance matrix, and the dimension-ality of the data. Block parameterization and randomization were discussed in section \ref{sec:blocks}. The survey was completed for 84 of these 108 trials and contains demographic information (preferred pronoun, age, and education), and subjective measures for each of the factors (preference, familiarity, ease of use, and confidence).

Below we look at the marginal performance of the block parameters and survey responses. After that, we build a battery of regression models to explore the variables and their interactions. Lastly, we look at the subjective measures between the factors.

\hypertarget{random-effect-regression-against-marks}{%
\section{Random effect regression against marks}\label{random-effect-regression-against-marks}}

To more thoroughly examine explanatory variables, we regress against marks. All models have a random effect term on the participant, which captures the effect of the individual participant. After we look at models of the block parameters we extend to compare against survey variables. Last, we compare how adding a random effect for data and regressing against time till last response fares against benchmark models. The matrices for models with more than a few terms quickly become rank deficient; there is not enough information in the data to explain all of the effect terms. In which case the least impactful terms are dropped.

In building a set of models to test we include all single term models, a model with all independent terms. We also include an interaction term of factor by location, allowing for the slope of each location to change across each level of the factor, which is feasible. For comparison, an overly complex model with many interaction terms is included.

\[
\begin{array}{ll}
\textbf{Fixed effects:}          &\textbf{Full model:} \\
\alpha                           &\widehat{Y_1} = \mu + \alpha_i + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha + \beta + \gamma + \delta &\widehat{Y_1} = \mu + \alpha_i + \beta_j + \gamma_k + \delta_l + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha * \beta + \gamma + \delta &\widehat{Y_1} = \mu + \alpha_i * \beta_j + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha * \beta * \gamma + \delta &\widehat{Y_1} = \mu + \alpha_i * \beta_j * \gamma_k + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha * \beta * \gamma * \delta &\widehat{Y_1} = \mu + \alpha_i * \beta_j * \gamma_k * \delta_l + \textbf{Z} + \textbf{W} + \epsilon
\% \end{array}
\% \]
\% \[
\% \begin{array}{ll}
\text{where } &\mu \text{ is the intercept of the model including the mean of random effect} \\
&\alpha_i \text{, fixed term for factor}~|~i\in (\text{pca, grand, radial}) \\
&\beta_j  \text{, fixed term for location}~|~j\in (\text{0\_1, 33\_66, 50\_50}) \text{ \% noise/signal mixing} \\
&\gamma_k \text{, fixed term for shape}~|~k\in (\text{EEE, EEV, EVV banana}) \text{ model shapes} \\
&\delta_l \text{, fixed term for dimension}~|~l\in (\text{4 variables \& 3 cluster, 6 variables \& 4 clusters}) \\
&\textbf{Z} \sim \mathcal{N}(0,~\tau), \text{ the random effect of participant} \\
&\textbf{W} \sim \mathcal{N}(0,~\upsilon), \text{ the random effect of simulation} \\
&\epsilon   \sim \mathcal{N}(0,~\sigma), \text{ the error of the model} \\
\end{array}
\]

\begin{verbatim}
# A tibble: 5 x 8
  `Fixed effects` `No. levels` `No. terms` AIC    BIC    `R2 cond. (on RE)`
  <chr>                  <int>       <int> <chr>  <chr>  <chr>             
1 a                          1           3 *1000* *1027* 0.18              
2 a+b+c+d                    4           8 1026   1075   0.187             
3 a*b+c+d                    5          12 1036   1103   0.198             
4 a*b*c+d                    8          28 1069   1207   0.238             
5 a*b*c*d                   15          54 1125   1380   *0.255*           
# ... with 2 more variables: R2 marg. (w/o RE) <chr>, RMSE <chr>
\end{verbatim}

\begin{verbatim}
                     Estimate Std. Error    df t value Pr(>|t|)    
(Intercept)             -0.12       0.08  43.9   -1.50     0.14    
fct=grand                0.15       0.09 622.4    1.74     0.08    
fct=radial               0.37       0.09 617.1    4.18     0.00 ***
loc=33_66                0.17       0.09  83.2    1.78     0.08    
loc=50_50                0.14       0.09  84.8    1.52     0.13    
shp=EEV                  0.04       0.06  11.5    0.79     0.44    
shp=ban                 -0.03       0.06  11.5   -0.48     0.64    
dim=6                   -0.06       0.05  11.5   -1.39     0.19    
fct=grand:loc=33_66     -0.06       0.13 587.3   -0.49     0.63    
fct=radial:loc=33_66    -0.34       0.13 585.2   -2.65     0.01  **
fct=grand:loc=50_50     -0.09       0.13 589.6   -0.68     0.50    
fct=radial:loc=50_50    -0.19       0.13 574.3   -1.43     0.15    
\end{verbatim}

We also want to visually explore the conditional variables in the model. Figure \ref{fig:ch4fig5} explores violin plots of marks by factor while faceting on the location (vertical) and shape (horizontal). Radial tends to increase the marks received, and especially so when there is no signal/noise mixing.

\begin{figure}

{\centering \includegraphics[width=1\linewidth,]{./figures_from_script/ch4_fig5_ABcd_violins} 

}

\caption{Violin plots of terms of the model $\widehat{Y_1} = \alpha * \beta + \gamma + \delta$. Overlaid with global significance from the Kruskal-Wallis test, and pairwise significance from the Wilcoxon test, both are non-parametric, ranked sum tests suitable for handling discrete data. Participants are more confident and find the radial easier to use relative to the grand tour. Participants claim low familiarity as we expect from crowdsourced participants. Radial is more preferred compared with either alternative for this task.}\label{fig:ch4fig5}
\end{figure}

\hypertarget{subjective-measures}{%
\section{Subjective measures}\label{subjective-measures}}

The 84 evaluations of the post-study survey also collect 4 subjective measures for each factor. Figure \ref{fig:ch4fig6} shows the Likert plots, or stacked percentage bar plots, alongside violin plots with the same non-parametric, ranked sum tests previously used. Participants preferred to use radial for this task. Participants were also more confident of their answers and found radial tours easier to use compared with the grand tour. All factors have reportedly low familiarity something we expect from crowdsourced participants.

\begin{figure}

{\centering \includegraphics[width=1\linewidth,]{./figures_from_script/ch4_fig6_subjective_measures} 

}

\caption{The subjective measures of the 84 responses of the post-study survey, 5 discrete Likert scale levels of aggrement. (L) Likert plots (stacked percent bar plots) with (R) violin plots of the same measures. Violin plots are overlaid with global significance from the Kruskal-Wallis test, and pairwise significance from the Wilcoxon test, both are non-parametric, ranked sum tests.}\label{fig:ch4fig6}
\end{figure}

\hypertarget{sec:conclusion}{%
\chapter{Conclusion}\label{sec:conclusion}}

Above we discussed an \(n=108\), with-in participant user study comparing the efficacy of 3 linear projection techniques. The participants performed a supervised cluster task, specifically the identification of which variables contribute to the separation between 2 target clusters. This was evaluated evenly over 4 block parameterizations. In summary, we find that radial tour increases accuracy while the grand tour decreases the time it takes to perform this task. These effects are large relative to the other block parameterizations, but smaller than the random effect of the participant. Radial tour was subjectively most preferred, lead to more confidence in answers, and is easier to use than alternatives.

There are several ways that this study could be extended. In addition to expanding the support of the block parameterizations, more interesting directions include: type of the task, visualizations used, and experience level of the target population. It is difficult to achieve good coverage given the number of possible permutations. Be sure to step back and plan the target support of your block parameters. Keep in mind the volume and quality of responses from participants especially when crowdsourcing. These planning steps are useful for navigating when the complexity of the application details.

\hypertarget{sec:spinifex}{%
\chapter{Accompanying tool: radial tour application}\label{sec:spinifex}}

To accompany this study we have produced a more general use tool to perform such exploratory analysis of high dimensional data. The R package, \{spinifex\}, \autocite{spyrison_spinifex_2020} contains a free, open-source \{shiny\} \autocite{chang_shiny_2020} application. The application allows users to upload, perform limited preprocessing, and interactively explore their data via interactive radial tour. The .html widget produced is a more interactive variant relative to the application in the user study. Screencaptures and more details are provided in the appendix. Data can be imported from .csv, .rds, or .rda format, and projections. Run the following R code to run the application locally.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"spinifex"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{spinifex}\SpecialCharTok{::}\FunctionTok{run\_app}\NormalTok{(}\StringTok{"radial\_tour"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec:acknowledgments}{%
\chapter{Acknowledgments}\label{sec:acknowledgments}}

This research was supported by an Australian Government Research Training Program (RTP) Scholarship. This article was created in R \autocite{r_core_team_r:_2020} and \{rmarkdown\} \autocite{xie_r_2018}. Visuals were prepared with \{spinifex\}. All packages used are available from the Comprehensive R Archive Network (CRAN) at \url{https://CRAN.R-project.org/}. The source files for this article, application, data, and analysis can be found at \url{https://github.com/nspyrison/spinifex_study/}. The source code for the \{spinifex\} package and accompanying shiny application can be found at \url{https://github.com/nspyrison/spinifex/}.

\printbibliography[heading=bibintoc]



\end{document}
