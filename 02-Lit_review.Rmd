---
chapter: 2
knit: "bookdown::render_book"
---

```{r setup_02, include=FALSE}
knitr::opts_chunk$set(
  fig.align = "center",
  echo = F, message = F, warning = F, error = F, cache = F, cache.lazy = F
)
library(spinifex)
library(gganimate) 
library(ggplot2)
library(magrittr)
library(knitr)
library(kableExtra)

## FUNCTION FOR STATIC OUTPPUT, for chap3 use to clean up code chunks.
array2static <- function(.m_tour, .data, .m_var, .cat, .mag =2.2)
{
  slides       <- array2df(array = .m_tour, data = .data)
  basis_slides <- slides$basis_slides
  data_slides  <- slides$data_slides
  n_slides     <- max(basis_slides$slide)
  if (n_slides != 15) 
    stop(paste0("stop: n_slides !=15 !!!!! n_slides = ", n_slides))
  
  # Initialize
  ## manip var asethetics
  p             <- nrow(basis_slides) / n_slides
  col_v         <- rep("grey80", p)
  col_v[.m_var] <- "blue"
  col_v         <- rep(col_v, n_slides)
  siz_v         <- rep(0.3, p)
  siz_v[.m_var] <- 1
  siz_v         <- rep(siz_v, n_slides)
  cat           <- rep(as.factor(.cat), n_slides)

  ## circle
  angle <- seq(0, 2 * pi, length = 180)
  circ  <- data.frame(c_x = cos(angle), c_y = sin(angle))
  circ[nrow(circ)+1, ] <- NA
  ## data asethetics
  data_slides <- data.frame(data_slides, cat = rep(.cat, n_slides))
  
  grid_b <- grid_t <-
    data.frame(slide = 1:n_slides, 
               x = .mag*rep(1:5, 3), y = .mag*rep(3:1, each = 5))
  grid_t$y <- grid_t$y + max(grid_t$y)
  # OUTER JOIN
  basis_grid <- merge(x = basis_slides, y = grid_t, by = "slide", all = TRUE)
  # CROSS JOIN
  circ_grid  <- merge(x = circ, y = grid_t, by = NULL)
  # OUTER JOIN
  data_grid  <- merge(x = data_slides, y = grid_b, by = "slide", all = TRUE)
  
  # BASIS
  gg1 <-
    ggplot(data = basis_grid) +
    # AXES LINE SEGMETNS
    geom_segment(aes(x = V1 + x, y = V2 + y, xend = x, yend = y),
                 color = col_v, size = siz_v) +
    # AXES TEXT LABELS
    geom_text(aes(x = V1 + x, y = V2 + y, label = lab_abbr),
              color = col_v, vjust = "outward", hjust = "outward") +
    # AXES FRAME NUM
    geom_text(aes(x = x - .7, y = y + 1.1, 
                  label = paste0("frame: ",slide)), color = "grey50") +
    # AXES CIRCLE PATH
    suppressWarnings( # Suppress for "Removed 1 rows containing missing values."
      geom_path(data = circ_grid, color = "grey80",
                mapping = aes(x = x+c_x, y = y+c_y))
    )
  
  # PROJECTION
  gg2 <- gg1 +
    # PROJ DATA POINTS
    geom_point(data = data_grid, size = .7,
               mapping = aes(x = V1 + x, y = V2 + y, color = cat),
               shape = as.integer(cat) + 15) +
    # PROJ DATA FRAME NUM
    geom_text(data = data_grid, color = "grey50",
              mapping = aes(x = x - .7, y = y + 1.1, 
                            label = paste0("frame: ",slide))) +
    theme_void() +
    scale_color_brewer(palette = "Dark2") + 
    # coord_fixed() +
    theme(legend.position="none",
          panel.border = element_rect(colour = "black", fill = NA))
  
  gg2
}
```

<!-- Imagine you are writing for your fellow PhD students. Topics that are well-known to them do not have to be included here. But things that they may not know about should be included. -->

<!-- Resist the temptation to discuss everything you've read in the last few years. And you are not writing a textbook either. This chapter is meant to provide the background necessary to understand the material in subsequent chapters. Stick to that. -->

<!-- You will need to organize the literature review around themes, and within each theme provide a story explaining the development of ideas to date. In each theme, you should get to the point where your ideas will fit in. But leave your ideas to later chapters. This way it is clear what has been done beforehand, and what new contributions you are making to the research field. -->


<!-- TODO: move this to tours lit rev. -->
<!-- This thesis focuses on tour methods for visualizing high-dimensional data. Tours are a family of algorithms for generating paths on the space of low-dimensional ($d=1, 2, 3, ..., p$) projections of high-dimensional ($p$) space. The resulting projection of the data (or model) is displayed using low-dimensional techniques such as histograms, dot plots, scatterplots, or parallel coordinate plots, and the path generates a movie or animation to shows many low-dimensional projections. The method can be applied with other techniques such as machine learning techniques like discriminant analysis, neural networks, support vector machines, to open the black box, and complements dimension reduction techniques like principal component analysis (PCA), multidimensional scaling (MDS) on nonlinear embeddings (e.g. tSNE).  -->

# Literature review {#ch:lit_review}

## Dynamic projections of multivariate data (tours) {#sec:tour}


### Overview

Exploratory data analysis (EDA) is a critical preliminary task at the root of any analysis. Understanding the distribution and summaries are at the heart of choosing and validating the assumptions of any methodology. However, there are risks associated with only looking at summary statistics alone. @anscombe_graphs_1973 and @matejka_same_2017 both point out the dangers of doing so, namely missing glaring patterns and peculiarity that can be held within data even when summary statistics are the same. It's clear that visual intuition, and consequently human-in-the-loop analysis, is still is a pivotal aspect of EDA.

There is an issue with visualizing data as it is dimensionality increases. In univariate data sets histograms, or smoothed density curves are employed to visualize data. In bivariate data $x-y$ scatterplots and contour plots (2D density) can be employed. In three dimensions the two most common techniques are 3D scatterplot\footnote{Graphs depicting 3 dimensions are typically printed on paper, or rendered on a 2D monitor, they are intrinsically 2D images of monocular 3D spaces, sometimes referred to as 2.5D data visualization, more on this in section \ref{sec:3d-terminology}.} or 2D scatterplot with the 3rd variable as an aesthetic (such as color, size, or height). Such variable mappings can afford another dimension or 2, but this is not a sustainable solution.

Visualization of multivariate data for modest $p$ numeric dimension, even 6 or 8, quickly becomes complex in. It's far too common that visualizing in data-space is dropped altogether in favor of modeling parameter-space, model-space, or worse: long tables of statistics without visuals [@wickham_visualizing_2015]. A solution that better scales with the dimensionality of data is needed; this is where dimensionality reduction comes in. This work will focus on a group of dynamic linear projection techniques know as *touring*. Broader review of dimensionality reduction techniques is discussed in @grinstein_high-dimensional_2002, and @heer_tour_2010. Tours are used for a couple of salient features: use of linear projections maintaining transparency back to the original variable space (which non-linear projections lose) and keeps all components and their information in tack (which static linear projections lose). Employing the breadth of tours extends the dimensionality visualization, and with it, the intrinsic understanding of structure and distribution of data that is more succinct or beyond the reach of summary statistics alone.

Tours perform linear dimensionality reduction, orthogonally projecting $p$-space down to $d(\leq p)$ dimensions. Many such projections are interpolated, each making small rotations in $p$-space. These frames are then viewed in order as an animation of the lower dimensional embedding changing as the original variable space is manipulated. Shadow puppets offer a useful analogy to aid in conceptualizing touring. Imagine a fixed light source facing a wall. When an object is introduced it projects a 2D shadow onto the wall. This is a physical representation of a simple projection, that from $p=3$ down to $d=2$. If the object rotates then the shadow correspondingly changes. Observers watching only the shadow are functionally watching a $d=2$ tour as the 3D object is manipulated. Some orientations explain more information about the shape of the object than others but watching an animation of the shadow changing gives a more robust understanding than looking at any one still frame. More complex structures generally require more time to comprehend the nature of the geometry. These features hold true in touring as well.


### Terminology and notation {#sec:terminology_tours}

Terminology varies across articles. In my work, I use the following:

* $n$, number of observations in the data.
* $p$, number of numeric variables, the dimensionality of data space.
* $d$, dimensionality of projection space.
* $\textbf{X}_{[n,~p]}$, a data matrix in variable-space, $\textbf{X} \in \mathbb{R}^{p}$. Typically centered, scaled, and optionally sphered.
* $\textbf{B}_{[p,~d]}$, orthonormal basis set ($d$ linear combinations of $p$ variables, each at a right angle, to the others, with a norm of 1), defining the axes directions for the projection from $p-$ to $d-$space.
* $\textbf{Y}_{[n,~d]}$, projected data matrix in projection-space, $\textbf{Y} \in \mathbb{R}^{d}$.
* For projections down to 1- and 2D, it is common to display each variables contribution and direction on its own axis (1D) or relative to a unit circle (2D), this is referred to as basis axes or sometimes the reference frame.
* Geometric objects are referred to in generalized dimensions; the use of plane is not necessarily a 2D surface, but a hyper-plane in the arbitrary dimensions of the projection space.

### Example illustrations

This sections a toy data set of 74 observations of flea beetles across 6 numeric variables corresponding to physical measurements, $\textbf{X}_{[74,~6]} \in \mathbb{R}^{6}$. Each observation belongs to one of 3 species as shown in the color and shape of the data points. Figure \@ref(fig:flea-refframe) shows one frame of a tour (left) and the corresponding reference frame (right) showing the linear combination of the variables onto the projection-space, a visual representation of the basis.

```{r flea-refframe, results='hide', fig.cap = "(left) one frame of a x-y scatterplot projection, the results of holes tour with a corresponding reference frame indicating the direction and magnitude the variables contribute to the 2D projection."}
f_dat  <- tourr::rescale(flea[,1:6])
f_cat  <- factor(flea$species)
f_path <- save_history(f_dat, guided_tour(holes()))
f_bas  <- matrix(f_path[,, max(dim(f_path)[3])], ncol=2)
f_mvar <- 5
f_proj <- data.frame(tourr::rescale(f_dat %*% f_bas))

view_basis(f_bas, labels = colnames(f_dat)) +
  geom_point(data = f_proj, 
             mapping = aes(x = X1 - 2, y = X2 - .5, color = f_cat), 
             pch = as.integer(f_cat) + 15)
```

```{r flea-basis}
rownames(f_bas) <- colnames(f_dat)
colnames(f_bas) <- c("x", "y")

#kable(f_bas) #%>% kable_styling(latex_options = "striped")
```

<!-- Removed cause analysis -->
<!-- In this projection you can see that the variables `aede1` and `tars2` are on the direction that is mostly orthogonal to the other variables, but explain in the clustering offset of the purple points, whereas the other variables explain the distinction between the green and orange groups.  -->

Tours view many such frames in sequence, by identifying some target frames and then interpolating between them as shown schematically in figure \@ref(fig:buja05fig), captured from figure 1 of @buja_computational_2005. For illustration figure \@ref(fig:flea-static) lays out frames of a tour,
A html version of a user-controlled steering can be found at https://nspyrison.netlify.com/thesis/flea_manualtour_mvar5/.
The sections below will outline path generation and enumerate display geoms.

(ref:buja05fig-cap) Screen capture of figure 1 from @buja_computational_2005. A schematic representation of randomly generated planes (from a grand tour) and intermediate interpolation planes. 

```{r buja05fig, echo=F, out.width='70%', fig.cap = "(ref:buja05fig-cap)"}
knitr::include_graphics("./figures/buja05fig.PNG")
```

(ref:flea-static-cap) Illustration of a radial manual tour (RO #1), a dynamic version can be viewed at https://nspyrison.netlify.com/thesis/flea_manualtour_mvar5/.

```{r flea-static, echo=F, warning=F, fig.height=7.2, fig.width=6, out.height='7.2in', out.width='6in', fig.cap = "(ref:flea-static-cap)"}
f_angle <- .27
f_mtour <- manual_tour(f_bas, manip_var = f_mvar, angle = f_angle)
if(dim(f_mtour)[3] != 15) message(
  paste0("!!!!! step3 n_slides != 15 !!!!! n_slides = ", dim(f_mtour)[3]))

#play_manual_tour(f_dat, f_bas, f_mvar, cat_var = f_cat, angle = f_angle)
array2static(.m_tour = f_mtour, .data = f_dat, .m_var = f_mvar, .cat = f_cat)
```


### History

Touring was first introduced by @asimov_grand_1985 with his purposed *grand tour* at the Stanford Linear Accelerator, Stanford University. In which, Asimov suggested three types of grand tours: torus, at-random, and random-walk. The original application of touring was on high energy physics on the PRIM-9 system.

Before choosing projection paths randomly, an exhaustive search of $p-$space of was suggested by @mcdonald_interactive_1982, also at the Stanford Linear Accelerator. This was later coined as *little tour*.
<!-- Buja and Asimov were in the acknowledgements. -->

@friedman_projection_1974 and later @huber_projection_1985 purposed projection pursuit (also referred to as PP). Projection pursuit involves identifying "interesting" projection, remove a single component of the data, and then iterates in this newly embedded subspace. Within each subspace the projection seeks for a local extremum via hill climbing algorithm on an objective function. This formed the basis for *guided tours* suggested by @hurley_analyzing_1990.

<!-- * torus: where a $p$-dimensional torus, $T^p$ is created from a Cartesian product of $p$ unit circles with $T^p \in \mathbb{R}^p$. Unfortunately, uniformity of the parameters does not correlate to uniform points on the surface of the torus. If step distance between frames is fixed, disproportionate time is spent between subspaces. If step distance is change to account for uniform points on the torus then the continuity of the tour is lost.   -->
<!-- * at-random: where each 2-frame is chosen at random without replacement. This affords an assured uniform distribution of subspaces but is far too discontinuous for observation. It also leaves no parameters to control. -->
<!-- * random-walk: combines the continuity of the torus method and the uniformity of the at-random method while leaving room for a control parameter.  -->

The grand and little tour have no input from the user aside from the starting basis. Guided tours allow for an index to be selected, but the bulk of touring development since has largely been around dynamic display, user interaction, geometric representation, and application. The extent to which will be expounded on in the following sections.
<!--see c2 paper, quoting: buja & asimov 1986, Hurley & Buja 1990, Wegman 1991, cook, buja, cabrera, & hurley 1995, buja, cook asimov & hurley 1997,cook & buja 1997. -->


### Path generation

A fundamental aspect of touring is the path of rotation. Of which there are four primary distinctions [@buja_computational_2005]: random choice, data driven, precomputed choice, data driven, and manual control.

* Random choice, *grand tour*, constrained random walks $p$-space. Paths are constrained for changes in direction small enough to maintain continuity and aid in user comprehension
    + torus-surface [@asimov_grand_1985]
    + at-random [@asimov_grand_1985]
    + random-walk [@asimov_grand_1985]
    + *local tour* [@wickham_tourr_2011], a sort of grand tour on a leash, such that it goes to a nearby random projection before returning to the original position and iterating to a new nearby projection.
* data driven, *guided tour*, optimizing some objective function/index within the projection-space, called projection pursuit (PP) [@hurley_analyzing_1990], including the following indexes:
    + holes [@cook_projection_1993] - moves points away from the center.
    + cmass [@cook_projection_1993] - moves points toward the center. 
    + lda [@lee_projection_2005] - linear discriminant analysis, seeks a projection where 2 or more classes are most separated.
    + pda [@lee_projection_2010] - penalized discriminant analysis for use in highly correlated variables when classification is needed.
    + convex [@laa_using_2019] - the ratio of area of convex and alpha hulls.
    + skinny [@laa_using_2019] - the ratio of of the perimeter distance to the area of the alpha hull.
    + stringy [@laa_using_2019] - based on the minimum spanning tree (MST), the diameter of the MST over the length of the MST.
    + dcor2D [@grimm_mbgraphic:_2017; @laa_using_2019] - distance correlation that finds linear and non-linear dependencies between variables.
    + splines2D [@grimm_mbgraphic:_2017; @laa_using_2019] - measure of non-linear dependence by fitting spline models.
    + other user-defined objective function can be implemented with the *tourr* package @wickham_tourr_2011.
* Precomputed choice, *planned tour*, in which the path has already been generated or defined.
    + *little tour* [@mcdonald_interactive_1982], where every permutation of variables is stepped through in order, analogous to a brute-force or exhaustive search.
    + a saved path of any other tour, typically an array of basis targets to interpolate between.
* Manual control, *manual tour*, a constrained rotation on selected manipulation variable and magnitude [@cook_manual_1997]. Typically used to explore the local area after identifying an interesting feature, perhaps via guided tour.
* *dependence tour*, combination of $n$ independent 1D tours. A vector describes the axis each variable will be displayed on. *ie* $c(1, 1, 2, 2)$ is a 4- to 2D tour with the first 2 variables on the first axis, and the remaining on the second.
    + *correlation tour* [@buja_data_1987], a special case of the dependence tour, analogous to canonical correlation analysis.

<!-- ### Interpolation -->

<!-- NOTE: Search 'Interpolator' in Wickham’s tourr paper--> 
<!-- After target bases are identified, the frames in-between need to be filled in. There are several methods to do so: -->

<!-- * Geodesic - via Gram-Schmidt process -->
<!-- * Givens rotations -->
<!-- * Householder reflections -->

### Path evaluation 

Consider $d=2$, then each projection is called a 2-frame (each spanning a 2-plane). Mathematically, a Grassmannian is the set of all possible unoriented 2-frames in $p$-space a , $\textbf{Gr}(2,~p)$. @asimov_grand_1985 pointed out that the unique 2-frames of the grand tour approaches $\textbf{Gr}(2,~p)$ as time goes to infinity. The *density* of a tour is defined as the fraction of the Grassmannian explored. Ideally an exploring tour will be dense, but the time taken to become dense vastly increases as variable space increase dimensionality. *Rapidity* is then defined as how quickly a tour encompasses the Grassmannian. Due to the random selection of a grand tour it will end up visiting homomorphisms of previous 2-frames, leading sub-optimal rapidity. 

The little tour introduced in @mcdonald_interactive_1982, on the other hand is necessarily both dense and rapid, performing essentially an exhaustive search on the Grassmannian. However, this path uninteresting and with long periods of similar projections strung together. The Grassmanian is necessarily large and increases with the square of $p$. Viewing of the whole Grassmanian is time consuming, and interesting projections are sparse, there was a clear space for computers to narrow the search space.

Guided tours [@hurley_analyzing_1990] optimize an objective function generating path will be relatively small subset of the Grassmannian. As such, density and rapidity are poor measures, however interesting projections are quickly identified. Recently, @laa_using_2019, compared projection pursuit indices with the metrics: *smoothness, squintability, flexibility, rotation invariance* and *speed*. 


### Geometric display by dimensionality

Up to this point this document has discussed 2D scatterplots, which offer the first and a simple case for viewing lower-dimensional embeddings of $p$-space. However, other geometrics (or geoms) offer perfectly valid projections as well.

* 1D geoms
    + 1D densities: such as histogram, average shifted histograms [@scott_averaged_1985], and kernel density [@scott_incorporating_1995].
    + image (pixel): [@wegman_pixel_2001]
    + time series: where multivariate values are independently lagged to view peak and trough alignment. Currently no package implementation, but use case is discussed in [@cook_manual_1997]. 
* 2D geoms
    + 2D density (available on GitHub at https://github.com/nspyrison/tourr)
    + $x-y$ scatterplot
* 3D geoms - these geoms do not perform projections in 3 dimensions, but rather are $d=2$ projections that utilize the manipulation dimension to give depth perception cues.
    + Anaglyphs, sometimes called stereo, where (typically) red images are positioned for the left channel and cyan for the right, when viewed with corresponding filter glasses give the depth perception of the image.
    + Depth, which gives depth cues via aesthetic mappings, most commonly size and/or color of data points.
* $d$-dimensional geoms
    + Andrews curves [@andrews_plots_1972], smoothed variant of parallel coordinate plots, discussed below. 
    + Chernoff faces [@chernoff_use_1973], variables linked to size of facial features for rapid cursory like-ness comparison of observations.
    + Parallel coordinate plots [@ocagne_coordonnees_1885], where any number of variables are plotted in parallel with observations linked to their corresponding variable value by polylines.
    + Scatterplot matrix [@becker_brushing_1987], showing a triangle matrix of bivariate scatterplots with 1D density on the diagonal.
    + Radial glyphs, radial variants of parallel coordinates including radar, spider, and star glyphs [@siegel_surgical_1972]. 


### Tour software implementations

Tours have not yet been widely adopted, this is likely due in part, to the fact that print and static .pdf output does not accommodate dynamic viewing. Conceptual abstraction and technically density have also hampered user growth. Due to small adoption and rapid advancement of technology support and maintenance of such implementations give them a particularly short life span. Despite the small user base, there have been a fair number of software implementing touring in some degree, including:
<!-- See Wickham’s thesis and C2 paper for lists. -->

* spinifex [github.com/nspyrison/spinifex](https://github.com/nspyrison/spinifex) -- R package, all platforms.
* tourr [@wickham_tourr_2011] -- R package, all platforms.
* CyrstalVision [@wegman_visual_2003] -- for Windows.
* GGobi [@swayne_ggobi:_2003] -- for Linux and Windows.
* DAVIS [@huh_davis:_2002] -- Java based, with GUI.
* ORCA [@sutherland_orca:_2000] -- Extensible toolkit build in Java.
* VRGobi [@nelson_xgobi_1998] -- for use with the C2, tours in stereoscopic 3D displays.
* ExplorN [@carr_explorn:_1996] -- for SGI Unix.
* XGobi [@swayne_xgobi:_1991] -- for Linux, Unix, and Windows (via emulation).
* XLispStat [@tierney_lisp-stat:_1990] -- for Unix and Windows.
* Explor4 [@carr_explor4:_1988] -- Four-dimensional data using stereo-ray glyphs.
* Prim-9 [@asimov_grand_1985;@fisherkeller_prim-9:_1974] -- on an internal operating system.



### Going further

Most previous implementation of dynamic projections are failing to compile. I have extended the graphic options for touring dynamic projections and reimplemented UCS on a compatible platform for more sustainable support (RO #1). A comparative study outlining the benefits of UCS vs alternatives is also absent from the literature (RO #2). @nelson_xgobi_1998 views $d=2$ projections in head tracked 3D comparing them to 2D counterparts but didn't implement $d=3$ dynamic projections, facet of RO #3 & 4.


## Multivariate data visualization in 3D {#sec:3d}

As this research pertains to numeric multivariate data, a wider overview of 3D data visualization is discussed in chapter 2 of @marriott_immersive_2018.


### Terminology {#sec:3d-terminology}

* 2D - representation of data in 2 dimensions, without use of depth perception cues and minimal aesthetic mapping (such as color, size, and height) to data points.
* 2.5D - Following the definition given in @ware_designing_2000: visualizations that are essentially 2D but select depth cues are used to provide some suggestion of 3D. However, the term 2.5D is commonly used for several meanings *due to the ambiguous use of 2.5D, this document errs on the side stating 3D with descriptions of depth cues used*.
* 3D - visualizations of 3 dimensions with a liberal use of depth cues unless otherwise qualified.
* Depth perception cues - an indication that indicates depth to an observer, including:
    * linear perspective - the property of parallel lines converging on a vanishing point.
    * aerial perspective - objects that are far away have lower contrast and color saturation due to light scattering in the atmosphere.
    * occulation (or interposition) - where closer objects partially block the view of further objects.
    * motion perspective/parallax - closer objects, move across the field of view faster than further objects.
    * accommodation - the change of focal length due to change in the shape of the eye. Effective for distances of less than 2 meters.
    * binocular stereopsis/disparity - the use of 2 images of slightly varied angles from the horizontal distance of the eyes. The disparity for distant objects is small, but it is significant for nearby objects.
    * binocular convergence - The ocular-motor cue due to stereopsis focusing on the same objects. Convergence is effective for distances up to 10 meters.
* Virtual reality (VR) - computer generated display of virtual spaces in place of physical vision.
* Augmented reality (AR) - computer generated display of information overlaid on a physical space.
* Mixed reality (MR) - any degree of virtual or augmented reality.
* Scatterplot matrices (SPLOM) - matrix display of pair-wise 2D scatterplots with 1D density on the diagonal.


### A rocky start

Scientific visualization has readily adopted mixed realities as a large amount of the science exist in 3 spatial dimensions, lending itself well to virtual immersion [@marriott_immersive_2018]. Data visualization, on the other hand, has been slow to utilize graphics above 2.5D, (and haptic interaction) primarily due to the mixed results of over-hyped of 3D visuals from the 1980's and 90's [@munzner_visualization_2014]. However, since then there have been several promising studies suggesting that it's time for data visualization to revisit and adopt 3D visuals for specific combinations of visuals and depth cues.


### 3D rotated projections vs 3 2D orthogonal projections 

3D shapes can be represented by 3 orthogonal 2D views, or rather 3 pairwise projections. When 3D representations are used with binocular cues, they are found to have more accurate perception than 2D counterparts [@lee_effects_1986, depicted in figure \@ref(fig:lee86fig)].

(ref:lee86fig-cap) Screen capture of "Figure 7. 3-D Block Model" from @lee_effects_1986. 

```{r lee86fig, echo=F, out.width='70%', fig.cap = "(ref:lee86fig-cap)"}
knitr::include_graphics("./figures/lee86fig.PNG")
```


Between 3D and split view 2D of 3-dimensional economics data @wickens_implications_1994, depicted in figure \@ref(fig:wickens94fig), asked participants integrative questions, finding that participants were faster to answer when questions involved three dimensions, while performance was similar when questions involved fewer dimensions.

(ref:wickens94fig-cap) Screen capture of "Figure 5. Example of a mesh display" from @wickens_implications_1994.

```{r wickens94fig, echo=F, out.width='70%', fig.cap = "(ref:wickens94fig-cap)"}
knitr::include_graphics("./figures/wickens94fig.PNG")
```


Using 3D rotated projection gives more accurate perception (relative to 2D) of a ball suspended above complex box shapes, while combinations of 2D and 3D give the most precise orientation and positioning information [@tory_visualization_2006, depicted in figure \@ref(fig:tory06fig)].

(ref:tory06fig-cap) Screen capture from @tory_visualization_2006: "Fig. 1 (a) 2D, (b) 3D Rotated, (c) 3D Shadow, (d) Orientation Icon, and (e) ExoVis displays used in Experiment 1 (position estimation). Participants estimated the height of the ball relative to the block shape. In this example, the ball is at height 1.5 diameters above the block shape."

```{r tory06fig, echo=F, out.width='50%', fig.cap = "(ref:tory06fig-cap)"}
knitr::include_graphics("./figures/tory06fig.PNG")
```


@sedlmair_empirical_2013, depicted in figure \@ref(fig:sedlmair13fig), tasked users with cluster separation across 2D scatterplot, 2D scatterplot matrices (SPLOMs) and interactive 3D scatterplots as viewed in monocular 3D from a standard monitor. They conclude that interactive 3D scatterplots perform worse for class separation. This result is surprisingly as the extra dimension theoretically allows for clustering structure to be seen and explored more clearly.

(ref:sedlmair13fig-cap) Screen capture of "Figure 5. Example of a mesh display" from @sedlmair_empirical_2013: "Fig. 5. (a)-(d): Screenshots of the entangled dataset `entangled1-3d-3cl-separate` designed to show the most possible benefits for i3D. (a),(b) two viewpoints of the same i3D PCA scatterplot. An accompanying video shows the full 3D rotation. (c) 2D PCA projection. (d) t-SNE untangles this class structure in 2D. (e)-(f): 2D scatterplots of the reduced `entangled2-15d-adjacent` dataset which we designed to have a ground truth entangled class structure in 15D. (e) Glimmer MDS cannot untangle the classes, neither can PCA and robPCA (see supplemental material). (f) t-SNE nicely untangles and separates the ground truth classes in 2D."

```{r sedlmair13fig, echo=F, out.width='100%', fig.cap = "(ref:sedlmair13fig-cap)"}
knitr::include_graphics("./figures/sedlmair13fig.PNG")
```


### Comparing 3D and 2D embeddings of multivariate data

@nelson_xgobi_1998, depicted in figure \@ref(fig:nelson98fig), had $n=15$ participants perform brushing and touring tasks (identification of clusters, structure, and data dimensionality) in 3D with head-tracked binocular VR. 3D proved to have substantial advantage for cluster identification and some advantage in in identifying shape. Brushing did take longer in VR, perhaps due to the lower familiarity of manipulating 3D spaces. 

(ref:nelson98fig-cap) Screen capture from @nelson_xgobi_1998: "Figure 4: This is a picture of a 3-D room, running VRGobi. Data is plotted in the center, with painting tools to the right and variable spheres to the left. In the viewing box the data can be seen to contain three clusters, and one is being brushed."

```{r nelson98fig, echo=F, out.width='70%', fig.cap = "(ref:nelson98fig-cap)"}
knitr::include_graphics("./figures/nelson98fig.PNG")
```


Another study, @gracia_new_2016, depicted in figure \@ref(fig:gracia16fig), performed dimensionality reduction down to 2- and 3D scatterplots, both displayed in monocular 3D on a standard monitor. Users were found to more accurately compare distances between points and identify outliers on 3D scatterplots. However, both tasks were performed slower with use of the 3D scatterplots and statistical significance was not reported.

(ref:gracia16fig-cap) Screen capture from @gracia_new_2016: "Figure 5. Distance perception test. Left-hand image: 2D version. Here, the yellow line could be perceived as roughly twice the length of the magenta line, thus the value to be introduced should be approximately 2.0. Right-hand image: 3D version. Here, the inclusion of an extra dimension could provide new information about the relation, in terms of distances, between both lines."

```{r gracia16fig, echo=F, out.width='70%', fig.cap = "(ref:gracia16fig-cap)"}
knitr::include_graphics("./figures/gracia16fig")
```


@wagner_filho_immersive_2018, depicted in figure \@ref(fig:wagner18fig), performed an $n=30$ empirical study of PCA embedded projections, and perception error across 4 tasks and 3 display types: 2D, 3D, and immersive. Overall task error was less in 3D and immersive relative to 2D. According to the user Likert-scale survey 2D is slightly easier to navigate and slightly more comfortable, while, 3D and immersive display are slightly easier to interact and moderately easier to find information.

(ref:wagner18fig-cap) Screen capture from @wagner_filho_immersive_2018, original captions contained in capture.

```{r wagner18fig, echo=F, out.width='50%', fig.cap = "(ref:wagner18fig-cap)"}
knitr::include_graphics("./figures/wagner18fig")
```


### Going further

When comparing between 3D and 2D orthogonal views studies in general show that perception accuracy is better in 3D, though manipulation speed is generally slower. The speed discrepancy is confounded by the difference in users familiar with manipulating 2D vs 3D spaces [@lee_effects_1986; @wickens_implications_1994; @tory_visualization_2006; counter example @sedlmair_empirical_2013]. Similar results have been shown in static, 3D embedded spaces [@gracia_new_2016; @wagner_filho_immersive_2018] and in dynamic 2D embedded spaces depicted in immersive 3D [@nelson_xgobi_1998]. After more than two decades of hardware advancement, the quality, support, and prevalence of VR technology is better and more easily accessible than ever. It's time to view dynamic 3D projections with immersive spaces and quantify the benefits (RO #3 & 4).
